{
  "hash": "70c50eb6d64a1510ccb8ac317fd8fec4",
  "result": {
    "markdown": "---\ntitle: Cluster Validity Indices\ndescription: This project was the seminal work of my master's degree in statistics.\nauthor: Matthias Quinn\ndate: 01/22/2023\ncode-fold: true\ncode-line-numbers: true\ncategories:\n  - code\n  - ML\n  - algorithms\n  - college\nimage: clustering-algorithms-in-Machine-Learning.jpg\nimage-alt: K-Means Example\nformat: html\nexecute:\n  cache: true\n---\n\n# Abstract\n\nThe following research examines various clustering methods and internal validity indices. Many clustering algorithms depend on various assumptions in order to identify subgroups, so there exists a need to objectively evaluate these algorithms, whether through complexity analyses or the proposed internal validity indices. The goal is to apply these indices to both artificial and real data in order to assess their fidelity. Currently, there exists no Python package to achieve this goal, but the proposed library offers $31$ indices to help the user choose the correct number of clusters in his/her data, regardless of the chosen clustering methodology.\n\n## 1. Introduction\n\nThe goal of this project is to understand and compute a variety of indices that indicate the optimal number of clusters to use when performing a cluster analysis.\n\nCluster analysis refers to a finding unique subgroup/clusters in a dataset where the observations within each cluster are more related to each other in some way compared to observations in another, separate cluster. There are a plethora of clustering techniques and algorithms, each with a different way of solving the above general problem. In addition, there does not exist a single, optimal clustering solution for any clustering problem. This is also known as the \"No Free Lunch\" theorem from David Wolpert and William Macready, which states that \"any two optimization algorthims are equivalent when their performance is averaged across all possible problems.\" [(Wolpert, Macready, 2005)](#NFL)\n\nSince cluster analysis is inherently an unsupervised learning technique, the selection of the optimal number of clusters is subjective. Consider the following example that was made via Python:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata, true_labels = make_blobs(n_samples=500, n_features=2, centers = 4, random_state=1234, cluster_std=0.9)\n\npoints = pd.DataFrame(data, columns=['x', 'y'])\n\nk4 = KMeans(n_clusters=4, random_state=1234).fit(points)\nk5 = KMeans(n_clusters=5, random_state=1234).fit(points)\nk6 = KMeans(n_clusters=6, random_state=1234).fit(points)\n\nfig = plt.figure(figsize=(12, 12))\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(223)\nax4 = fig.add_subplot(224)\n\nax1.scatter(points['x'], points['y'], color=\"black\")\nax1.axes.get_xaxis().set_visible(False)\nax1.axes.get_yaxis().set_visible(False)\nax2.scatter(points['x'], points['y'], c=k4.labels_.astype(float))\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax3.scatter(points['x'], points['y'], c=k5.labels_.astype(float))\nax3.axes.get_xaxis().set_visible(False)\nax3.axes.get_yaxis().set_visible(False)\nax4.scatter(points['x'], points['y'], c=k6.labels_.astype(float))\nax4.axes.get_xaxis().set_visible(False)\nax4.axes.get_yaxis().set_visible(False)\n\nax1.title.set_text('Original Data')\nax2.title.set_text('K=4 Clusters')\nax3.title.set_text('K=5 Clusters')\nax4.title.set_text('K=6 Clusters')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\miqui\\anaconda3\\envs\\MLDL\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\nC:\\Users\\miqui\\anaconda3\\envs\\MLDL\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\nC:\\Users\\miqui\\anaconda3\\envs\\MLDL\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](cluster-indices_files/figure-html/cell-2-output-2.png){width=917 height=931}\n:::\n:::\n\n\n",
    "supporting": [
      "cluster-indices_files"
    ],
    "filters": [],
    "includes": {}
  }
}