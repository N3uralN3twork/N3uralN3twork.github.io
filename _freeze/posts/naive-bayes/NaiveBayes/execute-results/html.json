{
  "hash": "b3e29413f426b845ad9ce6037a89de53",
  "result": {
    "markdown": "---\ntitle: \"Naive Bayes\"\ndescription: \"A quick notebook on how to create and interpret naive bayes models.\"\nauthor: \"Matthias Quinn\"\ndate: \"1/29/2020\"\ncategories:\n  - code\n  - ML\n  - algorithms\n  - college\nimage: NaiveBayesImage.jpg\nimage-alt: Naive Bayes in R\n---\n\n\n\n\nNaive Bayes is a supervised machine learning algorithm base on Bayes Theorem.\n\nNaive because it assumes all of the predictor variables to be completely independent of each other.\n\nBayes Rule:\n\n$P(A|B) = \\frac{P(B|A)P(A)}{P(B}$\n\n  * $P(A|B)$: Conditional probability of event A occuring, given event B\n  \n  * $P(A)$: Probability of event A occuring\n  \n  * $P(B)$: Probability of event B occuring\n  \n  * $P(B|A)$: Conditional probability of event B occuring, given event A\n\nIn Naive Bayes, there are multiple predictor variables and more than 1 output class.\n\nThe objective of a Naive Bayes algorithm is to measure the conditional probability of an event with a feature vector\n\n**Problem Statement:**\n\nPredict employee attrition\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"attrition\")\nattrition <- attrition %>%\n                mutate(JobLevel = factor(JobLevel),\n                       StockOptionLevel = factor(StockOptionLevel),\n                       TrainingTimesLastYear = factor(TrainingTimesLastYear))\n```\n:::\n\n\nCreate training (70%) and test (30%) sets and utilize reproducibility.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nsplit <- initial_split(data = attrition, prop = 0.7, strata = \"Attrition\")\n\ntrain <- training(split)\ntest  <- testing(split)\n```\n:::\n\n\nDistribution of Attrition rates across train and test sets.\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.table(table(train$Attrition))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n       No       Yes \n0.8394942 0.1605058 \n```\n:::\n\n```{.r .cell-code}\nprop.table(table(test$Attrition))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n       No       Yes \n0.8371041 0.1628959 \n```\n:::\n:::\n\nNotice that they are very similar.\n\n# A Naive Overview\n\nThe Naive Bayes classifier is founded on Bayesian probability, which incorporates the concept of conditional probability.\n\nIn our attrition data set, we are seeking the probability of an employee belonging to attrition class $C_{k}$ given the predictor variables $x_{1}, x_{2}, ... x_{n}$\n\nA posterior:\n\n$Posterior = \\frac{Prior * Likelihood}{Evidence}$\n\nAssumption:\n\nThe Naive Bayes classifier assumes that the predictor variables are conditionally **independent** of each other, with no correlation.\n\nAn assumption of normality is often used for continuous variables.\n\nBut you can see that normality assumption is not always held:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>% \n  select(Age, DailyRate, DistanceFromHome, HourlyRate, MonthlyIncome, MonthlyRate) %>% \n  gather(metric, value) %>% \n  ggplot(aes(value, fill = metric)) + \n  geom_density(show.legend = FALSE) + \n  facet_wrap(~ metric, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](NaiveBayes_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n## **Advantages and Shortcoming**\n\nThe Naive Bayes classifier is simple, fast, and scales well to large n.\n\nA major disadvantage is that it relies on the often-wrong assumption of equally important and independent features.\n\n## **Implementation**\n\nWe will utilize the *caret* package in R.\n\n1. Create response and feature data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeatures <- setdiff(names(train), \"Attrition\")\nx <- train[, features]\ny <- train$Attrition\n```\n:::\n\n\nInitialize 10-fold cross validation using caret's trainControl function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrControl <- trainControl(method = \"cv\", number = 10)\n```\n:::\n\n\n\nTraining the Naive Bayes model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb.fit <- train(\n  x = x,\n  y = y,\n  method = \"nb\",\n  trControl = trControl\n)\n```\n:::\n\n\n#Confusion Matrix to analyze our results:\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(nb.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCross-Validated (10 fold) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction   No  Yes\n       No  76.8  8.2\n       Yes  7.1  7.9\n                            \n Accuracy (average) : 0.8473\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(nb.fit)\n```\n\n::: {.cell-output-display}\n![](NaiveBayes_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nTo test the accuracy on our test data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- predict(nb.fit, newdata = test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconfusionMatrix(preds, reference = test$Attrition)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  329  38\n       Yes  41  34\n                                          \n               Accuracy : 0.8213          \n                 95% CI : (0.7823, 0.8559)\n    No Information Rate : 0.8371          \n    P-Value [Acc > NIR] : 0.8333          \n                                          \n                  Kappa : 0.3554          \n                                          \n Mcnemar's Test P-Value : 0.8220          \n                                          \n            Sensitivity : 0.8892          \n            Specificity : 0.4722          \n         Pos Pred Value : 0.8965          \n         Neg Pred Value : 0.4533          \n             Prevalence : 0.8371          \n         Detection Rate : 0.7443          \n   Detection Prevalence : 0.8303          \n      Balanced Accuracy : 0.6807          \n                                          \n       'Positive' Class : No              \n                                          \n```\n:::\n:::\n",
    "supporting": [
      "NaiveBayes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}