<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Matthias Quinn">
<meta name="dcterms.date" content="2023-01-22">
<meta name="description" content="This project was the seminal work of my master’s degree in statistics.">

<title>N3uralN3twork’s Website - Cluster Validity Indices</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/Combo-Chart-Icon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      background: #FFAC1C;
    }
    </style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
        <script type="text/javascript">
        window.PlotlyConfig = {MathJaxConfig: 'local'};
        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
        if (typeof require !== 'undefined') {
        require.undef("plotly");
        requirejs.config({
            paths: {
                'plotly': ['https://cdn.plot.ly/plotly-2.18.0.min']
            }
        });
        require(['plotly'], function(Plotly) {
            window._Plotly = Plotly;
        });
        }
        </script>
        

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Cluster Validity Indices</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Cluster Validity Indices</h1>
                  <div>
        <div class="description">
          This project was the seminal work of my master’s degree in statistics.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">ML</div>
                <div class="quarto-category">algorithms</div>
                <div class="quarto-category">college</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Matthias Quinn </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 22, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../images/Transhumanism Logo for Website.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/N3uralN3twork/n3uraln3twork.github.io" title="Website" class="sidebar-tool px-1"><i class="bi bi-globe"></i></a>
    <a href="https://github.com/N3uralN3twork" title="Personal Github" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/matthiasquinn" title="Personal Linkedin" class="sidebar-tool px-1"><i class="bi bi-linkedin"></i></a>
    <a href="mailto:miq_qedquinn@yahoo.com" title="Contact Me" class="sidebar-tool px-1"><i class="bi bi-question-circle"></i></a>
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../aboutme.html" class="sidebar-item-text sidebar-link">Who am I?</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../resumes.html" class="sidebar-item-text sidebar-link">Resumes</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../projects.html" class="sidebar-item-text sidebar-link">Projects</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pytorch.html" class="sidebar-item-text sidebar-link">PyTorch</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../publications.html" class="sidebar-item-text sidebar-link">Papers</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">1. Introduction</a></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods"><strong>2. Methods</strong></a>
  <ul class="collapse">
  <li><a href="#language" id="toc-language" class="nav-link" data-scroll-target="#language">2.1 Language</a></li>
  <li><a href="#related-work" id="toc-related-work" class="nav-link" data-scroll-target="#related-work">2.2 Related Work</a></li>
  </ul></li>
  <li><a href="#clustering-algorithms" id="toc-clustering-algorithms" class="nav-link" data-scroll-target="#clustering-algorithms">3. Clustering Algorithms</a>
  <ul class="collapse">
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means">3.1 K-Means</a>
  <ul class="collapse">
  <li><a href="#the-k-means-algorithm" id="toc-the-k-means-algorithm" class="nav-link" data-scroll-target="#the-k-means-algorithm">3.1.1 The K-Means Algorithm</a></li>
  <li><a href="#time-and-space-complexity" id="toc-time-and-space-complexity" class="nav-link" data-scroll-target="#time-and-space-complexity">3.1.2 Time and Space Complexity</a></li>
  </ul></li>
  <li><a href="#agglomerative-hierarchical-clustering" id="toc-agglomerative-hierarchical-clustering" class="nav-link" data-scroll-target="#agglomerative-hierarchical-clustering">3.2 Agglomerative Hierarchical Clustering</a>
  <ul class="collapse">
  <li><a href="#agglomerative-hierarchical-clustering-algorithm" id="toc-agglomerative-hierarchical-clustering-algorithm" class="nav-link" data-scroll-target="#agglomerative-hierarchical-clustering-algorithm">3.2.1 Agglomerative Hierarchical Clustering Algorithm</a></li>
  <li><a href="#time-and-space-complexity-1" id="toc-time-and-space-complexity-1" class="nav-link" data-scroll-target="#time-and-space-complexity-1">3.2.2 Time and Space Complexity</a></li>
  </ul></li>
  <li><a href="#dbscan" id="toc-dbscan" class="nav-link" data-scroll-target="#dbscan">3.3 DBSCAN</a></li>
  </ul></li>
  <li><a href="#internal-cluster-validity-indices" id="toc-internal-cluster-validity-indices" class="nav-link" data-scroll-target="#internal-cluster-validity-indices">4. Internal Cluster Validity Indices</a>
  <ul class="collapse">
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">4.0 Notation:</a></li>
  <li><a href="#ball-hall-index" id="toc-ball-hall-index" class="nav-link" data-scroll-target="#ball-hall-index">4.1 </a><a name="bhindex" class="nav-link" data-scroll-target="undefined"></a>Ball-Hall Index</li>
  <li><a href="#the-beale-index" id="toc-the-beale-index" class="nav-link" data-scroll-target="#the-beale-index">4.2 The Beale Index</a></li>
  <li><a href="#the-c-index" id="toc-the-c-index" class="nav-link" data-scroll-target="#the-c-index">4.3 The C Index</a></li>
  <li><a href="#the-calinski-harabasz-index" id="toc-the-calinski-harabasz-index" class="nav-link" data-scroll-target="#the-calinski-harabasz-index">4.4 The Calinski-Harabasz Index</a></li>
  <li><a href="#cubic-clustering-criterion" id="toc-cubic-clustering-criterion" class="nav-link" data-scroll-target="#cubic-clustering-criterion">4.5 Cubic Clustering Criterion</a></li>
  <li><a href="#d-index" id="toc-d-index" class="nav-link" data-scroll-target="#d-index">4.6 D Index</a></li>
  <li><a href="#davies-bouldin-index" id="toc-davies-bouldin-index" class="nav-link" data-scroll-target="#davies-bouldin-index">4.7 Davies-Bouldin Index</a></li>
  <li><a href="#duda-index" id="toc-duda-index" class="nav-link" data-scroll-target="#duda-index">4.8 Duda Index</a></li>
  <li><a href="#dunn-index" id="toc-dunn-index" class="nav-link" data-scroll-target="#dunn-index">4.9 Dunn Index</a></li>
  <li><a href="#frey-index" id="toc-frey-index" class="nav-link" data-scroll-target="#frey-index">4.10 Frey Index</a></li>
  <li><a href="#gamma-index" id="toc-gamma-index" class="nav-link" data-scroll-target="#gamma-index">4.11 Gamma Index</a></li>
  <li><a href="#g-plus-index" id="toc-g-plus-index" class="nav-link" data-scroll-target="#g-plus-index">4.12 G-Plus Index</a></li>
  <li><a href="#hartigan-index" id="toc-hartigan-index" class="nav-link" data-scroll-target="#hartigan-index">4.13 </a><a name="hartindex" class="nav-link" data-scroll-target="undefined"></a>Hartigan Index</li>
  <li><a href="#log-ss-ratio-index" id="toc-log-ss-ratio-index" class="nav-link" data-scroll-target="#log-ss-ratio-index">4.14 </a><a name="logssindex" class="nav-link" data-scroll-target="undefined"></a>Log SS Ratio Index</li>
  <li><a href="#marriot-index" id="toc-marriot-index" class="nav-link" data-scroll-target="#marriot-index">4.15 Marriot Index</a></li>
  <li><a href="#mcclain-index" id="toc-mcclain-index" class="nav-link" data-scroll-target="#mcclain-index">4.16 McClain Index</a></li>
  <li><a href="#point-biserial-index" id="toc-point-biserial-index" class="nav-link" data-scroll-target="#point-biserial-index">4.17 </a><a name="pbindex" class="nav-link" data-scroll-target="undefined"></a>Point-Biserial Index</li>
  <li><a href="#pseudo-t2-index" id="toc-pseudo-t2-index" class="nav-link" data-scroll-target="#pseudo-t2-index">4.18 Pseudo T2 Index</a></li>
  <li><a href="#ratkowsky-lance-index" id="toc-ratkowsky-lance-index" class="nav-link" data-scroll-target="#ratkowsky-lance-index">4.19 Ratkowsky-Lance Index</a></li>
  <li><a href="#ray-turi-index" id="toc-ray-turi-index" class="nav-link" data-scroll-target="#ray-turi-index">4.20 Ray-Turi Index</a></li>
  <li><a href="#r-squared-index" id="toc-r-squared-index" class="nav-link" data-scroll-target="#r-squared-index">4.21 </a><a name="r2index" class="nav-link" data-scroll-target="undefined"></a>R Squared Index</li>
  <li><a href="#rubin-index" id="toc-rubin-index" class="nav-link" data-scroll-target="#rubin-index">4.22 Rubin Index</a></li>
  <li><a href="#scott-symons-index" id="toc-scott-symons-index" class="nav-link" data-scroll-target="#scott-symons-index">4.23 Scott-Symons Index</a></li>
  <li><a href="#sd-index" id="toc-sd-index" class="nav-link" data-scroll-target="#sd-index">4.24 SD Index</a></li>
  <li><a href="#sdbw-index" id="toc-sdbw-index" class="nav-link" data-scroll-target="#sdbw-index">4.25 SDbw Index</a></li>
  <li><a href="#silhouette-score" id="toc-silhouette-score" class="nav-link" data-scroll-target="#silhouette-score">4.26 Silhouette Score</a></li>
  <li><a href="#tau-index" id="toc-tau-index" class="nav-link" data-scroll-target="#tau-index">4.27 Tau Index</a></li>
  <li><a href="#trace-cov-w-index" id="toc-trace-cov-w-index" class="nav-link" data-scroll-target="#trace-cov-w-index">4.28 Trace-Cov-W Index</a></li>
  <li><a href="#trace-w-index" id="toc-trace-w-index" class="nav-link" data-scroll-target="#trace-w-index">4.29 Trace-W Index</a></li>
  <li><a href="#wemmert-gancarski-index" id="toc-wemmert-gancarski-index" class="nav-link" data-scroll-target="#wemmert-gancarski-index">4.30 </a><a name="wgindex" class="nav-link" data-scroll-target="undefined"></a>Wemmert-Gancarski Index</li>
  <li><a href="#xie-beni-index" id="toc-xie-beni-index" class="nav-link" data-scroll-target="#xie-beni-index">4.31 </a><a name="xbindex" class="nav-link" data-scroll-target="undefined"></a>Xie-Beni Index</li>
  <li><a href="#indices-performance" id="toc-indices-performance" class="nav-link" data-scroll-target="#indices-performance"><strong>5.2 Indices Performance</strong></a>
  <ul class="collapse">
  <li><a href="#ball-hall-index-1" id="toc-ball-hall-index-1" class="nav-link" data-scroll-target="#ball-hall-index-1">5.2.1 Ball-Hall Index:</a></li>
  <li><a href="#c-index" id="toc-c-index" class="nav-link" data-scroll-target="#c-index">5.2.2 <span class="math inline">C</span> Index:</a></li>
  <li><a href="#dunn-index-1" id="toc-dunn-index-1" class="nav-link" data-scroll-target="#dunn-index-1">5.2.3 Dunn Index:</a></li>
  <li><a href="#hartigan-index-1" id="toc-hartigan-index-1" class="nav-link" data-scroll-target="#hartigan-index-1">5.2.4 Hartigan Index:</a></li>
  <li><a href="#point-biserial-index-1" id="toc-point-biserial-index-1" class="nav-link" data-scroll-target="#point-biserial-index-1">5.2.5 Point-biserial Index:</a></li>
  <li><a href="#r2-index" id="toc-r2-index" class="nav-link" data-scroll-target="#r2-index">5.2.6 <span class="math inline">R^{2}</span> Index:</a></li>
  <li><a href="#wemmert-gancarski-index-1" id="toc-wemmert-gancarski-index-1" class="nav-link" data-scroll-target="#wemmert-gancarski-index-1">5.2.7 Wemmert-Gancarski Index:</a></li>
  <li><a href="#wg-index" id="toc-wg-index" class="nav-link" data-scroll-target="#wg-index">WG Index:</a></li>
  <li><a href="#xie-beni-index-1" id="toc-xie-beni-index-1" class="nav-link" data-scroll-target="#xie-beni-index-1">5.2.8 Xie-Beni Index:</a></li>
  <li><a href="#speed-testing-the-xie-beni-index" id="toc-speed-testing-the-xie-beni-index" class="nav-link" data-scroll-target="#speed-testing-the-xie-beni-index">Speed-testing the Xie-Beni Index:</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">6. Conclusion</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/N3uralN3twork/n3uraln3twork.github.io/edit/main/posts/clustering-indices-project/cluster-indices.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/N3uralN3twork/n3uraln3twork.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>The following research examines various clustering methods and internal validity indices. Many clustering algorithms depend on various assumptions in order to identify subgroups, so there exists a need to objectively evaluate these algorithms, whether through complexity analyses or the proposed internal validity indices. The goal is to apply these indices to both artificial and real data in order to assess their fidelity. Currently, there exists no Python package to achieve this goal, but the proposed library offers <span class="math inline">31</span> indices to help the user choose the correct number of clusters in his/her data, regardless of the chosen clustering methodology.</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>The goal of this project is to understand and compute a variety of indices that indicate the optimal number of clusters to use when performing a cluster analysis.</p>
<p>Cluster analysis refers to a finding unique subgroup/clusters in a dataset where the observations within each cluster are more related to each other in some way compared to observations in another, separate cluster. There are a plethora of clustering techniques and algorithms, each with a different way of solving the above general problem. In addition, there does not exist a single, optimal clustering solution for any clustering problem. This is also known as the “No Free Lunch” theorem from David Wolpert and William Macready, which states that “any two optimization algorthims are equivalent when their performance is averaged across all possible problems.” <a href="#NFL">(Wolpert, Macready, 2005)</a></p>
<p>Since cluster analysis is inherently an unsupervised learning technique, the selection of the optimal number of clusters is subjective. Consider the following example that was made via Python:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a>data, true_labels <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">500</span>, n_features<span class="op">=</span><span class="dv">2</span>, centers <span class="op">=</span> <span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">1234</span>, cluster_std<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>points <span class="op">=</span> pd.DataFrame(data, columns<span class="op">=</span>[<span class="st">'x'</span>, <span class="st">'y'</span>])</span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a>k4 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">1234</span>).fit(points)</span>
<span id="cb1-12"><a href="#cb1-12"></a>k5 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">1234</span>).fit(points)</span>
<span id="cb1-13"><a href="#cb1-13"></a>k6 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">6</span>, random_state<span class="op">=</span><span class="dv">1234</span>).fit(points)</span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb1-16"><a href="#cb1-16"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">221</span>)</span>
<span id="cb1-17"><a href="#cb1-17"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">222</span>)</span>
<span id="cb1-18"><a href="#cb1-18"></a>ax3 <span class="op">=</span> fig.add_subplot(<span class="dv">223</span>)</span>
<span id="cb1-19"><a href="#cb1-19"></a>ax4 <span class="op">=</span> fig.add_subplot(<span class="dv">224</span>)</span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>ax1.scatter(points[<span class="st">'x'</span>], points[<span class="st">'y'</span>], color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb1-22"><a href="#cb1-22"></a>ax1.axes.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-23"><a href="#cb1-23"></a>ax1.axes.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-24"><a href="#cb1-24"></a>ax2.scatter(points[<span class="st">'x'</span>], points[<span class="st">'y'</span>], c<span class="op">=</span>k4.labels_.astype(<span class="bu">float</span>))</span>
<span id="cb1-25"><a href="#cb1-25"></a>ax2.axes.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-26"><a href="#cb1-26"></a>ax2.axes.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-27"><a href="#cb1-27"></a>ax3.scatter(points[<span class="st">'x'</span>], points[<span class="st">'y'</span>], c<span class="op">=</span>k5.labels_.astype(<span class="bu">float</span>))</span>
<span id="cb1-28"><a href="#cb1-28"></a>ax3.axes.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-29"><a href="#cb1-29"></a>ax3.axes.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-30"><a href="#cb1-30"></a>ax4.scatter(points[<span class="st">'x'</span>], points[<span class="st">'y'</span>], c<span class="op">=</span>k6.labels_.astype(<span class="bu">float</span>))</span>
<span id="cb1-31"><a href="#cb1-31"></a>ax4.axes.get_xaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-32"><a href="#cb1-32"></a>ax4.axes.get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a>ax1.title.set_text(<span class="st">'Original Data'</span>)</span>
<span id="cb1-35"><a href="#cb1-35"></a>ax2.title.set_text(<span class="st">'K=4 Clusters'</span>)</span>
<span id="cb1-36"><a href="#cb1-36"></a>ax3.title.set_text(<span class="st">'K=5 Clusters'</span>)</span>
<span id="cb1-37"><a href="#cb1-37"></a>ax4.title.set_text(<span class="st">'K=6 Clusters'</span>)</span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\miqui\anaconda3\envs\MLDL\lib\site-packages\sklearn\cluster\_kmeans.py:870: FutureWarning:

The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning

C:\Users\miqui\anaconda3\envs\MLDL\lib\site-packages\sklearn\cluster\_kmeans.py:870: FutureWarning:

The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning

C:\Users\miqui\anaconda3\envs\MLDL\lib\site-packages\sklearn\cluster\_kmeans.py:870: FutureWarning:

The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="cluster-indices_files/figure-html/cell-2-output-2.png" width="917" height="931"></p>
</div>
</div>
<p>The figure above helps to illustrate that the definition of a cluster is imprecise and that the best definition depends on the nature of the data and the user’s desired results.</p>
</section>
</section>
<section id="methods" class="level1">
<h1><strong>2. Methods</strong></h1>
<section id="language" class="level2">
<h2 class="anchored" data-anchor-id="language">2.1 Language</h2>
<p>Python was chosen as the language of choice due to the author’s familiarity with the various libraries made available in Python, including the NumPy and Sci-kit Learn modules. R was heavily used to double-check many of the algorithms and indices, relying mostly on the <strong>NbClust</strong> library <a href="#NBClust">(Charrad, 2014)</a>.</p>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">2.2 Related Work</h2>
<p>A variety of measures aiming to validate the results of a cluster analysis have been defined over the years and this project focuses on relative criteria, which consists of the evaluation of a clustering algorithm by comparing it with the same algorithm but varies in the input parameters.</p>
<p>The inspiration for this project came on the heels of a separate project where a cluster analysis was used, but choosing the number of clusters appropriate for the final conclusion was unclear, both numerically and graphically. R has great support for cluster analysis and choosing the number of clusters; in particular, the <em>NbClust</em> package provides extensive support for both tasks and is the main inspiration for this project. The <em>R</em> package came out in October 2014 and contains a variety of indices that help aid the user in choosing the appropriate number of clusters, given one of two clustering methods; namely, K-Means and agglomerative clustering.</p>
<p>However, when looking to validate results in another language, there was an apalling lack of libraries available for accomplishing the task. Python lacks, and still does as of writing, a comprehensive package for choosing an appropriate number of clusters. Python’s <em>Sci-kit Learn</em> library does have support for a plethora of clustering algorithms, including the ones used in this project, as well as a few others. However, it only supports three clustering indices.</p>
<p>The three clustering indices covered by Python’s <em>Sci-kit Learn</em> library are the silhouette coefficient, the Davies-Bouldin index, and the Calinski-Harabasz index. The proposed library, as well as <em>NbClust</em>, support all three of the indices in addition to many more.</p>
</section>
</section>
<section id="clustering-algorithms" class="level1">
<h1>3. Clustering Algorithms</h1>
<p>It is worth mentioning that the algorithms discussed below are not an exhaustive list of potential clustering methods and the proposed Python package supports any method that produces predictive labels. Also, R’s <em>NbClust</em> package only supports two clustering algorithms at the moment; namely, K-Means and Hierarchical clustering.</p>
<section id="k-means" class="level2">
<h2 class="anchored" data-anchor-id="k-means">3.1 K-Means</h2>
<p>The K-Means algorithm is a centroid-based clustering technique and is one of the most popular clustering algorithms, so it had to be covered. A slight alternative to the algorithm, K-medians, uses the median instead of the mean, but will not be covered in this project. The algorithm attempts to minimize the within-cluster variances, which will be formalized next.</p>
<section id="the-k-means-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-k-means-algorithm">3.1.1 The K-Means Algorithm</h3>
<p>Given a set of <em>n</em> observations each consisting of <em>d</em> dimensions, and wanting to partition those observations into <em>k</em> clusters, the goal is to minimize the within-cluster sum of squares (WSS). Formally, the K-Means algorithm is as follows:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="KMeans_Algorithm_Picture.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Fig.2 - Pseudocode for the generic K-Means algorithm</figcaption><p></p>
</figure>
</div>
<p>Initially, points are assigned to the initial centroids, which in this case is the mean. After the points are assigned to the centroid, the centroid is then updated until no more changes occur and the algorithm converges. There is no guarantee, however, that the algorithm will converge. The objective is to minimize the pairwise squared deviations of points in the same cluster:</p>
<p><span class="math display">argmin \displaystyle \sum_{i=1}^{k}\frac {1}{2 |S_i|} * \sum ||{x - y}||^{2} \tag{1}</span></p>
<p>The within cluster sum of squares, or what the <em>Sci-kit Learn</em> package calls <strong>inertia</strong>, is a measure of how internally coherent separate clusters are. The optimal value is 0 while lower values indicate more separated clusters. <em>Sci-kit Learn</em> uses Lloyd’s algorithm which follows the steps in the algorithm presented above and uses the squared Euclidean distance.</p>
</section>
<section id="time-and-space-complexity" class="level3">
<h3 class="anchored" data-anchor-id="time-and-space-complexity">3.1.2 Time and Space Complexity</h3>
<p>K-Means only needs to store the data points and each centroid. Specifically, the storage requirements for the algorithm is <span class="math inline">O(n(m + K))</span>, where m is the number of data points, n is the number of variables, and K is the pre-specified number of clusters. K-Means runs in linear time, in particular, <span class="math inline">O(I*K*m*n)</span>, where <span class="math inline">I</span> is the number of iterations until convergence. As long as the number of clusters remains significantly below the number of variables, the K-Means algorithm should run in linear time.</p>
</section>
</section>
<section id="agglomerative-hierarchical-clustering" class="level2">
<h2 class="anchored" data-anchor-id="agglomerative-hierarchical-clustering">3.2 Agglomerative Hierarchical Clustering</h2>
<p>Another commonly-used approach for clustering is that of hierarchical clustering, of which, there are two approaches; namely, Agglomerative, and Divisive.</p>
<p><strong>Agglomerative:</strong> Start with <span class="math inline">n</span> clusters and keep going until you have <span class="math inline">1</span> cluster.</p>
<p><strong>Divisive:</strong> Start with <span class="math inline">1</span> cluster and decompose until you have <span class="math inline">n</span> clusters.</p>
<p>This section will focus solely on the Agglomerative variant.</p>
<section id="agglomerative-hierarchical-clustering-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="agglomerative-hierarchical-clustering-algorithm">3.2.1 Agglomerative Hierarchical Clustering Algorithm</h3>
<p><img src="Agglomerative_Algorithm_Picture.png" class="img-fluid" data-fig-align="center" alt="Fig.3 - Pseudocode for the generic Agglomerative clustering algorithm"> The key operation of the Agglomerative Clustering algorithm is the calculation of the proximity between two clusters, where the definition of cluster proximity differentiates the various agglomerative flavors.</p>
<p>Ward’s method assumes that a cluster is represented by its centroid, but it measures the proximity between two clusters in terms of the increase in the SSE that results from merging the two clusters. Ward’s method attempts to minimize the sum of the squared distances of points from their cluster centroids. The initial cluster distances in Ward’ minimum variance method are defined to be the squared Euclidean distance between points:</p>
<p><span class="math display">d_{ij} = ||X_i - X_j||^2 \tag{2}</span></p>
</section>
<section id="time-and-space-complexity-1" class="level3">
<h3 class="anchored" data-anchor-id="time-and-space-complexity-1">3.2.2 Time and Space Complexity</h3>
<p>Ward’s method requires the storage of <span class="math inline">0.5n^{2}</span> where <span class="math inline">n</span> is the number of datapoints and the storage required to keep track of the clusters is proportional to the number of clusters, which is <span class="math inline">n-1</span>, excluding singleton clusters. Thus, the total space complexity of Ward’s method is <span class="math inline">O(n^{2})</span>.</p>
<p>Next, in terms of the time required to run the algorithm, a good place to start is the computation of the proximity matrix. <span class="math inline">O(n^{2})</span> time is required to compute the proximity matrix. Afterwards there are <span class="math inline">n-1</span> iterations involving steps <span class="math inline">3</span> and <span class="math inline">4</span> because there are <span class="math inline">n</span> clusters at the start and two clusters are merged after each iteration. In addition, because the algorithm exhaustively scans the proximity matrix for the lowest distances, the run time is <span class="math inline">O(n^{3})</span>.</p>
<p>Obviously, because of the rather large space and time complexity of the Agglomerative Clustering algorithm, it should not be used for large datasets. In addition, the Agglomerative Clustering algorithm tends to make good local decisions about which two clusters to combine since they can use information about the pairwise similarity of all of the data points. However, once a decision is made to merge two clusters, the merge is final and it cannot be undone.</p>
</section>
</section>
<section id="dbscan" class="level2">
<h2 class="anchored" data-anchor-id="dbscan">3.3 DBSCAN</h2>
<p>Density-based clustering locates regions of high density that are separated from one another by regions of low density. A cluster is considered to be a set of core samples and a set of non-core samples that are close to a core sample. In this approach, known as the <strong>center-based approach</strong>, the density for a particular point is estimated by counting the number of points within a specified radius, which is controlled by the Epsilon parameters.</p>
<p>There are three types of data points of consideration in the DBSCAN algorithm, namely:</p>
<p><strong>Core points:</strong> The points are inside the interior of a density-based cluster. A point is a core point if the number of points within a given neighborhood around the point exceeds a certain threshold, min_samples, as defined by <em>Sci-kit Learn</em>.</p>
<p><strong>Border points:</strong> A point that is not a core point, but rather falls within the neighborhood of a core point, which can fall within the neighborhood of several core points.</p>
<p><strong>Noise points:</strong> Any points that is neither a core point nor a border point.</p>
<p>An example of the three points described above are shown below:</p>
<p><img src="DBScan Points.png" class="img-fluid" data-fig-align="center" alt="Fig. 4 - Demonstration of Core, Border, and Noise points present during the usage of the DBSCAN algorithm"> ### 3.3.1 The DBSCAN algorithm</p>
<p>Informally, any two core points that are close enough, within a distance of the Epsilon parameter, are put into the same cluster. Additionally, any border point that is close enough to a core point is put in the same cluster as the core point. Finally, noise points are discarded. More formally:</p>
<p><img src="DBSCAN_Algorithm_Picture.png" class="img-fluid" data-fig-align="center" alt="Fig. 5 - Pseudocode for the generic DBSCAN clustering algorithm"> ### 3.3.2 Time and Space Complexity</p>
<p>In the worst case, the time complexity of the DBSCAN algorithm is <span class="math inline">O(n^{2})</span>, where <span class="math inline">n</span> is the number of data points, since DBSCAN visits each point of the data set. In <em>Sci-kit Learn</em>, the DBSCAN implementation uses two tree data structures, namely, ball trees and kd-trees, to determine the neighborhood of points. In particular, the kd-tree allows for the efficient retrieval of all points within a given distance of a specified points, meaning the time complexity for the DBSCAN algorithm can be as low as <span class="math inline">O(nlogn)</span></p>
<p>The implementation of the DBSCAN algorithm in <em>Sci-kit Learn</em> consumes <span class="math inline">n^{2}</span> floats, but this can be reduced to <span class="math inline">O(n)</span> since it is only necessary to store the cluster label and what type of point each datum belongs to.</p>
</section>
</section>
<section id="internal-cluster-validity-indices" class="level1">
<h1>4. Internal Cluster Validity Indices</h1>
<p>Users often have to select the best number of clusters in a data set without some indication of what the correct number of clusters should be. In addition, different clustering algorithms can lead to different clusters of the data. In addition, using different parameters for the same algorithm can also lead to different clustering results.</p>
<p>Since the need for effective evaluation standards has been described, this section will focus on a variety of indices that can be used to aid the user in selecting an appropriate amount of clusters.</p>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">4.0 Notation:</h2>
<p>This section contains information on notations used throughout the rest of the paper.</p>
<p>Let us denote the following:</p>
<p><span class="math inline">N</span> = total number of observations in a dataset</p>
<p><span class="math inline">m</span> = the total number of numeric variables used to cluster the data</p>
<p><span class="math inline">K</span> = the total number of clusters</p>
<p><span class="math inline">X</span> = the data matrix of shape <span class="math inline">[N, m]</span></p>
<p><span class="math inline">C_{k}</span> = A submatrix of <span class="math inline">X</span> made up of the rows of <span class="math inline">X</span> where some partition, <span class="math inline">P_{i}</span>, contains any point <span class="math inline">X_{i}</span> belonging to cluster <span class="math inline">k</span></p>
<p><span class="math inline">n_{k}</span> = the number of observations belonging to cluster <span class="math inline">C_{k}</span></p>
</section>
<section id="ball-hall-index" class="level2">
<h2 class="anchored" data-anchor-id="ball-hall-index">4.1 <a name="bhindex"></a>Ball-Hall Index</h2>
<p>This index is based on the average distance of the items to their respective cluster centroids. It is just the within-group sum of distances and is computed as:</p>
<p><span class="math display">BH = \frac{W_k}{K} \tag{3}</span></p>
<p>where <span class="math inline">W_k = \displaystyle \sum ||x_i - \bar{x_i}||</span> is the within-group dispersion matrix for the data clustered into <span class="math inline">K</span> clusters. This index requires the computation of centroids and <span class="math inline">N</span> within-group distances, which itself is <span class="math inline">O(mN)</span>. Thus, Ball-Hall is <span class="math inline">O(mN)</span> where <span class="math inline">m</span> is the number of variables in the data set to be clustered.</p>
<p>The maximum difference in the Ball-Hall indices between different clusters is considered to be the optimal number of clusters.</p>
</section>
<section id="the-beale-index" class="level2">
<h2 class="anchored" data-anchor-id="the-beale-index">4.2 The Beale Index</h2>
<p><a href="#Beale">Beale, 1969</a> introduced the idea of using an F-ratio to test the hypothesis of the existence of <span class="math inline">q_1</span> versus <span class="math inline">q_2</span> clusters in the data (<span class="math inline">q_2 &gt; q_1</span>). The optimal number of clusters is obtained by comparing <span class="math inline">F</span> with an <span class="math inline">F_{p, (n_m-2)p}</span> distribution, where the null hypothesis would be rejected for significantly large values of the F-statistic. In the proposed Python package, the user can control the <span class="math inline">\alpha</span> level used to compute the critical <span class="math inline">F</span> value, and the default value is <span class="math inline">0.10</span>.</p>
<p>Mathematically, the calculation of the Beale index is as follows:</p>
<p><span class="math display">Beale = F = \frac {\frac {V_{kl}}{W_k + W_l}}{(\frac {n_m-1}{n_m-2})*2^{\frac {2}{p}}-1} \tag{4}</span></p>
<p>where <span class="math inline">V_{kl} = W_m - W_k - W_l</span> and <span class="math inline">p</span> is the number of variables.</p>
</section>
<section id="the-c-index" class="level2">
<h2 class="anchored" data-anchor-id="the-c-index">4.3 The C Index</h2>
<p>The C index was reviewed by <a href="#C_Index">Hubert and Levin, 1976</a> and is calculated below:</p>
<p><span class="math display">C_{index} = \frac {S_w - S_{min}}{S_{max} - S_{min}}, C_{index} \in (0, 1) \tag{5}</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">S_{min}</span> = the sum of the <span class="math inline">N_W</span> smallest distances between all the pairs of points in the entire data set;</p></li>
<li><p><span class="math inline">S_{max}</span> = the sum of the <span class="math inline">N_W</span> largest distances between all the pairs of points in the entire data set.</p></li>
<li><p><span class="math inline">N_{W}</span> = <span class="math inline">\displaystyle \sum_{k=1}^{K} \frac {n_k(n_k - 1)}{2}</span></p></li>
</ul>
<p>The optimal number of clusters is indicated by the minimum value of the C index. Finally, the C-index has a time complexity of <span class="math inline">O(m^{2}(n + log_2m))</span>, mostly due to the computational cost of sorting the <span class="math inline">N_W</span> pairwise distances, which can be slow with a large number of observations.</p>
</section>
<section id="the-calinski-harabasz-index" class="level2">
<h2 class="anchored" data-anchor-id="the-calinski-harabasz-index">4.4 The Calinski-Harabasz Index</h2>
<p>The Calinski-Harabasz index <a href="#CH">Calinski, Harabasz, 1974</a>, also known as the Variance Ratio Criterion, is a measure of how similar an object is to its own cluster compared to other clusters. From <em>Sci-kit Learn’s</em> website, the score is defined as the ratio between the within-cluster dispersion and between-cluster dispersion. More formally put:</p>
<p><span class="math display">CH_q = \frac{trace(B_k)/(K-1)}{trace(W_k)/(N-K)} \tag{6}</span></p>
<p>where: <span class="math display">W = \displaystyle \sum W_k \tag{7}</span></p>
<p>and: <span class="math display">W_k = \displaystyle \sum (x_i - \bar{x_m})(x_i - \bar{x_m})^{T} \tag{8}</span></p>
<p>and: <span class="math display">trace(W_k) = \displaystyle \sum_{k=1}^n \sum (x_{ip} - \bar{x_{lp}})^{2} \tag{9}</span>,</p>
<p>where <span class="math inline">x_{ip}</span> is the <span class="math inline">p^{th}</span> attribute of the <span class="math inline">i^{th}</span> data object and <span class="math inline">\bar{x_{lp}}</span> is the <span class="math inline">p^{th}</span> attribute of the centroid of the <span class="math inline">l^{th}</span> cluster.</p>
<p>The maximum value of the CH score, given a range of clusters, is considered to be the best number of clusters. The Calinski-Harabasz index also has <span class="math inline">O(mN)</span> complexity.</p>
</section>
<section id="cubic-clustering-criterion" class="level2">
<h2 class="anchored" data-anchor-id="cubic-clustering-criterion">4.5 Cubic Clustering Criterion</h2>
<p>The CCC by <a href="#CCC">Sarle, 1983</a> is obtained by comparing the observed <span class="math inline">R^2</span> to the approximate expected <span class="math inline">R^2</span> using an approximate variance-stabilizing transformation. CCC values greater than 0 mean that the obtained <span class="math inline">R^2</span> is greater than would be expected if sampling from a uniform distribution. According to the SAS Institute, the best way to use the CCC is to plot its values against the number of clusters. The CCC is used to compare the <span class="math inline">R^{2}</span> you obtain for a given set of clusters with the <span class="math inline">R^{2}</span> you would get by clustering a uniformly/normally distributed set of points in <span class="math inline">p</span> dimensional space. In addition, the optimal number of clusters is indicated by the maximum CCC value. Mathematically, the CCC can be computed as follows:</p>
<p><span class="math display">CCC = ln[\frac {1 - E[R^{2}]}{1 - R^{2}}] * \frac {\sqrt{\frac {np}{2}}}{(0.001 + E[R^{2}])^{2}} \tag{10}</span></p>
<p>where,</p>
<p><span class="math display">R^{2} = 1 - \frac {p  + \sum u^{2}_j}{\sum u^{2}_j} \tag{11}</span></p>
<p>and,</p>
<p><span class="math display">E[R^{2}] = 1 - \displaystyle \frac {\sum_{j=1}^{p} \frac {1}{n + u_j} + \sum_{j=p+1}^{p} \frac {u^{2}_j}{n + u_j}}{\displaystyle \sum^{p}_{j=1} u^{2}_{j}} * [\frac {(n - k)^{2}}{n}] * [1 + \frac {4}{n}] \tag{12}</span></p>
<p>and,</p>
<p><span class="math display">u_j = \frac {s_j}{c} \tag{13}</span></p>
<p>and,</p>
<p><span class="math display">c = (\frac {v}{k})^{\frac {1}{p}} \tag{14}</span></p>
<p>and,</p>
<p><span class="math display">v = \displaystyle \prod_{j=1}^{p} s_{j} \tag{15}</span></p>
<p>and,</p>
<p><span class="math inline">p</span> is the largest integer less than <span class="math inline">k</span> such that <span class="math inline">u_p</span> is not less than one.</p>
<p>Finally, the reason for the odd ending to the expected r-squared formula was derived empirically to stabilize the variance across different combinations of observations, variables, and clusters.</p>
</section>
<section id="d-index" class="level2">
<h2 class="anchored" data-anchor-id="d-index">4.6 D Index</h2>
<p>The D Index from <a href="#DIndex">Lebart et al., 2000</a> is based on clustering gain on intra-cluster inertia, which measures the degree of homogeneity between the data associated with a cluster. It calculates their distances compared to the cluster centroid.</p>
<p>Mathematically, intra-cluster inertia is defined as:</p>
<p><span class="math display">W(P^k) = \frac {1}{k} \displaystyle \sum_{k=1}^{K} \frac {1}{n_k} \sum_{i=1}^{n_k} d(x_i, c_k) \tag{16}</span></p>
<p>The clustering gain on the intra-cluster inertia, given two partitions, <span class="math inline">P^{k-1}</span> composed of <span class="math inline">k-1</span> clusters and <span class="math inline">P^k</span> composed of <span class="math inline">k</span> clusters, is defined as:</p>
<p><span class="math display">Gain = W(P^{k-1}) - W(P^{k}) \tag{17}</span></p>
<p>The clustering gain above should be minimized. In addition, the optimal suggested number of clusters corresponds to a significant decrease of the first different of clustering gain, compared to the number of clusters. This “significant decrease” can be identified by examining the second differences of clustering gain.</p>
</section>
<section id="davies-bouldin-index" class="level2">
<h2 class="anchored" data-anchor-id="davies-bouldin-index">4.7 Davies-Bouldin Index</h2>
<p>The <a href="#DBIndex">Davies-Bouldin Index</a> is a function of the sum ratio of within-cluster scatter to between-cluster separation. A lower value indicates a better clustering solution, i.e.&nbsp;better separation between the clusters.</p>
<p>Mathematically, the index is defined as the average similarity between each cluster and its most similar one, where the similarity is defined as a measure <span class="math inline">R_{ij}</span> that is nonnegative and symmetric, like so:</p>
<p><span class="math display">R_{ij} = \frac {s_i + s_j}{d_{ij}} \tag{18}</span></p>
<p>Then the Davies-Bouldin index is defined as:</p>
<p><span class="math display">DB = \frac {1}{k} \displaystyle \sum_{i=1}^{k}max(R_{ij}) \tag{19}</span></p>
<p>Finally, in terms of computational complexity, the Davies-Bouldin index runs in <span class="math inline">O(m(k^2 + N))</span> time.</p>
</section>
<section id="duda-index" class="level2">
<h2 class="anchored" data-anchor-id="duda-index">4.8 Duda Index</h2>
<p>The Duda index, <a href="#Duda">Duda and Hart, 1973</a> is the ratio between the sum of squared errors within clusters when the data are partitioned into two clusters, and the squared errors when only one cluster is present.</p>
<p>Mathematically, this can be represented as:</p>
<p><span class="math display">Duda = \frac {W_k + W_l}{W_m} \tag{20}</span></p>
<p>The optimal number of suggested clusters is the smallest <span class="math inline">k</span> such that:</p>
<p><span class="math display">Duda \ge 1 - \frac {2}{\pi p} - 3.20 \sqrt{\frac {2(1 - \frac {8}{\pi^{2}p}}{n_m p}} \tag{21}</span></p>
<p>where 3.20 is a Z-score tested by <a href="#MilliganCooper">Milligan and Cooper, 1985</a></p>
</section>
<section id="dunn-index" class="level2">
<h2 class="anchored" data-anchor-id="dunn-index">4.9 Dunn Index</h2>
<p>The Dunn index <a href="#Dunn">(Dunn, 1974)</a> is the ratio between the minimal intercluster distance to the maximal intracluster distance. If the data set contains well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters should be large. Thus, the suggested optimal number of clusters corresponds to the <strong>maximum</strong> value of the Dunn index.</p>
<p>Mathematically:</p>
<p><span class="math display">Dunn = \frac {d_{min}}{d_{max}} \tag{22}</span></p>
<p>where:</p>
<p><span class="math display">d_{min} = \min{(\min{||M_{i}^{k} - M_{j}^{k'}||})} \tag{23}</span></p>
<p>and,</p>
<p><span class="math display">d_{max} = \max{(\max{||M_{i}^{k} - M_{j}^{k}||})} \tag{24}</span></p>
<p>and <span class="math inline">d_{max}</span> is known as the diameter of a cluster, or the largest distance separating two distinct points in a particular cluster.</p>
<p>In terms of computational complexity, the Dunn index runs in <span class="math inline">O(mN^{2})</span> time.</p>
</section>
<section id="frey-index" class="level2">
<h2 class="anchored" data-anchor-id="frey-index">4.10 Frey Index</h2>
<p>The Frey index,<a href="#Frey">proposed by Frey and Van Groenewoud, 1972</a>, is the ratio of the difference scores from two successive levels in a hierarchy. This means that the index should only be used on hierarchical clustering method. The numerator represents the difference between the mean between-cluster distances while the denominator represents the difference between the mean within-cluster distances, each from the two levels.</p>
<p>Mathematically, the Frey index can be written as:</p>
<p><span class="math display">Frey = \frac {\bar{S_{b_{j+1}}} - \bar{S_{b_j}}}{\bar{S_{w_{j+1}}} - \bar{S_{w_j}}} \tag{25}</span></p>
<p>where the mean between-cluster distance is:</p>
<p><span class="math display">\bar{S_b} = S_b / N_b \tag{26}</span></p>
<p>and the mean within-cluster distance is:</p>
<p><span class="math display">\bar{S_w} = S_w / N_w \tag{27}</span></p>
<p>The best clustering solution occurs when the ratio falls below 1.0, and the cluster level before that point is taken as the optimal partition. In addition, if the ratio never falls below 1.0, a one cluster solution is usually best.</p>
</section>
<section id="gamma-index" class="level2">
<h2 class="anchored" data-anchor-id="gamma-index">4.11 Gamma Index</h2>
<p>The Gamma Index compares all within-cluster dissimilarities and all between-cluster dissimilarities. A comparison is concordant/discordant if a within-cluster dissimilarity is less/greater than a between cluster dissimilarity. Ties are disregarded. Also, the maximum value of the index indicates the optimal suggested number of clusters.</p>
<p>Mathematically, the Gamma Index is defined as:</p>
<p><span class="math display">Gamma = \frac {s(+) - s(-)}{s(+) + s(-)} \tag{28}</span></p>
<p>where,</p>
<p><span class="math inline">s(+)</span> is the number of concordant comparisons</p>
<p>and,</p>
<p><span class="math inline">s(-)</span> is the number of discordant comparisons</p>
<p>Furthermore, the definition of concordant and discordant pairs is defined mathematically below:</p>
<p><span class="math display">s(+) = \frac {1}{2} \displaystyle \sum \sum \frac {1}{2} \sum \sum \delta(||x_i - x_j|| &lt; ||x_p - x_k||) \tag{29}</span></p>
<p>and,</p>
<p><span class="math display">s(-) = \frac {1}{2} \displaystyle \sum \sum \frac {1}{2} \sum \sum \delta(||x_i - x_j|| &gt; ||x_p - x_k||) \tag{30}</span></p>
<p>where,</p>
<p><span class="math inline">\delta(\cdot) = 1</span> if the corresponding inequality is satisfied and <span class="math inline">\delta(\cdot) = 0</span> otherwise.</p>
<p>In terms of computational complexity, this index requires the computation of all pairwise distances between objects, which itself is <span class="math inline">O(mN^{2})</span>. In addition, computing <span class="math inline">s(+)</span> and <span class="math inline">s(-)</span> requires <span class="math inline">O(\frac {N^{4}}{k})</span> time, so the total computational cost of the Gamma index is <span class="math inline">O(mN^{2} + \frac {N^{4}}{k})</span>. This index should almost never be used for large data sets as it is very resource heavy. The performance of this particular index, along with the G+ and Tau indices, will be discussed in the performance section later on.</p>
</section>
<section id="g-plus-index" class="level2">
<h2 class="anchored" data-anchor-id="g-plus-index">4.12 G-Plus Index</h2>
<p>The G+ index is another metric based on the relationships between just the discordant pairs of objects, <span class="math inline">s(-)</span>. It is simply the proportion of discordant pairs with respect to the maximum number of possible comparisons. The optimal number of clusters in the data is given by the minimum value of the index.</p>
<p>Mathematically, the index can be computed as:</p>
<p><span class="math display">G+ = \frac {2s(-)}{N_t * (N_t - 1)} \tag{31}</span></p>
<p>The computational run time of the G+ index is also <span class="math inline">O(mN^{2} + \frac {N^{4}}{k})</span> and as such should not be used for larger data sets.</p>
</section>
<section id="hartigan-index" class="level2">
<h2 class="anchored" data-anchor-id="hartigan-index">4.13 <a name="hartindex"></a>Hartigan Index</h2>
<p>The Hartigan index is based on the Euclidean within-cluster sums of squares. The optimal number of clusters is given by the maximum difference between successive clustering solutions.</p>
<p>Mathematically, it can be expressed as:</p>
<p><span class="math display">Hartigan = (W_k / W_{k+1}) * (N - K - 1) \tag{32}</span></p>
<p>where <span class="math inline">W_k = \displaystyle \sum ||x_i - \bar{x_i}||</span> is the within-cluster dispersion matrix for the data clustered into <span class="math inline">K</span> clusters</p>
<p>The computational run time of the Hartigan index is <span class="math inline">O(n(k^{2} + m))</span> and is discussed in further detail ahead.</p>
</section>
<section id="log-ss-ratio-index" class="level2">
<h2 class="anchored" data-anchor-id="log-ss-ratio-index">4.14 <a name="logssindex"></a>Log SS Ratio Index</h2>
<p>This index, from <a href="https://people.inf.elte.hu/fekete/algoritmusok_msc/klaszterezes/John%20A.%20Hartigan-Clustering%20Algorithms-John%20Wiley%20&amp;%20Sons%20(1975).pdf">J. A. Hartigan. Clustering algorithms. New York: Wiley, 1975.</a> considers the within (<span class="math inline">WGSS</span>) and between (<span class="math inline">BGSS</span>) cluster sums of squares. It should be noted that the log used is base <span class="math inline">10</span>, not <span class="math inline">e</span>.</p>
<p>The time complexity of this index is, at worst, <span class="math inline">O(n(k^{2} + m))</span> and at best <span class="math inline">O(nm)</span> when <span class="math inline">k^{2} &lt;&lt; m</span>. Thus, this index is ideal for data with naturally large clusters.</p>
</section>
<section id="marriot-index" class="level2">
<h2 class="anchored" data-anchor-id="marriot-index">4.15 Marriot Index</h2>
<p>The Marriot Index <a href="#Marriot">Marriot, 1971</a> is based on the determinant of the within-group covariance matrix.</p>
<p>Mathematically, it is computed as:</p>
<p><span class="math display">k^{2} * det(W_{k}) \tag{33}</span></p>
<p>The suggested optimal number of clusters is based on the maximum difference between successive levels of the Marriot index. The overall computational complexity of the Marriot index is <span class="math inline">O(m^{2}N + m^{3})</span>, making it suitable for large datasets with a medium number of dimensions.</p>
</section>
<section id="mcclain-index" class="level2">
<h2 class="anchored" data-anchor-id="mcclain-index">4.16 McClain Index</h2>
<p>The McClain and Rao index <a href="#McClain">McClain and Rao, 1975</a> is the ratio of the average within cluster distance - divided by the number of within cluster distances - and the average value between cluster distances - divided by the number of cluster distances.</p>
<p>Mathematically, it is formed by:</p>
<p><span class="math display">McClain = \frac {\bar{S_w}}{\bar{S_b}}  = \frac {S_w / N_w}{S_b / N_b} \tag{34}</span></p>
<p>The optimal suggested number of clusters corresponds to the minimum value of the index. Finally, in terms of computational complexity, the McClain Rao index runs in <span class="math inline">O(mN^{2})</span> time.</p>
</section>
<section id="point-biserial-index" class="level2">
<h2 class="anchored" data-anchor-id="point-biserial-index">4.17 <a name="pbindex"></a>Point-Biserial Index</h2>
<p>The Point Biserial index is the point-biserial correlation between the dissimilarity matrix and a corresponding matrix consisting of either 0’s or 1’s. A value of <span class="math inline">0</span> is assigned if the two corresponding points are clustered together by the algorithm and <span class="math inline">1</span> otherwise.</p>
<p>Mathematically, the index is defined as:</p>
<p><span class="math display">PB = \displaystyle \frac {[\bar{S_b} - \bar{S_w}] * \sqrt{[N_w*N_b/N_t^2]}}{s_d} \tag{35}</span></p>
<p>where,</p>
<p><span class="math display">\bar{S_w} = S_{w} / N_{w} \tag{36}</span></p>
<p>and,</p>
<p><span class="math display">\bar{S_b} = S_b / N_b \tag{37}</span></p>
<p>and,</p>
<p><span class="math display">s_d \tag{38}</span></p>
<p>is the standard deviation over all of the distances</p>
<p>and,</p>
<p><span class="math inline">N_k</span> = The number of data points in each cluster, so an array of length <span class="math inline">K</span></p>
<p><span class="math inline">N_{T}</span> = <span class="math inline">\frac {N*(N-1)}{2}</span> = Total number of pairs of distinct points in the data set.</p>
<p><span class="math inline">N_{W}</span> = <span class="math inline">\displaystyle \frac {n_k * (n_k - 1)}{2}</span> = The number of paris of points belonging to clusters <span class="math inline">k</span> and <span class="math inline">k'</span>.</p>
<p><span class="math inline">N_{B}</span> = <span class="math inline">\displaystyle \sum_{k&lt;k'} {n_k * (n_{k'})}</span> = The number of pairs of points belonging to clusters <span class="math inline">k</span> and <span class="math inline">k'</span> where <span class="math inline">k</span> and <span class="math inline">k'</span> are not the same cluster.</p>
<p>In terms of computational complexity, the Point-Biserial index takes <span class="math inline">O(nm^{2})</span> time to be computed.</p>
</section>
<section id="pseudo-t2-index" class="level2">
<h2 class="anchored" data-anchor-id="pseudo-t2-index">4.18 Pseudo T2 Index</h2>
<p>The Pseudo <span class="math inline">t^{2}</span> index from <a href="#PsuedoT2">Duda and Hart, 1973</a> can only be applied to hierarchical clustering methods, but is included for all clustering methods in the proposed Python package. This is because the user should understand that the metric should only be used for that specific method.</p>
<p>Mathematically, the index is computed as:</p>
<p><span class="math display">Pseudo-t^{2} = \frac {V_{kl}}{\frac {W_k + W_l}{n_k + n_l - 2}} \tag{39}</span></p>
<p>The suggested optimal number of clusters is based on the smallest <span class="math inline">q</span> such that:</p>
<p><span class="math display">Pseudo-t^{2} \le (\displaystyle \frac {1 - CritValue}{CritValue}) \times (n_k + n_l -2) \tag{40}</span></p>
</section>
<section id="ratkowsky-lance-index" class="level2">
<h2 class="anchored" data-anchor-id="ratkowsky-lance-index">4.19 Ratkowsky-Lance Index</h2>
<p>The <a href="#RLIndex">Ratkowsky and Lance, 1978</a> index is based on the average of the ratio between the between-group sum of squares and the total sum of squares for each variable.</p>
<p>Mathematically, it is computed as:</p>
<p><span class="math display">RL = \frac {\bar{S}}{K^{1/2}} \tag{41}</span></p>
<p>where,</p>
<p><span class="math display">\bar{S^{2}} = \frac {1}{p}\displaystyle \sum_{j=1}^{p} \frac {BGSS_j}{TSS_j} \tag{42}</span></p>
<p>and,</p>
<p><span class="math display">BGSS_j = \displaystyle \sum_{k=1}^{m}n_k*(c_{kj} - \bar{x_j})^{2} \tag{43}</span></p>
<p>and,</p>
<p><span class="math display">TSS_j = \displaystyle \sum_{i=1}^{n} (x_{ij} - \bar{x_j})^{2} \tag{44}</span></p>
<p>The optimal number of clusters is suggested by the maximum value of the index. Finally, the index runs in <span class="math inline">O(m(k^{2} + N))</span> time.</p>
</section>
<section id="ray-turi-index" class="level2">
<h2 class="anchored" data-anchor-id="ray-turi-index">4.20 Ray-Turi Index</h2>
<p>This index, proposed by <a href="#Ray-Turi">Ray and Turi in 1999</a>, came off of the heels of image segmentation, which separates parts of images into differing components. The index is simply the ratio between the intra-cluster distance and the inter-cluster distance.</p>
<p>Mathematically, it can be written as:</p>
<p><span class="math display">RT = \frac {intra}{inter} \tag{45}</span></p>
<p>where:</p>
<p><span class="math display">intra = \frac {1}{N} \displaystyle \sum_{i=1}^{k} \sum ||x - c_{k}||^{2} \tag{46}</span></p>
<p>and,</p>
<p><span class="math display">inter = min(||c_{k} - c_{k+1}||^{2}) \tag{47}</span></p>
<p>and <span class="math inline">c_k</span> is the centroid of cluster <span class="math inline">C_{k}</span></p>
<p>Since the goal is to minimize the intra-cluster distance - the numerator - the optimal cluster partitioning corresponds to the minimum value of the Ray-Turi index.</p>
</section>
<section id="r-squared-index" class="level2">
<h2 class="anchored" data-anchor-id="r-squared-index">4.21 <a name="r2index"></a>R Squared Index</h2>
<p>While not included as part of the <em>NbClust</em> standard library, the <span class="math inline">R^{2}</span> index is inherently necessary to compute, since it is relied upon by the cubic clustering criterion, as mentioned above. The maximum difference between successive clustering levels is regarded as the best partition. The mathematical formulation is as follows:</p>
<p><span class="math display">R^{2} = 1 - \frac {p  + \sum u^{2}_j}{\sum u^{2}_j} \tag{48}</span></p>
<p><span class="math display">R^{2} = 1 - \frac {tr(X^{T}X - \bar{X}^{T}Z^{T}Z\bar{X})}{tr(X^{T}X)} \tag{49}</span></p>
<p>where:</p>
<p><span class="math display">\bar{X} = (Z^{T}Z)^{-1}Z^{T}X \tag{50}</span></p>
<p>and,</p>
<p><span class="math display">X^{T}X \tag{51}</span></p>
<p>is the total sum of squares and cross-products matrix of shape <span class="math inline">(p \times p)</span>.</p>
</section>
<section id="rubin-index" class="level2">
<h2 class="anchored" data-anchor-id="rubin-index">4.22 Rubin Index</h2>
<p>The Rubin Index, proposed by <a href="#Rubin">Friedman and Rubin in 1967</a>, is based on the ratio of the determinants of the data covariance matrix and within-group covariance matrix.</p>
<p>Mathematically, it can be formed like so:</p>
<p><span class="math display">Rubin = \frac {det(T)}{det(W_k)} \tag{52}</span></p>
<p>The optimal clustering solution is given by the minimum value of the second differences between clustering levels. In terms of computational complexity, the Rubin index runs in <span class="math inline">O(m^{2}N + m^{3})</span> time.</p>
</section>
<section id="scott-symons-index" class="level2">
<h2 class="anchored" data-anchor-id="scott-symons-index">4.23 Scott-Symons Index</h2>
<p>The Scott-Symons index, <a href="#Scott-Symons">Scott and Symons, 1971</a>, uses information from the determinants of the data covariance matrix and the within-group covariance matrix, similar to the Rubin index.</p>
<p>Mathematically, it can be written as:</p>
<p><span class="math display">n*log(\frac {det(T)}{det(W_k)}) \tag{53}</span></p>
<p>where:</p>
<p><span class="math display">T = \displaystyle \sum_{i=1}^{N} (x_i - \bar{x})(x_i - \bar{x})^{T}</span></p>
<p>and,</p>
<p><span class="math display">W_k = \displaystyle \sum (x_i - \bar{x_m})(x_i - \bar{x_m})^{T} \tag{54}</span></p>
<p>The optimal suggested number of clusters is given by the maximum value of the index. Finally, the index runs in <span class="math inline">O(m^{2}N + m^{3})</span> time.</p>
</section>
<section id="sd-index" class="level2">
<h2 class="anchored" data-anchor-id="sd-index">4.24 SD Index</h2>
<p>The SD validity index is based on the concepts of average scattering for clusters and total separation between clusters.</p>
<p>Mathematically, it is computed as:</p>
<p><span class="math display">SDindex(q) = \alpha*Scat(q) + Dis(q) \tag{55}</span></p>
<p>The scattering term, or the average compactness of clusters, is computed as:</p>
<p><span class="math display">Scat(q) = \frac {\frac {1}{q} \sum ||\sigma^{k}||}{||\sigma||} \tag{56}</span></p>
<p>where:</p>
<p><span class="math display">Dis(q) = \frac {D_{max}}{D_{min}} \displaystyle \sum_{k=1}^{q} (\sum_{z=1}^{q}||c_k - c_z||)^{-1} \tag{57}</span></p>
<p>and <span class="math inline">D_{max}</span> is the maximum distance between cluster centers and <span class="math inline">D_{min}</span> is the minimum distance between cluster centers. In addition, <span class="math inline">\sigma</span> is the vector of variances for each variable in the data set and <span class="math inline">\sigma^{k}</span> is the variance vector for each cluster, <span class="math inline">C_k</span>.</p>
<p>It should be noted that <span class="math inline">\alpha</span> is a weighting factor equal to Dis(<span class="math inline">q_{max}</span>) where <span class="math inline">q_{max}</span> is the maximum number of clusters. The optimal suggested number of clusters is indicated by the minimum value of the SD index.</p>
</section>
<section id="sdbw-index" class="level2">
<h2 class="anchored" data-anchor-id="sdbw-index">4.25 SDbw Index</h2>
<p>The SDbw index is based on the compactness and separation between clusters.</p>
<p>Mathematically, it can be computed as:</p>
<p><span class="math display">SDbw(q) = Scat(q) + Density(q) \tag{58}</span></p>
<p>where:</p>
<p><span class="math display">Density(q) = \frac {1}{q(q-1)} \displaystyle \sum_{i=1}^{q} (\sum_{j=1}^{q} \frac {density(u_{ij})}{max(density(c_i), density(c_j)}) \tag{59}</span></p>
<p>and,</p>
<p><span class="math display">density(u_{ij}) = \displaystyle \sum_{l=1}^{n_{ij}} f(x_l, u_{ij}) \tag{60}</span></p>
<p>The <span class="math inline">Scat(q)</span> term is calculated in the same way as above. The new term - <span class="math inline">Density(q)</span> - is the inter-cluster density, which is the average density in the region among clusters in relation to the density of the clusters.</p>
<p>The optimal number of clusters is the partition that minimizes the SDbw index.</p>
</section>
<section id="silhouette-score" class="level2">
<h2 class="anchored" data-anchor-id="silhouette-score">4.26 Silhouette Score</h2>
<p>Introduced by <a href="#Silhouette">Rousseeuw in 1987</a>, the Silhouette Coefficient is composed of two scores:</p>
<ul>
<li><p><strong>a:</strong> The mean distance between a sample and all other points in the same class</p></li>
<li><p><strong>b:</strong> The mean distance between a sample and all other points in the next nearest cluster</p></li>
</ul>
<p>Mathematically, this can be written as:</p>
<p><span class="math display">s = \frac {b - a}{max(a, b)} \tag{61}</span></p>
<p>Additionally, the Silhouette Score for a set of sample is just the mean Silhouette Coefficient for each sample, like so:</p>
<p><span class="math display">Silhouette = \frac {\sum_{i=1}^{n} S(i)}{n} \tag{62}</span></p>
<p>where:</p>
<p><span class="math display">a = \frac {\sum d_{ij}}{n_r - 1} \tag{63}</span></p>
<p>and,</p>
<p><span class="math display">b = min(d_{iCs}) \tag{64}</span></p>
<p>and,</p>
<p><span class="math display">d_{iCs} = \frac {\sum d_{ij}}{n_s} \tag{65}</span></p>
<p>The optimal number of clusters is indicated by the maximum value of the index. Scores around <span class="math inline">0</span> indicate overlapping clusters. Finally, the index can be calculated in <span class="math inline">O(mN^{2})</span> time.</p>
</section>
<section id="tau-index" class="level2">
<h2 class="anchored" data-anchor-id="tau-index">4.27 Tau Index</h2>
<p>The Tau index is based on the <span class="math inline">\tau</span> correlation between the matrix that stores all of the distances between pairs of observations and a matrix that represents whether a given pair of observations belongs to the same cluster or not. It can be written in terms of concordant and discordant pairs of observations, just like the Gamma and G+ indices.</p>
<p>Mathematically, it can be written as:</p>
<p><span class="math display">\displaystyle \frac {s(+) - s(-)}{\sqrt{[(N_t (N_t - 1) / (2 - t)) * (N_t (N_t - 1) / 2)]}} \tag{66}</span></p>
<p>where <span class="math inline">N_t</span> is the total number of distances and <span class="math inline">t</span> is the number of comparisons between two pairs of observations.</p>
<p>The optimal suggested clustering solution is given by the maximum value of this index. In terms of computational complexity, the Tau index runs in the same time as the Gamma index; namely, <span class="math inline">O(mN^{2} + \frac {N^{4}}{k})</span> making it suitable for small datasets.</p>
</section>
<section id="trace-cov-w-index" class="level2">
<h2 class="anchored" data-anchor-id="trace-cov-w-index">4.28 Trace-Cov-W Index</h2>
<p>The Trace(Cov(W)) index involves using the pooled covariance matrix instead of the within-group covariance matrix.</p>
<p>Mathematically, it can be written as:</p>
<p><span class="math display">TraceCovW = trace(COV[W_k]) = trace(W_p) \tag{67}</span></p>
<p>where:</p>
<p><span class="math display">W_k = \displaystyle \sum \sum (x_i - c_k)(x_i - c_k)^{T} \tag{68}</span></p>
<p>and,</p>
<p><span class="math display">W_p = \frac {1}{N-k} * \displaystyle \sum_{i=1}^{k}W_i \tag{69}</span></p>
<p>and,</p>
<p><span class="math display">W_i = \displaystyle \sum(x_i - \bar{x_i})(x_i - \bar{x_i})^{T} \tag{70}</span></p>
<p>The optimal suggested number of clusters is indicated by the maximal difference in index scores. Finally, in terms of computational efficiency, the index can be calculated in <span class="math inline">O(mN)</span> time, making it ideal for larger data sets.</p>
</section>
<section id="trace-w-index" class="level2">
<h2 class="anchored" data-anchor-id="trace-w-index">4.29 Trace-W Index</h2>
<p>The Trace(W) index is a simple, difference-like criterion and is one of the most popular choices when selecting the appropriate number of clusters.</p>
<p>Mathematically, it can be calculated as:</p>
<p><span class="math display">Trace(W) = trace(W_k) \tag{71}</span></p>
<p>where <span class="math inline">W_k</span> is the within-group covariance matrix.</p>
<p>The optimal suggested number of clusters corresponds to the maximum value of the second differences of the index. Finally, in terms of computational complexity, the index runs in <span class="math inline">O(mN)</span> time, making it ideal for large data sets.</p>
</section>
<section id="wemmert-gancarski-index" class="level2">
<h2 class="anchored" data-anchor-id="wemmert-gancarski-index">4.30 <a name="wgindex"></a>Wemmert-Gancarski Index</h2>
<p>The Wemmert-Gancarski index, is simply the weighted mean of the quotient of distances between a set of points and the barycenter of the cluster those points belong to.</p>
<p>Mathematically, it can be calculated as:</p>
<p><span class="math display">WG = \displaystyle \frac {1}{N} \sum_{k=1}^{K} \max{(0, n_k - \sum_{i \in I_k} R(M_i))} \tag{72}</span></p>
<p>where:</p>
<p><span class="math display">R(M_i) = \displaystyle \frac {||M_i - G^{k}||}{\displaystyle \min_{k \ne k'} ||M_i - G^{k'}||} \tag{73}</span></p>
<p>and,</p>
<p><span class="math display">n_k = |x_{k}| \tag{74}</span></p>
<p>is the cardinality / number of points in a particular cluster</p>
<p>and:</p>
<p><span class="math display">\displaystyle \sum_{k=1}^{K} n_k = N \tag{75}</span></p>
</section>
<section id="xie-beni-index" class="level2">
<h2 class="anchored" data-anchor-id="xie-beni-index">4.31 <a name="xbindex"></a>Xie-Beni Index</h2>
<p>The Xie-Beni index is an index primarily applied to fuzzy clustering solutions. It is defined as the quotient between the mean quadratic error and the minimum of the minimal squared distances between the points in the clusters.</p>
<p>Mathematically, the index can be calculated as:</p>
<p><span class="math display">XB = \frac {1}{N} * \frac {WGSS}{min(D(C_k, C_{k'})^{2})} \tag{76}</span></p>
<p>where:</p>
<p><span class="math display">D(C_k, C_{k'}) = min(d(M_i, M_j)) \tag{77}</span></p>
<p>and <span class="math inline">d(M_i, M_j)</span> is the distance between the cluster centroids.</p>
<p>The optimal suggested number of clusters corresponds to the minimum value of the index.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> sklearn.cluster</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> scipy.cluster</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> sklearn.datasets</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="im">import</span> time</span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> pdist, cdist, euclidean</span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer</span>
<span id="cb3-15"><a href="#cb3-15"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="dv">30</span>)</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a>sns.set_context(<span class="st">'poster'</span>)</span>
<span id="cb3-18"><a href="#cb3-18"></a>sns.set_palette(<span class="st">'Paired'</span>, <span class="dv">10</span>)</span>
<span id="cb3-19"><a href="#cb3-19"></a>sns.set_color_codes()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Creating sample data-frames to cluster via <strong>NumPy:</strong></p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>dataset_sizes <span class="op">=</span> np.hstack([np.arange(<span class="dv">1</span>, <span class="dv">6</span>) <span class="op">*</span> <span class="dv">500</span>, np.arange(<span class="dv">3</span>,<span class="dv">7</span>) <span class="op">*</span> <span class="dv">1000</span>, np.arange(<span class="dv">4</span>,<span class="dv">17</span>) <span class="op">*</span> <span class="dv">2000</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Plotting the results:</p>
</section>
<section id="indices-performance" class="level2">
<h2 class="anchored" data-anchor-id="indices-performance"><strong>5.2 Indices Performance</strong></h2>
<p>This part will pertain to demonstrating the efficiency of the various clustering metrics offered by the proposed library. In addition, it will detail the various methods used to improve upon the efficiency of previous implementations, if they exist.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> benchmark_algorithm(dataset_sizes, indice_function, function_args, function_kwds,</span>
<span id="cb5-2"><a href="#cb5-2"></a>                        n_columns<span class="op">=</span><span class="dv">10</span>, n_clusters<span class="op">=</span><span class="dv">3</span>, max_time<span class="op">=</span><span class="dv">100</span>, repeats<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb5-3"><a href="#cb5-3"></a>    </span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="co"># * Initialize the result with NaNs so that any unfilled entries </span></span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span class="co"># * will be considered NULL when we convert to a pandas dataframe at the end</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>    result <span class="op">=</span> np.nan <span class="op">*</span> np.ones((<span class="bu">len</span>(dataset_sizes), repeats))</span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="cf">for</span> index, size <span class="kw">in</span> <span class="bu">enumerate</span>(dataset_sizes):</span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(repeats):</span>
<span id="cb5-9"><a href="#cb5-9"></a>            <span class="co"># * Use sklearns make_blobs to generate a random dataset with specified size</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>            <span class="co"># * dimension and number of clusters</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>            data, labels <span class="op">=</span> make_blobs(n_samples<span class="op">=</span>size, </span>
<span id="cb5-12"><a href="#cb5-12"></a>                                      n_features<span class="op">=</span>n_columns, </span>
<span id="cb5-13"><a href="#cb5-13"></a>                                      centers<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a></span>
<span id="cb5-15"><a href="#cb5-15"></a>            <span class="co"># * Start the clustering with a timer</span></span>
<span id="cb5-16"><a href="#cb5-16"></a>            start_time <span class="op">=</span> timer()</span>
<span id="cb5-17"><a href="#cb5-17"></a>            indice_function(data, labels, <span class="op">*</span>function_args, <span class="op">**</span>function_kwds)</span>
<span id="cb5-18"><a href="#cb5-18"></a>            time_taken <span class="op">=</span> np.<span class="bu">round</span>(timer() <span class="op">-</span> start_time, <span class="dv">4</span>)</span>
<span id="cb5-19"><a href="#cb5-19"></a>            <span class="bu">print</span>(<span class="ss">f"Run complete for size: </span><span class="sc">{</span>size<span class="sc">}</span><span class="ss">; took </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(time_taken, <span class="dv">4</span>)<span class="sc">}</span><span class="ss"> seconds"</span>)</span>
<span id="cb5-20"><a href="#cb5-20"></a>            <span class="co"># * If we are taking more than max_time then abort -- we don't</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>            <span class="co"># * want to spend excessive time on slow algorithms</span></span>
<span id="cb5-22"><a href="#cb5-22"></a>            <span class="cf">if</span> time_taken <span class="op">&gt;</span> max_time:</span>
<span id="cb5-23"><a href="#cb5-23"></a>                result[index, s] <span class="op">=</span> time_taken</span>
<span id="cb5-24"><a href="#cb5-24"></a>                <span class="cf">return</span> pd.DataFrame(np.vstack([dataset_sizes.repeat(repeats), </span>
<span id="cb5-25"><a href="#cb5-25"></a>                                               result.flatten()]).T, columns<span class="op">=</span>[<span class="st">'Size'</span>,<span class="st">'Seconds'</span>])</span>
<span id="cb5-26"><a href="#cb5-26"></a>            <span class="cf">else</span>:</span>
<span id="cb5-27"><a href="#cb5-27"></a>                result[index, s] <span class="op">=</span> time_taken</span>
<span id="cb5-28"><a href="#cb5-28"></a>        </span>
<span id="cb5-29"><a href="#cb5-29"></a>    <span class="co"># * Return the result as a dataframe for easier handling with seaborn afterwards</span></span>
<span id="cb5-30"><a href="#cb5-30"></a>    <span class="cf">return</span> pd.DataFrame(np.vstack([dataset_sizes.repeat(repeats), </span>
<span id="cb5-31"><a href="#cb5-31"></a>                                   result.flatten()]).T, columns<span class="op">=</span>[<span class="st">'Size'</span>,<span class="st">'Seconds'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="ball-hall-index-1" class="level3">
<h3 class="anchored" data-anchor-id="ball-hall-index-1">5.2.1 Ball-Hall Index:</h3>
<p>The <a href="#bhindex">Ball-Hall Index</a> is a relatively simple index that is essentially the mean dispersion of all points, relative to the cluster they belong to. As such, the index can be computed in <span class="math inline">O(nm)</span> time, making it ideal for larger datasets since it only requires computing the centroids of each cluster and the distances of each point within each cluster.</p>
<p>The author’s first attempt at implementing the index was unsuccessful, mostly due to a lack of understanding of the index. As a result, the initial run will not be used as a comparison and instead, only the <em>NbClust</em> version and the author’s new version will be compared.</p>
<p>Starting with the <em>NbClust</em> algorithm first, for a dataset with shape <span class="math inline">(10000, 5)</span> and <span class="math inline">K=3</span>, the mean time to compute the index was <strong>0.01027</strong> seconds for <span class="math inline">100</span> trial runs. In addition, the mean amount of resources used to compute the index was around <span class="math inline">14.2</span> MiB. In contrast, the new, working implementation included in the proposed library had a mean run time of <strong>0.00257</strong> seconds, using the same circumstances as the first experiment. In addition, the mean amount of memory used was around <span class="math inline">0.059</span> MiB, used to store the <span class="math inline">K</span> new, split dataframes from the for-loop.</p>
<p>All together, the new, proposed implementation offers about a <span class="math inline">9.5 \times</span> speedup and uses around <span class="math inline">0.41 \%</span> of the memory. Reasons for these improvements come mostly from <em>NbClust’s</em> use of the <strong>sweep</strong> function to compute the mean differences, as well as the multiple uses of the <strong>split</strong> function, which splits a dataframe into chunks.</p>
</section>
<section id="c-index" class="level3">
<h3 class="anchored" data-anchor-id="c-index">5.2.2 <span class="math inline">C</span> Index:</h3>
<p>The author does <strong>not</strong> believe the <em>NbClust</em> library calculates the <span class="math inline">C</span> index correctly. For instance, while the source code for the <span class="math inline">C</span> index correctly calculates the number of within-cluster distances, <span class="math inline">N_W</span>, it does not correctly compute the sum of the <span class="math inline">N_W</span> largest/smallest pairwise distances, <span class="math inline">S_{min}</span> and <span class="math inline">S_{max}</span> respectively. In addition, it does not utilize a sorting/partitioning method of any kind, which is necessary for finding the largest/smallest pairwise distances.</p>
<p>To alleviate this problem, I rewrote the index to account for all parts of the original equation, from <a href="#C_Index">Hubert and Levin, 1976</a>.</p>
<p>In terms of performance, the <em>NbClust</em> implementation will not be considered, for reasons listed above. It should be noted that a previous implementation of the <span class="math inline">C</span> index was made available in the Python language by John Vorsten in 2020, around the same time that I was working on this very project.</p>
<p>As stated previously, the <span class="math inline">C</span> index can be computed in <span class="math inline">O(m^{2}(n + log_2m))</span> time, which can be prohibitive if the number of variables used in the clustering is large. For reference, when <span class="math inline">N = 10,000</span>, the number of pairwise distances is <span class="math inline">49,995,000</span>; for <span class="math inline">N</span> observations, the number of pairwise distances is <span class="math inline">N(N-1)/2</span>. Using the above example, the mean time to compute the index via John’s implementation was <strong>27.5</strong> seconds, compared to the proposed library’s mean run-time of <strong>8.31</strong> seconds.</p>
<p>The largest bottleneck in computing this index is calculating <span class="math inline">S_{min}</span> and <span class="math inline">S_{max}</span>, because it requires - at minimum - <span class="math inline">O(n+ n\log N_{W})</span> time to locate the <span class="math inline">N_{W}</span> smallest/largest pairwise distances. The main difference between the proposed library’s implementation and John’s is the calculation of <span class="math inline">S_{min}</span> and <span class="math inline">S_{max}</span>. In particular, John’s approach uses <em>Numpy’s</em> <a href="https://numpy.org/doc/stable/reference/generated/numpy.sort.html#numpy.sort">sort</a> function, which by itself run’s in <span class="math inline">O(n^{2})</span> time, because the underlying algorithm used is a <strong>quicksort</strong>. However, the new library utilizes <em>Numpy’s</em> <a href="https://numpy.org/doc/stable/reference/generated/numpy.partition.html">partition</a> function, which runs in <span class="math inline">O(n)</span> time, via the <strong>introselect</strong> algorithm, and does not require sorting the <span class="math inline">N_{W}</span> largest/smallest arrays.</p>
</section>
<section id="dunn-index-1" class="level3">
<h3 class="anchored" data-anchor-id="dunn-index-1">5.2.3 Dunn Index:</h3>
<p>The Dunn index requires the computation of the entire pairwise distance matrix, just like the <span class="math inline">C</span> index above, and has a time complexity of <span class="math inline">O(N^{2}m)</span>. This becomes a problem as <span class="math inline">N</span> grows large, but can be dealt with by precomputing the distance matrix and storing it in memory to query when needed. This reduces the time complexity of the algorithm to just <span class="math inline">O(K^{2})</span>, where <span class="math inline">K</span> is the number of clusters under consideration.</p>
<p>In a more practical setting, for a dataset with dimensions <span class="math inline">(10000, 5)</span> and <span class="math inline">K = 3</span>, the mean time to compute the Dunn index, without precomputing the pairwise distances, is <span class="math inline">4.72</span> seconds via the proposed Python library. In contrast, the mean time to compute the same index with the same controls in place is <span class="math inline">10.84</span> seconds via the <em>NbClust</em> library. However, when utilizing a pre-computed distance matrix, the times fall to <span class="math inline">4.03</span> seconds and <span class="math inline">9.42</span> seconds for the proposed library and <em>NbClust</em> library, respectively.</p>
<p>The reasoning behind the <span class="math inline">O(K^{2})</span> solution lies behind the fact that in order find the maximum diameter / minimum intercluster distance, it requires a nested for-loop, which is used to go through each of the <span class="math inline">K</span> labels. Indexing an array takes <span class="math inline">O(1)</span> time, so that lookup is dominated by the outer loops in the expression.</p>
</section>
<section id="hartigan-index-1" class="level3">
<h3 class="anchored" data-anchor-id="hartigan-index-1">5.2.4 Hartigan Index:</h3>
<p>Hartigan is credited with two internal clustering indices, the <a href="#hartindex">Hartigan Index</a> and the <a href="#logssindex">Log Between/Within SS Index</a>. This section will focus on the Hartigan index mentioned in the <strong>NbClust</strong> library. It was previously mentioned that the index runs in <span class="math inline">O(n(k^{2} + m))</span> time, which can be quite fast when the number of requested clusters is small and the number of observations is also relatively small.</p>
<p>In the original implementation from the <strong>NbClust</strong> library, using the same setup as above, the index had a mean run-time of <span class="math inline">0.0286</span> seconds and used about <span class="math inline">0.92</span> MiB of memory to compute. In contrast, the proposed library had a mean run-time of <span class="math inline">0.0046</span> seconds and used about <span class="math inline">0.004</span> MiB of memory. For easier reference, the proposed library offers around a <span class="math inline">6 \times</span> performance gain while utilizing about <span class="math inline">230 \times</span> less memory.</p>
<p>It should be noted that both implementations perform fairly well on most datasets, so long as the dimensions of the dataset are of medium-size or lower.</p>
</section>
<section id="point-biserial-index-1" class="level3">
<h3 class="anchored" data-anchor-id="point-biserial-index-1">5.2.5 Point-biserial Index:</h3>
<p>As mentioned <a href="#pbindex">earlier</a>, the point-biserial index, referenced as the PB index from here on, runs in <span class="math inline">O(nm^{2})</span> time, where <span class="math inline">m</span> is the number of variables being clustered. However, an original implementation of the algorithm to compute the PB index runs in <span class="math inline">O(n^{2}m)</span> time, on average. For smaller inputs, this difference in run-time may be negligible; however, for larger datasets, the speed-up is quite dramatic. Using posterior analysis, the proposed library offers a speed-up in run-time up to 2600% (<span class="math inline">n=10,000</span>).</p>
<p>The main bottlenecks to the original implementation were, in decreasing order of execution time: the double for-loop to compute the distances and the design arrays (<span class="math inline">\mu</span> = 112.5 seconds), converting the design array to a factor (<span class="math inline">\mu</span> = 84 seconds), and garbage collection (<span class="math inline">\mu</span> = 43.5 seconds).</p>
<p>A few methods were employed to improve upon the efficiency of the R implementation. Firstly, the design vector was discarded in favor of computing the number of points within/between the given clusters (<span class="math inline">N_W, N_B</span>), which both run in <span class="math inline">O(k)</span> time instead of <span class="math inline">O(n^{2}m)</span> time, since the most tedious computation is counting the number of labels in each cluster. In addition, the biserial function used in NbClust’s version uses a separate code block to compute the biserial correlation between the design matrix and the distances, even though both libraries employ the following equality:</p>
<p><span class="math display">PB = s_n * r_{pb}(A, B) = [(S_W / N_W - S_B / N_B) * \displaystyle \frac {\sqrt{N_W * N_B}}{N_T}] / s_d \tag{78}</span></p>
</section>
<section id="r2-index" class="level3">
<h3 class="anchored" data-anchor-id="r2-index">5.2.6 <span class="math inline">R^{2}</span> Index:</h3>
<p>The <a href="#r2index"><span class="math inline">R^{2}</span> Index</a> has the usual interpretation of the amount of variation explained by the clustering solution. It should also be noted that the <span class="math inline">R^{2}</span> index is not the best method of criterion if your clustering solution is irregularly shaped or highly elongated.</p>
<p>Regarding the time complexity, the original implementation of the algorithm ran in <span class="math inline">O(k*n^{2}*m^{2})</span> time. This is because the algorithm has to, for each cluster <span class="math inline">k</span> and for each variable <span class="math inline">m</span>: compute the euclidean distance between the cluster center and the observation <span class="math inline">n</span>, write the product to an array, and then sum the result. The reasoning for the <span class="math inline">O(n^{2})</span> in the computation is because the euclidean distance requires <span class="math inline">O(nm)</span> time to be computed. For datasets with lots of observations and many variables to cluster, this time can be unfeasible to work with.</p>
<p>The new algorithm offers a <span class="math inline">812x</span> speedup in compute time, since it does not rely on a double for-loop or the computation of euclidean distances. Rather, it takes the raw data matrix and labels and computes a series of matrix multiplications.</p>
</section>
<section id="wemmert-gancarski-index-1" class="level3">
<h3 class="anchored" data-anchor-id="wemmert-gancarski-index-1">5.2.7 Wemmert-Gancarski Index:</h3>
<p>The <a href="#wgindex">Wemmert-Gancarski Index</a> was an interesting metric since there was not a lot of literature regarding this index. For reference, the main source for the mathematical formulation of this index was the clusterCrit library vignette.</p>
<p>Initially, the index seemed a bit daunting since a plethora of methods were tried but none of them worked out. Instead, it required a pivot in thinking about how to structure and implement; rather than thinking about building the index in terms of what the output of each step should be, a shape-first approach was taken. More specifically, there were 5 steps to successfully implement the index using the approach of output shapes rather than just the output itself. A diagram of the author’s approach is shown below for easier reference:</p>
</section>
<section id="wg-index" class="level3">
<h3 class="anchored" data-anchor-id="wg-index">WG Index:</h3>
<iframe src="https://drive.google.com/file/d/1hCsoy6wW3fzN05MJmcvlS8xuaGJjeiai/preview" width="1400" height="480" allow="autoplay" align="center"></iframe>
<p>Regarding the computational cost of computing the WG index, the modified OpenEnsembles version of the WG index had a mean run time of <span class="math inline">2.74</span> seconds; this is in stark contrast to the author’s proposed implementation that runs in <span class="math inline">0.118</span> seconds. The proposal is not only more than <span class="math inline">23</span> times faster than the previous implementation, but it also uses less memory as well (not a significant difference). Using the same setup as the other indices previously described, the mean amount of memory used per run for the new algorithm was around <span class="math inline">0.578</span> MiB, compared to the original implementation’s <span class="math inline">0.622</span> MiB memory usage; or <span class="math inline">8\%</span> less memory.</p>
<p>It should be noted that there were two resource-demanding tasks in the proposed implementation: namely, finding a quotient and finding the distances from each point to its centroid. Finding the quotient between the distance from a point to its barycenter and the minimum distance from a point and all of the other barycenters took, on average, only <span class="math inline">0.0013</span> seconds. Finding the distances from all of the points to their centroids resulted in a mean run time of about <span class="math inline">0.003</span> seconds, using the same setup as above. Finally, the same tasks that were bottlenecks for the new version were also present in the old implementation, but the key difference lies in how those obstacles were handled.</p>
</section>
<section id="xie-beni-index-1" class="level3">
<h3 class="anchored" data-anchor-id="xie-beni-index-1">5.2.8 Xie-Beni Index:</h3>
<p>The <a href="#xbindex">Xie-Beni Index</a> defines the inter-cluster separation as the minimum square distance between cluster centers, and the intra-cluster compactness as the mean square distance between each data object and its cluster center.</p>
<p>In terms of computational complexity, the original implementation of the Xie-Beni index ran in <span class="math inline">O(k*n^{2}*m^{2})</span> time. With similar reasoning as the <span class="math inline">R^{2}</span> index, this can be costly to run for larger datasets, especially if the true clustering solution is not fuzzy, which this index was specifically created for. The updated implementation of the index runs in <span class="math inline">O(n^{2})</span> time, necessary for the computation of the distance matrix. In addition, the new algorithm offers up to a <span class="math inline">291x</span> speedup compared to the author’s previous application.</p>
</section>
<section id="speed-testing-the-xie-beni-index" class="level3">
<h3 class="anchored" data-anchor-id="speed-testing-the-xie-beni-index">Speed-testing the Xie-Beni Index:</h3>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Old version:</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">def</span> oldXB(X, labels):</span>
<span id="cb6-3"><a href="#cb6-3"></a>    X <span class="op">=</span> pd.DataFrame(X)</span>
<span id="cb6-4"><a href="#cb6-4"></a>    numCluster <span class="op">=</span> <span class="bu">int</span>(<span class="bu">max</span>(labels) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>    numObject <span class="op">=</span> <span class="bu">len</span>(labels)</span>
<span id="cb6-6"><a href="#cb6-6"></a>    sumNorm <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-7"><a href="#cb6-7"></a>    list_centers <span class="op">=</span> []</span>
<span id="cb6-8"><a href="#cb6-8"></a></span>
<span id="cb6-9"><a href="#cb6-9"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(numCluster):</span>
<span id="cb6-10"><a href="#cb6-10"></a>        <span class="co"># get all members from cluster i</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>        indices <span class="op">=</span> [t <span class="cf">for</span> t, x <span class="kw">in</span> <span class="bu">enumerate</span>(labels) <span class="cf">if</span> x <span class="op">==</span> i]</span>
<span id="cb6-12"><a href="#cb6-12"></a>        clusterMember <span class="op">=</span> X.iloc[indices, :]</span>
<span id="cb6-13"><a href="#cb6-13"></a>        <span class="co"># compute the cluster center</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>        clusterCenter <span class="op">=</span> np.mean(clusterMember, <span class="dv">0</span>)</span>
<span id="cb6-15"><a href="#cb6-15"></a>        list_centers.append(clusterCenter)</span>
<span id="cb6-16"><a href="#cb6-16"></a>        <span class="co"># iterate through each member of the cluster</span></span>
<span id="cb6-17"><a href="#cb6-17"></a>        <span class="cf">for</span> member <span class="kw">in</span> clusterMember.iterrows():</span>
<span id="cb6-18"><a href="#cb6-18"></a>            sumNorm <span class="op">=</span> sumNorm <span class="op">+</span> np.power(euclidean(member[<span class="dv">1</span>], clusterCenter), <span class="dv">2</span>)</span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a>    minDis <span class="op">=</span> <span class="bu">min</span>(pdist(list_centers))</span>
<span id="cb6-21"><a href="#cb6-21"></a></span>
<span id="cb6-22"><a href="#cb6-22"></a>    <span class="co"># compute the fitness</span></span>
<span id="cb6-23"><a href="#cb6-23"></a>    score <span class="op">=</span> sumNorm <span class="op">/</span> (numObject <span class="op">*</span> <span class="bu">pow</span>(minDis, <span class="dv">2</span>))</span>
<span id="cb6-24"><a href="#cb6-24"></a>    <span class="cf">return</span> score</span>
<span id="cb6-25"><a href="#cb6-25"></a></span>
<span id="cb6-26"><a href="#cb6-26"></a><span class="kw">def</span> centers2(X, labels):</span>
<span id="cb6-27"><a href="#cb6-27"></a>    x <span class="op">=</span> pd.DataFrame(X)</span>
<span id="cb6-28"><a href="#cb6-28"></a>    labels <span class="op">=</span> np.array(labels)</span>
<span id="cb6-29"><a href="#cb6-29"></a>    k <span class="op">=</span> <span class="bu">int</span>(np.<span class="bu">max</span>(labels) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb6-30"><a href="#cb6-30"></a>    n_cols <span class="op">=</span> x.shape[<span class="dv">1</span>]</span>
<span id="cb6-31"><a href="#cb6-31"></a>    centers <span class="op">=</span> np.array(np.zeros(shape<span class="op">=</span>(k, n_cols)))</span>
<span id="cb6-32"><a href="#cb6-32"></a></span>
<span id="cb6-33"><a href="#cb6-33"></a>    <span class="co"># Getting the centroids:</span></span>
<span id="cb6-34"><a href="#cb6-34"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb6-35"><a href="#cb6-35"></a>        centers[i, :] <span class="op">=</span> np.mean(x.iloc[labels <span class="op">==</span> i], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-36"><a href="#cb6-36"></a></span>
<span id="cb6-37"><a href="#cb6-37"></a>    <span class="cf">return</span> centers</span>
<span id="cb6-38"><a href="#cb6-38"></a></span>
<span id="cb6-39"><a href="#cb6-39"></a><span class="co"># New version:</span></span>
<span id="cb6-40"><a href="#cb6-40"></a><span class="kw">def</span> myXB(X, labels):</span>
<span id="cb6-41"><a href="#cb6-41"></a>    <span class="co">"My version of computing the Xie-Beni index."</span></span>
<span id="cb6-42"><a href="#cb6-42"></a>    </span>
<span id="cb6-43"><a href="#cb6-43"></a>    nrows <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb6-44"><a href="#cb6-44"></a></span>
<span id="cb6-45"><a href="#cb6-45"></a>    <span class="co"># Get the centroids:</span></span>
<span id="cb6-46"><a href="#cb6-46"></a>    centroids <span class="op">=</span> centers2(X, labels)</span>
<span id="cb6-47"><a href="#cb6-47"></a></span>
<span id="cb6-48"><a href="#cb6-48"></a>    <span class="co"># Computing the WGSS:</span></span>
<span id="cb6-49"><a href="#cb6-49"></a>    <span class="kw">def</span> getMinDist(obs, code_book):</span>
<span id="cb6-50"><a href="#cb6-50"></a>            dist <span class="op">=</span> cdist(obs, code_book)</span>
<span id="cb6-51"><a href="#cb6-51"></a>            code <span class="op">=</span> dist.argmin(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-52"><a href="#cb6-52"></a>            min_dist <span class="op">=</span> dist[np.arange(<span class="bu">len</span>(code)), code]</span>
<span id="cb6-53"><a href="#cb6-53"></a>            <span class="cf">return</span> min_dist</span>
<span id="cb6-54"><a href="#cb6-54"></a>                </span>
<span id="cb6-55"><a href="#cb6-55"></a>    euc_distance_to_centroids <span class="op">=</span> getMinDist(X, centroids)</span>
<span id="cb6-56"><a href="#cb6-56"></a></span>
<span id="cb6-57"><a href="#cb6-57"></a>    WGSS <span class="op">=</span> np.<span class="bu">sum</span>(euc_distance_to_centroids<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb6-58"><a href="#cb6-58"></a></span>
<span id="cb6-59"><a href="#cb6-59"></a>    <span class="co"># Computing the minimum squared distance to the centroids:</span></span>
<span id="cb6-60"><a href="#cb6-60"></a>    MinSqCentroidDist <span class="op">=</span> np.<span class="bu">min</span>(pdist(centroids, metric<span class="op">=</span><span class="st">'sqeuclidean'</span>))</span>
<span id="cb6-61"><a href="#cb6-61"></a></span>
<span id="cb6-62"><a href="#cb6-62"></a>    <span class="co"># COmputing the XB index:</span></span>
<span id="cb6-63"><a href="#cb6-63"></a>    xb <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> nrows) <span class="op">*</span> (WGSS <span class="op">/</span> MinSqCentroidDist)</span>
<span id="cb6-64"><a href="#cb6-64"></a></span>
<span id="cb6-65"><a href="#cb6-65"></a>    <span class="cf">return</span> xb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>oldxb <span class="op">=</span> benchmark_algorithm(dataset_sizes, oldXB, (), {})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 500; took 0.0439 seconds
Run complete for size: 500; took 0.0422 seconds
Run complete for size: 500; took 0.0408 seconds
Run complete for size: 1000; took 0.0748 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 1000; took 0.0761 seconds
Run complete for size: 1000; took 0.0814 seconds
Run complete for size: 1500; took 0.1104 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 1500; took 0.1092 seconds
Run complete for size: 1500; took 0.1027 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 2000; took 0.1386 seconds
Run complete for size: 2000; took 0.1345 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 2000; took 0.1965 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 2500; took 0.2259 seconds
Run complete for size: 2500; took 0.1997 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 2500; took 0.1871 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 3000; took 0.2377 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 3000; took 0.2225 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 3000; took 0.3046 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 4000; took 0.2689 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 4000; took 0.2988 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 4000; took 0.3213 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 5000; took 0.3179 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 5000; took 0.3531 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 5000; took 0.3521 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 6000; took 0.3976 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 6000; took 0.4837 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 6000; took 0.4561 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 8000; took 0.5604 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 8000; took 0.5961 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 8000; took 0.568 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 10000; took 0.7559 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 10000; took 0.6787 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 10000; took 0.6996 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 12000; took 0.9949 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 12000; took 0.9771 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 12000; took 1.0966 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 14000; took 1.2651 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 14000; took 1.2118 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 14000; took 1.3354 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 16000; took 1.3454 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 16000; took 1.2079 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 16000; took 1.0968 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 18000; took 1.2166 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 18000; took 1.215 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 18000; took 1.2833 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 20000; took 1.4117 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 20000; took 1.3423 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 20000; took 1.3897 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 22000; took 1.4809 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 22000; took 1.5475 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 22000; took 1.492 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 24000; took 1.674 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 24000; took 1.633 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 24000; took 1.6321 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 26000; took 1.7576 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 26000; took 1.7878 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 26000; took 1.7258 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 28000; took 1.8395 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 28000; took 1.9374 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 28000; took 1.8941 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 30000; took 2.0428 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 30000; took 2.041 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 30000; took 2.0524 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 32000; took 2.1921 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 32000; took 2.2152 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 32000; took 2.154 seconds</code></pre>
</div>
</div>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1"></a>newXB <span class="op">=</span> benchmark_algorithm(dataset_sizes, myXB, (), {})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 500; took 0.0035 seconds
Run complete for size: 500; took 0.003 seconds
Run complete for size: 500; took 0.0029 seconds
Run complete for size: 1000; took 0.0031 seconds
Run complete for size: 1000; took 0.0032 seconds
Run complete for size: 1000; took 0.0021 seconds
Run complete for size: 1500; took 0.0026 seconds
Run complete for size: 1500; took 0.0028 seconds
Run complete for size: 1500; took 0.0022 seconds
Run complete for size: 2000; took 0.0023 seconds
Run complete for size: 2000; took 0.0021 seconds
Run complete for size: 2000; took 0.0027 seconds
Run complete for size: 2500; took 0.0028 seconds
Run complete for size: 2500; took 0.0025 seconds
Run complete for size: 2500; took 0.0023 seconds
Run complete for size: 3000; took 0.0022 seconds
Run complete for size: 3000; took 0.0028 seconds
Run complete for size: 3000; took 0.0022 seconds
Run complete for size: 4000; took 0.0025 seconds
Run complete for size: 4000; took 0.0025 seconds
Run complete for size: 4000; took 0.0028 seconds
Run complete for size: 5000; took 0.0027 seconds
Run complete for size: 5000; took 0.0027 seconds
Run complete for size: 5000; took 0.003 seconds
Run complete for size: 6000; took 0.0029 seconds
Run complete for size: 6000; took 0.0027 seconds
Run complete for size: 6000; took 0.003 seconds
Run complete for size: 8000; took 0.0031 seconds
Run complete for size: 8000; took 0.0032 seconds
Run complete for size: 8000; took 0.0031 seconds
Run complete for size: 10000; took 0.0034 seconds
Run complete for size: 10000; took 0.004 seconds
Run complete for size: 10000; took 0.0036 seconds
Run complete for size: 12000; took 0.0037 seconds
Run complete for size: 12000; took 0.004 seconds
Run complete for size: 12000; took 0.0038 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 14000; took 0.0049 seconds
Run complete for size: 14000; took 0.0046 seconds
Run complete for size: 14000; took 0.0047 seconds
Run complete for size: 16000; took 0.0045 seconds
Run complete for size: 16000; took 0.0045 seconds
Run complete for size: 16000; took 0.0041 seconds
Run complete for size: 18000; took 0.0052 seconds
Run complete for size: 18000; took 0.0049 seconds
Run complete for size: 18000; took 0.0058 seconds
Run complete for size: 20000; took 0.0052 seconds
Run complete for size: 20000; took 0.0054 seconds
Run complete for size: 20000; took 0.0056 seconds
Run complete for size: 22000; took 0.0063 seconds
Run complete for size: 22000; took 0.0065 seconds
Run complete for size: 22000; took 0.0067 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 24000; took 0.0066 seconds
Run complete for size: 24000; took 0.0068 seconds
Run complete for size: 24000; took 0.0065 seconds
Run complete for size: 26000; took 0.0069 seconds
Run complete for size: 26000; took 0.0068 seconds
Run complete for size: 26000; took 0.007 seconds
Run complete for size: 28000; took 0.0072 seconds
Run complete for size: 28000; took 0.0071 seconds
Run complete for size: 28000; took 0.0078 seconds
Run complete for size: 30000; took 0.0074 seconds</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Run complete for size: 30000; took 0.009 seconds
Run complete for size: 30000; took 0.0079 seconds
Run complete for size: 32000; took 0.0081 seconds
Run complete for size: 32000; took 0.0081 seconds
Run complete for size: 32000; took 0.0073 seconds</code></pre>
</div>
</div>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1"></a>means <span class="op">=</span> oldxb.groupby(<span class="st">"Size"</span>)[<span class="st">"Seconds"</span>].mean()</span>
<span id="cb71-2"><a href="#cb71-2"></a>stds <span class="op">=</span> oldxb.groupby(<span class="st">"Size"</span>)[<span class="st">"Seconds"</span>].std()</span>
<span id="cb71-3"><a href="#cb71-3"></a>inter <span class="op">=</span> pd.DataFrame(pd.concat([means, stds], keys<span class="op">=</span>[<span class="st">"Mean"</span>, <span class="st">"Std. Dev."</span>], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb71-4"><a href="#cb71-4"></a>olderXB <span class="op">=</span> inter.reset_index()</span>
<span id="cb71-5"><a href="#cb71-5"></a></span>
<span id="cb71-6"><a href="#cb71-6"></a>means2 <span class="op">=</span>newXB.groupby(<span class="st">"Size"</span>)[<span class="st">"Seconds"</span>].mean()</span>
<span id="cb71-7"><a href="#cb71-7"></a>stds2 <span class="op">=</span> newXB.groupby(<span class="st">"Size"</span>)[<span class="st">"Seconds"</span>].std()</span>
<span id="cb71-8"><a href="#cb71-8"></a>inter2 <span class="op">=</span> pd.DataFrame(pd.concat([means2, stds2], keys<span class="op">=</span>[<span class="st">"Mean"</span>, <span class="st">"Std. Dev."</span>], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb71-9"><a href="#cb71-9"></a>newerXB <span class="op">=</span> inter2.reset_index()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1"></a>resultsXB <span class="op">=</span> pd.concat([olderXB, newerXB],</span>
<span id="cb72-2"><a href="#cb72-2"></a>                    axis<span class="op">=</span><span class="dv">0</span>, keys<span class="op">=</span>[<span class="st">"Old XB"</span>, <span class="st">"My XB"</span>])</span>
<span id="cb72-3"><a href="#cb72-3"></a>resultsXB <span class="op">=</span> resultsXB.reset_index()</span>
<span id="cb72-4"><a href="#cb72-4"></a>resultsXB.columns <span class="op">=</span> [<span class="st">"Indice"</span>, <span class="st">"Run"</span>, <span class="st">"NObs."</span>, <span class="st">"Mean Time"</span>, <span class="st">"SD Time"</span>]</span>
<span id="cb72-5"><a href="#cb72-5"></a>resultsXB.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>Indice</th>
      <th>Run</th>
      <th>NObs.</th>
      <th>Mean Time</th>
      <th>SD Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Old XB</td>
      <td>0</td>
      <td>500.0</td>
      <td>0.042300</td>
      <td>0.001552</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Old XB</td>
      <td>1</td>
      <td>1000.0</td>
      <td>0.077433</td>
      <td>0.003496</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Old XB</td>
      <td>2</td>
      <td>1500.0</td>
      <td>0.107433</td>
      <td>0.004143</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Old XB</td>
      <td>3</td>
      <td>2000.0</td>
      <td>0.156533</td>
      <td>0.034673</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Old XB</td>
      <td>4</td>
      <td>2500.0</td>
      <td>0.204233</td>
      <td>0.019793</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1"></a>fig <span class="op">=</span> px.scatter(resultsXB, x<span class="op">=</span><span class="st">"NObs."</span>, y<span class="op">=</span><span class="st">"Mean Time"</span>, color<span class="op">=</span><span class="st">"Indice"</span>,</span>
<span id="cb73-2"><a href="#cb73-2"></a>                 title<span class="op">=</span><span class="st">"Performance Comparison of Various Indice Implementations"</span>).update_traces(mode<span class="op">=</span><span class="st">'lines+markers'</span>)</span>
<span id="cb73-3"><a href="#cb73-3"></a>fig.update_layout(xaxis_title<span class="op">=</span><span class="st">"Number of Observations"</span>,</span>
<span id="cb73-4"><a href="#cb73-4"></a>                 yaxis_title<span class="op">=</span><span class="st">"Time taken to complete (s)"</span>)</span>
<span id="cb73-5"><a href="#cb73-5"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>                            <div id="f13c2f5a-f135-4582-9127-59d722f293df" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("f13c2f5a-f135-4582-9127-59d722f293df")) {                    Plotly.newPlot(                        "f13c2f5a-f135-4582-9127-59d722f293df",                        [{"hovertemplate":"Indice=Old XB<br>NObs.=%{x}<br>Mean Time=%{y}<extra></extra>","legendgroup":"Old XB","marker":{"color":"#636efa","symbol":"circle"},"mode":"lines+markers","name":"Old XB","orientation":"v","showlegend":true,"x":[500.0,1000.0,1500.0,2000.0,2500.0,3000.0,4000.0,5000.0,6000.0,8000.0,10000.0,12000.0,14000.0,16000.0,18000.0,20000.0,22000.0,24000.0,26000.0,28000.0,30000.0,32000.0],"xaxis":"x","y":[0.042300000000000004,0.07743333333333334,0.10743333333333334,0.15653333333333333,0.20423333333333335,0.2549333333333333,0.29633333333333334,0.34103333333333335,0.44580000000000003,0.5748333333333333,0.7113999999999999,1.0228666666666666,1.2707666666666666,1.2167000000000001,1.2383,1.3812333333333333,1.5068000000000001,1.6463666666666665,1.7570666666666668,1.8903333333333332,2.0454000000000003,2.1870999999999996],"yaxis":"y","type":"scatter"},{"hovertemplate":"Indice=My XB<br>NObs.=%{x}<br>Mean Time=%{y}<extra></extra>","legendgroup":"My XB","marker":{"color":"#EF553B","symbol":"circle"},"mode":"lines+markers","name":"My XB","orientation":"v","showlegend":true,"x":[500.0,1000.0,1500.0,2000.0,2500.0,3000.0,4000.0,5000.0,6000.0,8000.0,10000.0,12000.0,14000.0,16000.0,18000.0,20000.0,22000.0,24000.0,26000.0,28000.0,30000.0,32000.0],"xaxis":"x","y":[0.0031333333333333335,0.0028,0.002533333333333333,0.0023666666666666667,0.002533333333333333,0.0024,0.0026,0.0028000000000000004,0.0028666666666666667,0.0031333333333333335,0.0036666666666666666,0.003833333333333333,0.004733333333333333,0.004366666666666667,0.005299999999999999,0.005399999999999999,0.0065,0.006633333333333334,0.0069,0.007366666666666667,0.008100000000000001,0.007833333333333333],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Number of Observations"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Time taken to complete (s)"}},"legend":{"title":{"text":"Indice"},"tracegroupgap":0},"title":{"text":"Performance Comparison of Various Indice Implementations"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('f13c2f5a-f135-4582-9127-59d722f293df');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>6. Conclusion</h1>
<p>This project’s objective was to introduce a new Python package for computing internal clustering indices for any particular clustering solution. Given a set of observations, the user is able to run various clustering models as well as a large variety of clustering indices to determine how well a clustering solution fits the data, via the proposed library. It was shown that this library is analogous to the R library <em>NbClust</em> and borrows much of it’s inspiration from the authors previous work. The proposed library offers <span class="math inline">31</span> unique indices to assess the quality of a particular clustering method, sufficient enough to compare against each other.</p>
<p>It was also shown that the proposed library improves upon the <em>NbClust</em> library in a variety of avenues. In addition to offering a wider plethora of indices at the user’s disposal, it also offers better performance for computing most metrics, which was shown above. Furthermore, a difference in design pattern for the two libraries should be noted. While the <em>NbClust</em> library computes various indices for two popular clustering methods: namely, K-Means and agglomerative clustering, the proposed library allows for any clustering method available. This is because the author started the project with the intention of being method-agnostic, allowing the end-user to make that decision.</p>
<p>Finally, the proposed package was not meant to be a replacement for the <em>NbClust</em> library. Rather, the author’s intention was that both libraries would be used in conjunction with one another so as to form a basis for comparison in a user’s project. Also, a user wouldn’t have to utilize multiple languages for his/her project and instead could now use a singular environment to complete his/her tasks.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">© CC-By Matthias Quinn, 2023</div>   
    <div class="nav-footer-right">This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>