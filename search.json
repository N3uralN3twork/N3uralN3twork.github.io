[
  {
    "objectID": "resumes.html",
    "href": "resumes.html",
    "title": "Resumes",
    "section": "",
    "text": ">\n\n\n\n\n\n\n\n\n\n\nNotice\n\n\n\nAdditional information coming soon!"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Website\n  \n  \n    \n     Personal Github\n  \n  \n    \n     Personal Linkedin\n  \n  \n    \n     Contact Me\n  \n\n  \n  \nI am currently a data analyst (or scientist, depending on the project) working in the insurance industry. I’ve still have a lot to learn before I’m ready to transition to a full data science position, but I’m confident that - in the coming years - I’ll be able to fully qualify for such a role at Progressive.\n\n\nCleveland State University | Cleveland, OH\n| M.S. in Statistics | Sept 2020 - June 2021\n\nCleveland State University | Cleveland, OH\n| B.A. in Finance | Sept 2016 - June 2020\n\n\n\nSenior Data Scientist | Progressive Insurance | Feb. 2023 - Present\nData Analyst | Progressive Insurance | July 2021 - Jan. 2023"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Cluster Validity Indices\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nThis project was the seminal work of my master’s degree in statistics.\n\n\n\n\n\n\nJan 22, 2023\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nPower Analysis Calculator\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\ndesigns\n\n\n\n\nThis post is a walkthrough of various power analyses in R.\n\n\n\n\n\n\nNov 13, 2022\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nJapanese Population Decline over Time (Leaflet)\n\n\n\n\n\n\n\ncode\n\n\nmaps\n\n\n\n\nThis post is a demonstration of map-making with Leaflet in R.\n\n\n\n\n\n\nNov 6, 2022\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nStructural Equation Modelling\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\n\n\nThis post is a walkthrough of a structural equation model in R.\n\n\n\n\n\n\nAug 12, 2022\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nA notebook on what XGBoost is and how it works, including a sample application to forest cover types.\n\n\n\n\n\n\nDec 8, 2021\n\n\nMatthias Quinn, Matthew Brigham\n\n\n\n\n\n\n  \n\n\n\n\nRandomization Schemas\n\n\n\n\n\n\n\ncode\n\n\nstatistics\n\n\ndesigns\n\n\ncollege\n\n\n\n\nThis project is a full demonstration of designing an experiment, with an application in Shiny.\n\n\n\n\n\n\nSep 18, 2020\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nNaive Bayes\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nA quick notebook on how to create and interpret naive bayes models.\n\n\n\n\n\n\nJan 29, 2020\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression with caret\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nA quick notebook on how to create and interpret LR models.\n\n\n\n\n\n\nOct 26, 2019\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nMulti-factor Model\n\n\n\n\n\n\n\ncode\n\n\nfinance\n\n\ncollege\n\n\n\n\nThis project is a financial case study on mutual funds and using MFM to predict risk-adjusted scores of execess returns.\n\n\n\n\n\n\nApr 4, 2019\n\n\nMatthias Quinn, Touw, Lux\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sem-cfa/SEM-Indices.html",
    "href": "posts/sem-cfa/SEM-Indices.html",
    "title": "Structural Equation Modelling",
    "section": "",
    "text": "Learn how to create and interpret structural equation models using R’s lavaan.\n\n\n\n\n\n\nWiP\nStructural Equation Models (SEM) help to model the dependence relationships among multiple variables, simultaneously. In other words, we can model not only multiple variables, but each of those dependent variables can have multiple covariates attached to them."
  },
  {
    "objectID": "posts/logistic-regress-caret/logregcaret.html",
    "href": "posts/logistic-regress-caret/logregcaret.html",
    "title": "Logistic Regression with caret",
    "section": "",
    "text": "Logistic Regression"
  },
  {
    "objectID": "posts/logistic-regress-caret/logregcaret.html#step-1.-split-the-data-into-training-and-test-sets",
    "href": "posts/logistic-regress-caret/logregcaret.html#step-1.-split-the-data-into-training-and-test-sets",
    "title": "Logistic Regression with caret",
    "section": "Step 1. Split the data into training and test sets",
    "text": "Step 1. Split the data into training and test sets\n\n\nCode\nset.seed(12345789)\ninTrain <- createDataPartition(y = GermanCredit$Class, p = 0.6, list = FALSE, )\ntraining <- GermanCredit[inTrain, ]\ntesting  <- GermanCredit[-inTrain, ]\n\n\nTo create the logistic model, we’ll use the train function from caret.\n\n\nCode\nlmodel <- train(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + CreditHistory.Critical,\n                data = training,\n                method = \"glm\",\n                family = \"binomial\")\n\n\nTo obtain the odds for the coefficients and remove the log:\n\n\nCode\nexp(coef(lmodel$finalModel))\n\n\n           (Intercept)                    Age          ForeignWorker \n             3.0516947              1.0095028              0.2494196 \n   Property.RealEstate            Housing.Own CreditHistory.Critical \n             1.7752920              1.7580389              2.2363912 \n\n\nKeep in mind that you can’t just read the coefficients like you would in simple linear regression.\nFor example, our model is suggesting that for every one unit increase in Age, the (odds) of the consumer having good credit increases by a factor of 1.02\n95% Confidence Interval for terms:\n\n\nCode\nlogistic = glm(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + CreditHistory.Critical,\n                data = training,\n                family = \"binomial\")\n\ncbind(OR = exp(coef(logistic)), exp(confint(logistic)), pValue = (summary(logistic)$coefficients[, 4]))\n\n\nWaiting for profiling to be done...\n\n\n                              OR      2.5 %     97.5 %       pValue\n(Intercept)            3.0516947 0.74027688 20.9221296 0.1700607984\nAge                    1.0095028 0.99357880  1.0262071 0.2501534852\nForeignWorker          0.2494196 0.03925941  0.8803206 0.0651729645\nProperty.RealEstate    1.7752920 1.15611482  2.7758529 0.0100252083\nHousing.Own            1.7580389 1.18604549  2.6003291 0.0047917432\nCreditHistory.Critical 2.2363912 1.45002355  3.5270854 0.0003723358\n\n\nEssentially, if the confidence interval contains 1, it is not very helpful\n##Making Predictions: To make predictions, simple use the predict function from base R.\n\n\nCode\npredictions = predict(lmodel, newdata = testing, type = \"prob\")\nhead(predictions)\n\n\n         Bad      Good\n2  0.2547727 0.7452273\n6  0.4854772 0.5145228\n10 0.2040837 0.7959163\n15 0.5020246 0.4979754\n17 0.1683434 0.8316566\n18 0.3710473 0.6289527"
  },
  {
    "objectID": "posts/logistic-regress-caret/logregcaret.html#model-evaluation-and-diagnostics",
    "href": "posts/logistic-regress-caret/logregcaret.html#model-evaluation-and-diagnostics",
    "title": "Logistic Regression with caret",
    "section": "Model Evaluation and Diagnostics",
    "text": "Model Evaluation and Diagnostics\nIs this model any good? How well does it fit our data? Which predictors are the most important?\n\n1. Likelihood Ratio Test:\nTests whether the full model or a lesser model is the better fit for the data. A smaller p-value provides sufficient evidence that the full model is better than a reduced model.\n\n\nCode\nmod_fit_one <- glm(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + \n                     CreditHistory.Critical,\n                   data=training,\n                   family=\"binomial\")\nmod_fit_two <- glm(Class ~ Age + ForeignWorker,\n                   data=training,\n                   family = binomial(link = \"logit\"))\n\nanova(mod_fit_one, mod_fit_two, test =\"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + \n    CreditHistory.Critical\nModel 2: Class ~ Age + ForeignWorker\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       594     691.14                          \n2       597     723.12 -3  -31.976 5.294e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConclusion The full model is a better fit than a reduced model.\n\n\n2. Pseudo R^2\nLinear regression has the well-known R^{2}. However, logistic regression does not, so we’ll use something called McFadden’s R^{2}, which is given by: 1-\\frac{ln(LM)}{ln(L0)} where ln(LM) is the log likelihood value for the fitted model and ln(L0) is the log likelihood for the null model with just the intercept. Values closer to 0 indicate less predictive power and values closer to 1 indicate more predictive power.\n\n\nCode\nlibrary(pscl)\n\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\n\nCode\npR2(mod_fit_one)\n\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-345.57110076 -366.51858123   41.89496095    0.05715257    0.06744294 \n         r2CU \n   0.09562580 \n\n\nThe output above indicates that are model has low predictive power\n\n\n3. Hosmer-Lemeshow Test\nThis test examines whether the proportion of events are similar to the predicted probabilities of occurrence in subgroups of the data using a chi-square test. Interpretation: Small values with large p-values indicate a good fit to the data while large values with p-values below 0.05 indicate a poor fit. Null hypothesis: the model fits the data\n\n\n4. Variable Importance\nMeasures the absolute value of the t-statistic for each model parameter\n\n\nCode\nvarImp(lmodel)\n\n\nglm variable importance\n\n                       Overall\nCreditHistory.Critical  100.00\nHousing.Own              69.35\nProperty.RealEstate      59.15\nForeignWorker            28.81\nAge                       0.00\n\n\n\n\n5. Confusion Matrix\n\n\nCode\npreds = predict(lmodel, newdata = testing)\nconfusionMatrix(data = preds, reference = testing$Class)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Bad Good\n      Bad   12   15\n      Good 108  265\n                                          \n               Accuracy : 0.6925          \n                 95% CI : (0.6447, 0.7374)\n    No Information Rate : 0.7             \n    P-Value [Acc > NIR] : 0.651           \n                                          \n                  Kappa : 0.0596          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.1000          \n            Specificity : 0.9464          \n         Pos Pred Value : 0.4444          \n         Neg Pred Value : 0.7105          \n             Prevalence : 0.3000          \n         Detection Rate : 0.0300          \n   Detection Prevalence : 0.0675          \n      Balanced Accuracy : 0.5232          \n                                          \n       'Positive' Class : Bad"
  },
  {
    "objectID": "posts/random-schemas/random-schemas.html",
    "href": "posts/random-schemas/random-schemas.html",
    "title": "Randomization Schemas",
    "section": "",
    "text": "Produce the same schema that you have, which represents the desired study design. The input to your programs should be the parameters of # of sites, # of subjects/site, a randomization ratio, and various stratification levels.\n\n\n\n\n\n\nA reactive application that creates a design schema, given the researchers design parameters.\nThis project was finished in about 5 weeks.\nLet’s say we’re a researcher interested in deciding which treatment to give to our target audience. Since we want to make sure we follow proper guidelines, we decide to reach out to a friend of ours, a statistician, for advice.\nShe says that a good practice would be to create a credible, experimental design to find what we’re looking for. This project is essentially a reproducible example of our goals with the following condition:"
  },
  {
    "objectID": "posts/japan-population/japanMaps.html",
    "href": "posts/japan-population/japanMaps.html",
    "title": "Japanese Population Decline over Time (Leaflet)",
    "section": "",
    "text": "Learn how to create an interactive choropleth map using Leaflet.\n\n\n\n\n\n\nA reactive map, made with Leaflet, that displays Japan’s population distribution and other statistics in a presentable format.\nThe main purpose of this project was to remember how to use Leaflet in R. I had remembered that I started this project about 2 years ago and could not figure out, for the life of me, how to create an interactive choropleth map.\nFor some background, I had done static maps before using ggplot2, but I wanted to learn something new and that’s when I stumbled upon Leaflet. In all honesty, the most difficult piece was finding the data to draw the borders of a given area, which I now know is hosted by UC Davis.\nIf only I knew that a couple of years ago…"
  },
  {
    "objectID": "posts/naive-bayes/NaiveBayes.html",
    "href": "posts/naive-bayes/NaiveBayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Source\nNaive Bayes is a supervised machine learning algorithm base on Bayes Theorem.\nNaive because it assumes all of the predictor variables to be completely independent of each other.\nBayes Rule:\nP(A|B) = \\frac{P(B|A)P(A)}{P(B}\nIn Naive Bayes, there are multiple predictor variables and more than 1 output class.\nThe objective of a Naive Bayes algorithm is to measure the conditional probability of an event with a feature vector\nProblem Statement:\nPredict employee attrition\nCreate training (70%) and test (30%) sets and utilize reproducibility.\nDistribution of Attrition rates across train and test sets.\nNotice that they are very similar."
  },
  {
    "objectID": "posts/naive-bayes/NaiveBayes.html#advantages-and-shortcoming",
    "href": "posts/naive-bayes/NaiveBayes.html#advantages-and-shortcoming",
    "title": "Naive Bayes",
    "section": "Advantages and Shortcoming",
    "text": "Advantages and Shortcoming\nThe Naive Bayes classifier is simple, fast, and scales well to large n.\nA major disadvantage is that it relies on the often-wrong assumption of equally important and independent features."
  },
  {
    "objectID": "posts/naive-bayes/NaiveBayes.html#implementation",
    "href": "posts/naive-bayes/NaiveBayes.html#implementation",
    "title": "Naive Bayes",
    "section": "Implementation",
    "text": "Implementation\nWe will utilize the caret package in R.\n\nCreate response and feature data\n\n\n\nCode\nfeatures <- setdiff(names(train), \"Attrition\")\nx <- train[, features]\ny <- train$Attrition\n\n\nInitialize 10-fold cross validation using caret’s trainControl function.\n\n\nCode\ntrControl <- trainControl(method = \"cv\", number = 10)\n\n\nTraining the Naive Bayes model:\n\n\nCode\nnb.fit <- train(\n  x = x,\n  y = y,\n  method = \"nb\",\n  trControl = trControl\n)\n\n\n#Confusion Matrix to analyze our results:\n\n\nCode\nconfusionMatrix(nb.fit)\n\n\nCross-Validated (10 fold) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction   No  Yes\n       No  76.8  8.2\n       Yes  7.1  7.9\n                            \n Accuracy (average) : 0.8473\n\n\n\n\nCode\nplot(nb.fit)\n\n\n\n\n\nTo test the accuracy on our test data set:\n\n\nCode\npreds <- predict(nb.fit, newdata = test)\n\n\n\n\nCode\nconfusionMatrix(preds, reference = test$Attrition)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  329  38\n       Yes  41  34\n                                          \n               Accuracy : 0.8213          \n                 95% CI : (0.7823, 0.8559)\n    No Information Rate : 0.8371          \n    P-Value [Acc > NIR] : 0.8333          \n                                          \n                  Kappa : 0.3554          \n                                          \n Mcnemar's Test P-Value : 0.8220          \n                                          \n            Sensitivity : 0.8892          \n            Specificity : 0.4722          \n         Pos Pred Value : 0.8965          \n         Neg Pred Value : 0.4533          \n             Prevalence : 0.8371          \n         Detection Rate : 0.7443          \n   Detection Prevalence : 0.8303          \n      Balanced Accuracy : 0.6807          \n                                          \n       'Positive' Class : No"
  },
  {
    "objectID": "posts/sample-size-app/powerAnalysis.html",
    "href": "posts/sample-size-app/powerAnalysis.html",
    "title": "Power Analysis Calculator",
    "section": "",
    "text": "Learn how to use R’s Shiny to conduct power analyses for various scenarios.\n\n\n\n\n\n\nA reactive application, made with Shiny, to help users conduct a small sample of scenarios.\nI’m often asked, as a part of my job, to figure out how many entities/people are needed to see a significant effect on some process or metric. These seemingly simple requests can have a profound effect on where business leaders decide to go with a project, if at all. Thus, the reasoning behind this calculator is to provide people with a no-code interface to some common statistical tests.\nA couple of months ago (2022), I was asked to give a few presentations to my team and a couple of outside business folks on how to properly conduct a power analysis. I wasn’t the strongest at performing such tests, as I had no formal training on how to do one and was taught more theoretical aspects, but I went ahead and researched on my own. Along the way, I began to realize that - unless you know what you’re doing - it was very easy to be given some sample numbers and given an output. Unfortunately, the validity of the inputs was not always bulletproof. Sometimes, the requester wasn’t quite sure of the numbers he/she was presenting, so it would lead to some discussions around his/her purposes.\nThe most difficult part of the project was figuring out the UI, as I was inspired by G*Power’s user interface, but I didn’t necessarily want a straight copy. So, I settled for a seemingly simple approach.\nHonestly, the most difficult steps are 2 and 3, as mentioned earlier. Thus, it’s imperative that a user understands what he/she is doing before receiving the results. As an example, a request was made to me to figure out how many days an A/B test would have to run to provide statistically significant results. I performed a power analysis and came back with my estimate on how long it would take to achieve a provable result.\n35 \\ days\nAs it turns out, upon further questioning, the requester told me that the test was only going to run for a maximum of 30 days. There wasn’t any way for results to be statistically verifiable. Instead of lying to her, I was upfront and told her that she wasn’t going to get what she wanted in just 30 days, which paid off for me in the end. She understood what I said after just a little more convincing, and the sandbox went off without any problems."
  },
  {
    "objectID": "posts/sample-size-app/powerAnalysis.html#purpose-of-power-analysis",
    "href": "posts/sample-size-app/powerAnalysis.html#purpose-of-power-analysis",
    "title": "Power Analysis Calculator",
    "section": "Purpose of Power Analysis",
    "text": "Purpose of Power Analysis\nWhen designing experiments, a key question to keep in mind is:\nHow many animals/subjects do I need for my experiment?\nTo small of a sample size can under detect the effect of interest; too large of a sample size, and you be lead to wasting unnecessary resources.\n\n\n\n\n\n\nGoal\n\n\n\nWe want to have enough samples to reasonably detect an effect if it really is there without wasting limited resources on too many samples.\n\n\n\nEffect size\n\nThe magnitude of the effect under the alternative hypothesis\nThe larger the effect, the easier it is to detect and thus requires fewer samples\n\nPower\n\nThe probability of correctly rejecting the null hypothesis if it is false\nPower is equal to 1-\\beta where \\beta is the probability of a Type II error (FN)\nHigher power allows for a better chance of detecting a true effect, if it exists\nA standard power is equal to 0.80\n\nSignificance level (\\alpha)\n\nThe probability of falsely rejecting the null hypothesis even though it is true\nThe lower the significance level, the more likely you are to avoid a false positive and the more samples are needed\nA standard significance level is equal to 0.05"
  },
  {
    "objectID": "posts/sample-size-app/powerAnalysis.html#effect-sizes",
    "href": "posts/sample-size-app/powerAnalysis.html#effect-sizes",
    "title": "Power Analysis Calculator",
    "section": "Effect Sizes",
    "text": "Effect Sizes\nThe effect size in a power analysis is dependent on the sample data at hand.\n\nHow to estimate the effect size (Options)\n\nUse background information to get means and variation, then calculate the effect size directly\nUse background information from similar studies to get means and variation, then calculate the effect size directly\nWith no prior information, make an estimated guess on the effect size expected\n\n\nEffect Size = \\frac {\\mu_{1} - \\mu_{2}}{\\sigma}"
  },
  {
    "objectID": "posts/sample-size-app/powerAnalysis.html#table-of-tests-and-functions",
    "href": "posts/sample-size-app/powerAnalysis.html#table-of-tests-and-functions",
    "title": "Power Analysis Calculator",
    "section": "Table of Tests and Functions",
    "text": "Table of Tests and Functions\n\nTable of Statistical Tests and R Functions\n\n\n\n\n\n\n\n\n\nID\nName of Test\nIn R?\nPackage\nFunction\n\n\n\n\n1\nOne Mean t-test\nYes\npwr\npwr.t.test\n\n\n2\nTwo Means t-test\nYes\npwr\npwr.t.test\n\n\n3\nPaired t-test\nYes\npwr\npwr.t.test\n\n\n4\nOne-way ANOVA\nYes\npwr\npwr.anova.test\n\n\n5\nSingle proportion test\nYes\npwr\npwr.p.test\n\n\n6\nTwo proportions test\nYes\npwr\npwr.2p.test\n\n\n7\nChi-squared test\nYes\npwr\npwr.chisq.test\n\n\n8\nSimple Linear Regression\nYes\npwr\npwr.f2.test\n\n\n9\nMultiple Linear Regression\nYes\npwr\npwr.f2.test\n\n\n10\nCorrelation\nYes\npwr\npwr.r.test\n\n\n11\nOne Mean Wilcoxon Test\nYes*\npwr\npwr.t.test + 15%\n\n\n12\nMann-Whitney Test\nYes*\npwr\npwr.t.test + 15%\n\n\n13\nPaired Wilcoxon Test\nYes*\npwr\npwr.t.test + 15%\n\n\n14\nKruskal-Wallace Test\nYes*\npwr\npwr.anova.test + 15%\n\n\n15\nRepeated Measures ANOVA\nYes\nWebPower\nwp.rmanova\n\n\n16\nMulti-way ANOVA\nYes\nWebPower\nwp.kanova\n\n\n17\nMulti-way ANOVA\nYes\nWebPower\nwp.kanova\n\n\n18\nLogistic Regression\nYes\nWebPower\nwp.logistic\n\n\n19\nPoisson Regression\nYes\nWebPower\nwp.poisson\n\n\n20\nMultilevel Modelling: CRT\nYes\nWebPower\nwp.crt2arm\n\n\n21\nMultilevel Modelling: MRT\nYes\nWebPower\nwp.mrt2arm\n\n\n22\nGLMM\nYes**\nSimr & lme4\n\n\n\n\n\nSources\nShiny - Using custom CSS in your app"
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html",
    "href": "posts/clustering-indices-project/cluster-indices.html",
    "title": "Cluster Validity Indices",
    "section": "",
    "text": "The following research examines various clustering methods and internal validity indices. Many clustering algorithms depend on various assumptions in order to identify subgroups, so there exists a need to objectively evaluate these algorithms, whether through complexity analyses or the proposed internal validity indices. The goal is to apply these indices to both artificial and real data in order to assess their fidelity. Currently, there exists no Python package to achieve this goal, but the proposed library offers 31 indices to help the user choose the correct number of clusters in his/her data, regardless of the chosen clustering methodology.\n\n\nThe goal of this project is to understand and compute a variety of indices that indicate the optimal number of clusters to use when performing a cluster analysis.\nCluster analysis refers to a finding unique subgroup/clusters in a dataset where the observations within each cluster are more related to each other in some way compared to observations in another, separate cluster. There are a plethora of clustering techniques and algorithms, each with a different way of solving the above general problem. In addition, there does not exist a single, optimal clustering solution for any clustering problem. This is also known as the “No Free Lunch” theorem from David Wolpert and William Macready, which states that “any two optimization algorthims are equivalent when their performance is averaged across all possible problems.” (Wolpert, Macready, 2005)\nSince cluster analysis is inherently an unsupervised learning technique, the selection of the optimal number of clusters is subjective. Consider the following example that was made via Python:\n\n\nCode\nfrom sklearn.datasets import make_blobs\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata, true_labels = make_blobs(n_samples=500, n_features=2, centers = 4, random_state=1234, cluster_std=0.9)\n\npoints = pd.DataFrame(data, columns=['x', 'y'])\n\nk4 = KMeans(n_clusters=4, random_state=1234).fit(points)\nk5 = KMeans(n_clusters=5, random_state=1234).fit(points)\nk6 = KMeans(n_clusters=6, random_state=1234).fit(points)\n\nfig = plt.figure(figsize=(12, 12))\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(223)\nax4 = fig.add_subplot(224)\n\nax1.scatter(points['x'], points['y'], color=\"black\")\nax1.axes.get_xaxis().set_visible(False)\nax1.axes.get_yaxis().set_visible(False)\nax2.scatter(points['x'], points['y'], c=k4.labels_.astype(float))\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax3.scatter(points['x'], points['y'], c=k5.labels_.astype(float))\nax3.axes.get_xaxis().set_visible(False)\nax3.axes.get_yaxis().set_visible(False)\nax4.scatter(points['x'], points['y'], c=k6.labels_.astype(float))\nax4.axes.get_xaxis().set_visible(False)\nax4.axes.get_yaxis().set_visible(False)\n\nax1.title.set_text('Original Data')\nax2.title.set_text('K=4 Clusters')\nax3.title.set_text('K=5 Clusters')\nax4.title.set_text('K=6 Clusters')\n\nplt.show()\n\n\nC:\\Users\\miqui\\anaconda3\\envs\\MLDL\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\miqui\\anaconda3\\envs\\MLDL\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\miqui\\anaconda3\\envs\\MLDL\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\nThe figure above helps to illustrate that the definition of a cluster is imprecise and that the best definition depends on the nature of the data and the user’s desired results."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#language",
    "href": "posts/clustering-indices-project/cluster-indices.html#language",
    "title": "Cluster Validity Indices",
    "section": "2.1 Language",
    "text": "2.1 Language\nPython was chosen as the language of choice due to the author’s familiarity with the various libraries made available in Python, including the NumPy and Sci-kit Learn modules. R was heavily used to double-check many of the algorithms and indices, relying mostly on the NbClust library (Charrad, 2014)."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#related-work",
    "href": "posts/clustering-indices-project/cluster-indices.html#related-work",
    "title": "Cluster Validity Indices",
    "section": "2.2 Related Work",
    "text": "2.2 Related Work\nA variety of measures aiming to validate the results of a cluster analysis have been defined over the years and this project focuses on relative criteria, which consists of the evaluation of a clustering algorithm by comparing it with the same algorithm but varies in the input parameters.\nThe inspiration for this project came on the heels of a separate project where a cluster analysis was used, but choosing the number of clusters appropriate for the final conclusion was unclear, both numerically and graphically. R has great support for cluster analysis and choosing the number of clusters; in particular, the NbClust package provides extensive support for both tasks and is the main inspiration for this project. The R package came out in October 2014 and contains a variety of indices that help aid the user in choosing the appropriate number of clusters, given one of two clustering methods; namely, K-Means and agglomerative clustering.\nHowever, when looking to validate results in another language, there was an apalling lack of libraries available for accomplishing the task. Python lacks, and still does as of writing, a comprehensive package for choosing an appropriate number of clusters. Python’s Sci-kit Learn library does have support for a plethora of clustering algorithms, including the ones used in this project, as well as a few others. However, it only supports three clustering indices.\nThe three clustering indices covered by Python’s Sci-kit Learn library are the silhouette coefficient, the Davies-Bouldin index, and the Calinski-Harabasz index. The proposed library, as well as NbClust, support all three of the indices in addition to many more."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#k-means",
    "href": "posts/clustering-indices-project/cluster-indices.html#k-means",
    "title": "Cluster Validity Indices",
    "section": "3.1 K-Means",
    "text": "3.1 K-Means\nThe K-Means algorithm is a centroid-based clustering technique and is one of the most popular clustering algorithms, so it had to be covered. A slight alternative to the algorithm, K-medians, uses the median instead of the mean, but will not be covered in this project. The algorithm attempts to minimize the within-cluster variances, which will be formalized next.\n\n3.1.1 The K-Means Algorithm\nGiven a set of n observations each consisting of d dimensions, and wanting to partition those observations into k clusters, the goal is to minimize the within-cluster sum of squares (WSS). Formally, the K-Means algorithm is as follows:\n\n\n\nFig.2 - Pseudocode for the generic K-Means algorithm\n\n\nInitially, points are assigned to the initial centroids, which in this case is the mean. After the points are assigned to the centroid, the centroid is then updated until no more changes occur and the algorithm converges. There is no guarantee, however, that the algorithm will converge. The objective is to minimize the pairwise squared deviations of points in the same cluster:\nargmin \\displaystyle \\sum_{i=1}^{k}\\frac {1}{2 |S_i|} * \\sum ||{x - y}||^{2} \\tag{1}\nThe within cluster sum of squares, or what the Sci-kit Learn package calls inertia, is a measure of how internally coherent separate clusters are. The optimal value is 0 while lower values indicate more separated clusters. Sci-kit Learn uses Lloyd’s algorithm which follows the steps in the algorithm presented above and uses the squared Euclidean distance.\n\n\n3.1.2 Time and Space Complexity\nK-Means only needs to store the data points and each centroid. Specifically, the storage requirements for the algorithm is O(n(m + K)), where m is the number of data points, n is the number of variables, and K is the pre-specified number of clusters. K-Means runs in linear time, in particular, O(I*K*m*n), where I is the number of iterations until convergence. As long as the number of clusters remains significantly below the number of variables, the K-Means algorithm should run in linear time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#agglomerative-hierarchical-clustering",
    "href": "posts/clustering-indices-project/cluster-indices.html#agglomerative-hierarchical-clustering",
    "title": "Cluster Validity Indices",
    "section": "3.2 Agglomerative Hierarchical Clustering",
    "text": "3.2 Agglomerative Hierarchical Clustering\nAnother commonly-used approach for clustering is that of hierarchical clustering, of which, there are two approaches; namely, Agglomerative, and Divisive.\nAgglomerative: Start with n clusters and keep going until you have 1 cluster.\nDivisive: Start with 1 cluster and decompose until you have n clusters.\nThis section will focus solely on the Agglomerative variant.\n\n3.2.1 Agglomerative Hierarchical Clustering Algorithm\n The key operation of the Agglomerative Clustering algorithm is the calculation of the proximity between two clusters, where the definition of cluster proximity differentiates the various agglomerative flavors.\nWard’s method assumes that a cluster is represented by its centroid, but it measures the proximity between two clusters in terms of the increase in the SSE that results from merging the two clusters. Ward’s method attempts to minimize the sum of the squared distances of points from their cluster centroids. The initial cluster distances in Ward’ minimum variance method are defined to be the squared Euclidean distance between points:\nd_{ij} = ||X_i - X_j||^2 \\tag{2}\n\n\n3.2.2 Time and Space Complexity\nWard’s method requires the storage of 0.5n^{2} where n is the number of datapoints and the storage required to keep track of the clusters is proportional to the number of clusters, which is n-1, excluding singleton clusters. Thus, the total space complexity of Ward’s method is O(n^{2}).\nNext, in terms of the time required to run the algorithm, a good place to start is the computation of the proximity matrix. O(n^{2}) time is required to compute the proximity matrix. Afterwards there are n-1 iterations involving steps 3 and 4 because there are n clusters at the start and two clusters are merged after each iteration. In addition, because the algorithm exhaustively scans the proximity matrix for the lowest distances, the run time is O(n^{3}).\nObviously, because of the rather large space and time complexity of the Agglomerative Clustering algorithm, it should not be used for large datasets. In addition, the Agglomerative Clustering algorithm tends to make good local decisions about which two clusters to combine since they can use information about the pairwise similarity of all of the data points. However, once a decision is made to merge two clusters, the merge is final and it cannot be undone."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#dbscan",
    "href": "posts/clustering-indices-project/cluster-indices.html#dbscan",
    "title": "Cluster Validity Indices",
    "section": "3.3 DBSCAN",
    "text": "3.3 DBSCAN\nDensity-based clustering locates regions of high density that are separated from one another by regions of low density. A cluster is considered to be a set of core samples and a set of non-core samples that are close to a core sample. In this approach, known as the center-based approach, the density for a particular point is estimated by counting the number of points within a specified radius, which is controlled by the Epsilon parameters.\nThere are three types of data points of consideration in the DBSCAN algorithm, namely:\nCore points: The points are inside the interior of a density-based cluster. A point is a core point if the number of points within a given neighborhood around the point exceeds a certain threshold, min_samples, as defined by Sci-kit Learn.\nBorder points: A point that is not a core point, but rather falls within the neighborhood of a core point, which can fall within the neighborhood of several core points.\nNoise points: Any points that is neither a core point nor a border point.\nAn example of the three points described above are shown below:\n ### 3.3.1 The DBSCAN algorithm\nInformally, any two core points that are close enough, within a distance of the Epsilon parameter, are put into the same cluster. Additionally, any border point that is close enough to a core point is put in the same cluster as the core point. Finally, noise points are discarded. More formally:\n ### 3.3.2 Time and Space Complexity\nIn the worst case, the time complexity of the DBSCAN algorithm is O(n^{2}), where n is the number of data points, since DBSCAN visits each point of the data set. In Sci-kit Learn, the DBSCAN implementation uses two tree data structures, namely, ball trees and kd-trees, to determine the neighborhood of points. In particular, the kd-tree allows for the efficient retrieval of all points within a given distance of a specified points, meaning the time complexity for the DBSCAN algorithm can be as low as O(nlogn)\nThe implementation of the DBSCAN algorithm in Sci-kit Learn consumes n^{2} floats, but this can be reduced to O(n) since it is only necessary to store the cluster label and what type of point each datum belongs to."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#notation",
    "href": "posts/clustering-indices-project/cluster-indices.html#notation",
    "title": "Cluster Validity Indices",
    "section": "4.0 Notation:",
    "text": "4.0 Notation:\nThis section contains information on notations used throughout the rest of the paper.\nLet us denote the following:\nN = total number of observations in a dataset\nm = the total number of numeric variables used to cluster the data\nK = the total number of clusters\nX = the data matrix of shape [N, m]\nC_{k} = A submatrix of X made up of the rows of X where some partition, P_{i}, contains any point X_{i} belonging to cluster k\nn_{k} = the number of observations belonging to cluster C_{k}"
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#ball-hall-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#ball-hall-index",
    "title": "Cluster Validity Indices",
    "section": "4.1 Ball-Hall Index",
    "text": "4.1 Ball-Hall Index\nThis index is based on the average distance of the items to their respective cluster centroids. It is just the within-group sum of distances and is computed as:\nBH = \\frac{W_k}{K} \\tag{3}\nwhere W_k = \\displaystyle \\sum ||x_i - \\bar{x_i}|| is the within-group dispersion matrix for the data clustered into K clusters. This index requires the computation of centroids and N within-group distances, which itself is O(mN). Thus, Ball-Hall is O(mN) where m is the number of variables in the data set to be clustered.\nThe maximum difference in the Ball-Hall indices between different clusters is considered to be the optimal number of clusters."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#the-beale-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#the-beale-index",
    "title": "Cluster Validity Indices",
    "section": "4.2 The Beale Index",
    "text": "4.2 The Beale Index\nBeale, 1969 introduced the idea of using an F-ratio to test the hypothesis of the existence of q_1 versus q_2 clusters in the data (q_2 > q_1). The optimal number of clusters is obtained by comparing F with an F_{p, (n_m-2)p} distribution, where the null hypothesis would be rejected for significantly large values of the F-statistic. In the proposed Python package, the user can control the \\alpha level used to compute the critical F value, and the default value is 0.10.\nMathematically, the calculation of the Beale index is as follows:\nBeale = F = \\frac {\\frac {V_{kl}}{W_k + W_l}}{(\\frac {n_m-1}{n_m-2})*2^{\\frac {2}{p}}-1} \\tag{4}\nwhere V_{kl} = W_m - W_k - W_l and p is the number of variables."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#the-c-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#the-c-index",
    "title": "Cluster Validity Indices",
    "section": "4.3 The C Index",
    "text": "4.3 The C Index\nThe C index was reviewed by Hubert and Levin, 1976 and is calculated below:\nC_{index} = \\frac {S_w - S_{min}}{S_{max} - S_{min}}, C_{index} \\in (0, 1) \\tag{5}\nwhere\n\nS_{min} = the sum of the N_W smallest distances between all the pairs of points in the entire data set;\nS_{max} = the sum of the N_W largest distances between all the pairs of points in the entire data set.\nN_{W} = \\displaystyle \\sum_{k=1}^{K} \\frac {n_k(n_k - 1)}{2}\n\nThe optimal number of clusters is indicated by the minimum value of the C index. Finally, the C-index has a time complexity of O(m^{2}(n + log_2m)), mostly due to the computational cost of sorting the N_W pairwise distances, which can be slow with a large number of observations."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#the-calinski-harabasz-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#the-calinski-harabasz-index",
    "title": "Cluster Validity Indices",
    "section": "4.4 The Calinski-Harabasz Index",
    "text": "4.4 The Calinski-Harabasz Index\nThe Calinski-Harabasz index Calinski, Harabasz, 1974, also known as the Variance Ratio Criterion, is a measure of how similar an object is to its own cluster compared to other clusters. From Sci-kit Learn’s website, the score is defined as the ratio between the within-cluster dispersion and between-cluster dispersion. More formally put:\nCH_q = \\frac{trace(B_k)/(K-1)}{trace(W_k)/(N-K)} \\tag{6}\nwhere: W = \\displaystyle \\sum W_k \\tag{7}\nand: W_k = \\displaystyle \\sum (x_i - \\bar{x_m})(x_i - \\bar{x_m})^{T} \\tag{8}\nand: trace(W_k) = \\displaystyle \\sum_{k=1}^n \\sum (x_{ip} - \\bar{x_{lp}})^{2} \\tag{9},\nwhere x_{ip} is the p^{th} attribute of the i^{th} data object and \\bar{x_{lp}} is the p^{th} attribute of the centroid of the l^{th} cluster.\nThe maximum value of the CH score, given a range of clusters, is considered to be the best number of clusters. The Calinski-Harabasz index also has O(mN) complexity."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#cubic-clustering-criterion",
    "href": "posts/clustering-indices-project/cluster-indices.html#cubic-clustering-criterion",
    "title": "Cluster Validity Indices",
    "section": "4.5 Cubic Clustering Criterion",
    "text": "4.5 Cubic Clustering Criterion\nThe CCC by Sarle, 1983 is obtained by comparing the observed R^2 to the approximate expected R^2 using an approximate variance-stabilizing transformation. CCC values greater than 0 mean that the obtained R^2 is greater than would be expected if sampling from a uniform distribution. According to the SAS Institute, the best way to use the CCC is to plot its values against the number of clusters. The CCC is used to compare the R^{2} you obtain for a given set of clusters with the R^{2} you would get by clustering a uniformly/normally distributed set of points in p dimensional space. In addition, the optimal number of clusters is indicated by the maximum CCC value. Mathematically, the CCC can be computed as follows:\nCCC = ln[\\frac {1 - E[R^{2}]}{1 - R^{2}}] * \\frac {\\sqrt{\\frac {np}{2}}}{(0.001 + E[R^{2}])^{2}} \\tag{10}\nwhere,\nR^{2} = 1 - \\frac {p  + \\sum u^{2}_j}{\\sum u^{2}_j} \\tag{11}\nand,\nE[R^{2}] = 1 - \\displaystyle \\frac {\\sum_{j=1}^{p} \\frac {1}{n + u_j} + \\sum_{j=p+1}^{p} \\frac {u^{2}_j}{n + u_j}}{\\displaystyle \\sum^{p}_{j=1} u^{2}_{j}} * [\\frac {(n - k)^{2}}{n}] * [1 + \\frac {4}{n}] \\tag{12}\nand,\nu_j = \\frac {s_j}{c} \\tag{13}\nand,\nc = (\\frac {v}{k})^{\\frac {1}{p}} \\tag{14}\nand,\nv = \\displaystyle \\prod_{j=1}^{p} s_{j} \\tag{15}\nand,\np is the largest integer less than k such that u_p is not less than one.\nFinally, the reason for the odd ending to the expected r-squared formula was derived empirically to stabilize the variance across different combinations of observations, variables, and clusters."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#d-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#d-index",
    "title": "Cluster Validity Indices",
    "section": "4.6 D Index",
    "text": "4.6 D Index\nThe D Index from Lebart et al., 2000 is based on clustering gain on intra-cluster inertia, which measures the degree of homogeneity between the data associated with a cluster. It calculates their distances compared to the cluster centroid.\nMathematically, intra-cluster inertia is defined as:\nW(P^k) = \\frac {1}{k} \\displaystyle \\sum_{k=1}^{K} \\frac {1}{n_k} \\sum_{i=1}^{n_k} d(x_i, c_k) \\tag{16}\nThe clustering gain on the intra-cluster inertia, given two partitions, P^{k-1} composed of k-1 clusters and P^k composed of k clusters, is defined as:\nGain = W(P^{k-1}) - W(P^{k}) \\tag{17}\nThe clustering gain above should be minimized. In addition, the optimal suggested number of clusters corresponds to a significant decrease of the first different of clustering gain, compared to the number of clusters. This “significant decrease” can be identified by examining the second differences of clustering gain."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#davies-bouldin-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#davies-bouldin-index",
    "title": "Cluster Validity Indices",
    "section": "4.7 Davies-Bouldin Index",
    "text": "4.7 Davies-Bouldin Index\nThe Davies-Bouldin Index is a function of the sum ratio of within-cluster scatter to between-cluster separation. A lower value indicates a better clustering solution, i.e. better separation between the clusters.\nMathematically, the index is defined as the average similarity between each cluster and its most similar one, where the similarity is defined as a measure R_{ij} that is nonnegative and symmetric, like so:\nR_{ij} = \\frac {s_i + s_j}{d_{ij}} \\tag{18}\nThen the Davies-Bouldin index is defined as:\nDB = \\frac {1}{k} \\displaystyle \\sum_{i=1}^{k}max(R_{ij}) \\tag{19}\nFinally, in terms of computational complexity, the Davies-Bouldin index runs in O(m(k^2 + N)) time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#duda-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#duda-index",
    "title": "Cluster Validity Indices",
    "section": "4.8 Duda Index",
    "text": "4.8 Duda Index\nThe Duda index, Duda and Hart, 1973 is the ratio between the sum of squared errors within clusters when the data are partitioned into two clusters, and the squared errors when only one cluster is present.\nMathematically, this can be represented as:\nDuda = \\frac {W_k + W_l}{W_m} \\tag{20}\nThe optimal number of suggested clusters is the smallest k such that:\nDuda \\ge 1 - \\frac {2}{\\pi p} - 3.20 \\sqrt{\\frac {2(1 - \\frac {8}{\\pi^{2}p}}{n_m p}} \\tag{21}\nwhere 3.20 is a Z-score tested by Milligan and Cooper, 1985"
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#dunn-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#dunn-index",
    "title": "Cluster Validity Indices",
    "section": "4.9 Dunn Index",
    "text": "4.9 Dunn Index\nThe Dunn index (Dunn, 1974) is the ratio between the minimal intercluster distance to the maximal intracluster distance. If the data set contains well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters should be large. Thus, the suggested optimal number of clusters corresponds to the maximum value of the Dunn index.\nMathematically:\nDunn = \\frac {d_{min}}{d_{max}} \\tag{22}\nwhere:\nd_{min} = \\min{(\\min{||M_{i}^{k} - M_{j}^{k'}||})} \\tag{23}\nand,\nd_{max} = \\max{(\\max{||M_{i}^{k} - M_{j}^{k}||})} \\tag{24}\nand d_{max} is known as the diameter of a cluster, or the largest distance separating two distinct points in a particular cluster.\nIn terms of computational complexity, the Dunn index runs in O(mN^{2}) time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#frey-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#frey-index",
    "title": "Cluster Validity Indices",
    "section": "4.10 Frey Index",
    "text": "4.10 Frey Index\nThe Frey index,proposed by Frey and Van Groenewoud, 1972, is the ratio of the difference scores from two successive levels in a hierarchy. This means that the index should only be used on hierarchical clustering method. The numerator represents the difference between the mean between-cluster distances while the denominator represents the difference between the mean within-cluster distances, each from the two levels.\nMathematically, the Frey index can be written as:\nFrey = \\frac {\\bar{S_{b_{j+1}}} - \\bar{S_{b_j}}}{\\bar{S_{w_{j+1}}} - \\bar{S_{w_j}}} \\tag{25}\nwhere the mean between-cluster distance is:\n\\bar{S_b} = S_b / N_b \\tag{26}\nand the mean within-cluster distance is:\n\\bar{S_w} = S_w / N_w \\tag{27}\nThe best clustering solution occurs when the ratio falls below 1.0, and the cluster level before that point is taken as the optimal partition. In addition, if the ratio never falls below 1.0, a one cluster solution is usually best."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#gamma-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#gamma-index",
    "title": "Cluster Validity Indices",
    "section": "4.11 Gamma Index",
    "text": "4.11 Gamma Index\nThe Gamma Index compares all within-cluster dissimilarities and all between-cluster dissimilarities. A comparison is concordant/discordant if a within-cluster dissimilarity is less/greater than a between cluster dissimilarity. Ties are disregarded. Also, the maximum value of the index indicates the optimal suggested number of clusters.\nMathematically, the Gamma Index is defined as:\nGamma = \\frac {s(+) - s(-)}{s(+) + s(-)} \\tag{28}\nwhere,\ns(+) is the number of concordant comparisons\nand,\ns(-) is the number of discordant comparisons\nFurthermore, the definition of concordant and discordant pairs is defined mathematically below:\ns(+) = \\frac {1}{2} \\displaystyle \\sum \\sum \\frac {1}{2} \\sum \\sum \\delta(||x_i - x_j|| < ||x_p - x_k||) \\tag{29}\nand,\ns(-) = \\frac {1}{2} \\displaystyle \\sum \\sum \\frac {1}{2} \\sum \\sum \\delta(||x_i - x_j|| > ||x_p - x_k||) \\tag{30}\nwhere,\n\\delta(\\cdot) = 1 if the corresponding inequality is satisfied and \\delta(\\cdot) = 0 otherwise.\nIn terms of computational complexity, this index requires the computation of all pairwise distances between objects, which itself is O(mN^{2}). In addition, computing s(+) and s(-) requires O(\\frac {N^{4}}{k}) time, so the total computational cost of the Gamma index is O(mN^{2} + \\frac {N^{4}}{k}). This index should almost never be used for large data sets as it is very resource heavy. The performance of this particular index, along with the G+ and Tau indices, will be discussed in the performance section later on."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#g-plus-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#g-plus-index",
    "title": "Cluster Validity Indices",
    "section": "4.12 G-Plus Index",
    "text": "4.12 G-Plus Index\nThe G+ index is another metric based on the relationships between just the discordant pairs of objects, s(-). It is simply the proportion of discordant pairs with respect to the maximum number of possible comparisons. The optimal number of clusters in the data is given by the minimum value of the index.\nMathematically, the index can be computed as:\nG+ = \\frac {2s(-)}{N_t * (N_t - 1)} \\tag{31}\nThe computational run time of the G+ index is also O(mN^{2} + \\frac {N^{4}}{k}) and as such should not be used for larger data sets."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#hartigan-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#hartigan-index",
    "title": "Cluster Validity Indices",
    "section": "4.13 Hartigan Index",
    "text": "4.13 Hartigan Index\nThe Hartigan index is based on the Euclidean within-cluster sums of squares. The optimal number of clusters is given by the maximum difference between successive clustering solutions.\nMathematically, it can be expressed as:\nHartigan = (W_k / W_{k+1}) * (N - K - 1) \\tag{32}\nwhere W_k = \\displaystyle \\sum ||x_i - \\bar{x_i}|| is the within-cluster dispersion matrix for the data clustered into K clusters\nThe computational run time of the Hartigan index is O(n(k^{2} + m)) and is discussed in further detail ahead."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#log-ss-ratio-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#log-ss-ratio-index",
    "title": "Cluster Validity Indices",
    "section": "4.14 Log SS Ratio Index",
    "text": "4.14 Log SS Ratio Index\nThis index, from J. A. Hartigan. Clustering algorithms. New York: Wiley, 1975. considers the within (WGSS) and between (BGSS) cluster sums of squares. It should be noted that the log used is base 10, not e.\nThe time complexity of this index is, at worst, O(n(k^{2} + m)) and at best O(nm) when k^{2} << m. Thus, this index is ideal for data with naturally large clusters."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#marriot-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#marriot-index",
    "title": "Cluster Validity Indices",
    "section": "4.15 Marriot Index",
    "text": "4.15 Marriot Index\nThe Marriot Index Marriot, 1971 is based on the determinant of the within-group covariance matrix.\nMathematically, it is computed as:\nk^{2} * det(W_{k}) \\tag{33}\nThe suggested optimal number of clusters is based on the maximum difference between successive levels of the Marriot index. The overall computational complexity of the Marriot index is O(m^{2}N + m^{3}), making it suitable for large datasets with a medium number of dimensions."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#mcclain-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#mcclain-index",
    "title": "Cluster Validity Indices",
    "section": "4.16 McClain Index",
    "text": "4.16 McClain Index\nThe McClain and Rao index McClain and Rao, 1975 is the ratio of the average within cluster distance - divided by the number of within cluster distances - and the average value between cluster distances - divided by the number of cluster distances.\nMathematically, it is formed by:\nMcClain = \\frac {\\bar{S_w}}{\\bar{S_b}}  = \\frac {S_w / N_w}{S_b / N_b} \\tag{34}\nThe optimal suggested number of clusters corresponds to the minimum value of the index. Finally, in terms of computational complexity, the McClain Rao index runs in O(mN^{2}) time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#point-biserial-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#point-biserial-index",
    "title": "Cluster Validity Indices",
    "section": "4.17 Point-Biserial Index",
    "text": "4.17 Point-Biserial Index\nThe Point Biserial index is the point-biserial correlation between the dissimilarity matrix and a corresponding matrix consisting of either 0’s or 1’s. A value of 0 is assigned if the two corresponding points are clustered together by the algorithm and 1 otherwise.\nMathematically, the index is defined as:\nPB = \\displaystyle \\frac {[\\bar{S_b} - \\bar{S_w}] * \\sqrt{[N_w*N_b/N_t^2]}}{s_d} \\tag{35}\nwhere,\n\\bar{S_w} = S_{w} / N_{w} \\tag{36}\nand,\n\\bar{S_b} = S_b / N_b \\tag{37}\nand,\ns_d \\tag{38}\nis the standard deviation over all of the distances\nand,\nN_k = The number of data points in each cluster, so an array of length K\nN_{T} = \\frac {N*(N-1)}{2} = Total number of pairs of distinct points in the data set.\nN_{W} = \\displaystyle \\frac {n_k * (n_k - 1)}{2} = The number of paris of points belonging to clusters k and k'.\nN_{B} = \\displaystyle \\sum_{k<k'} {n_k * (n_{k'})} = The number of pairs of points belonging to clusters k and k' where k and k' are not the same cluster.\nIn terms of computational complexity, the Point-Biserial index takes O(nm^{2}) time to be computed."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#pseudo-t2-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#pseudo-t2-index",
    "title": "Cluster Validity Indices",
    "section": "4.18 Pseudo T2 Index",
    "text": "4.18 Pseudo T2 Index\nThe Pseudo t^{2} index from Duda and Hart, 1973 can only be applied to hierarchical clustering methods, but is included for all clustering methods in the proposed Python package. This is because the user should understand that the metric should only be used for that specific method.\nMathematically, the index is computed as:\nPseudo-t^{2} = \\frac {V_{kl}}{\\frac {W_k + W_l}{n_k + n_l - 2}} \\tag{39}\nThe suggested optimal number of clusters is based on the smallest q such that:\nPseudo-t^{2} \\le (\\displaystyle \\frac {1 - CritValue}{CritValue}) \\times (n_k + n_l -2) \\tag{40}"
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#ratkowsky-lance-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#ratkowsky-lance-index",
    "title": "Cluster Validity Indices",
    "section": "4.19 Ratkowsky-Lance Index",
    "text": "4.19 Ratkowsky-Lance Index\nThe Ratkowsky and Lance, 1978 index is based on the average of the ratio between the between-group sum of squares and the total sum of squares for each variable.\nMathematically, it is computed as:\nRL = \\frac {\\bar{S}}{K^{1/2}} \\tag{41}\nwhere,\n\\bar{S^{2}} = \\frac {1}{p}\\displaystyle \\sum_{j=1}^{p} \\frac {BGSS_j}{TSS_j} \\tag{42}\nand,\nBGSS_j = \\displaystyle \\sum_{k=1}^{m}n_k*(c_{kj} - \\bar{x_j})^{2} \\tag{43}\nand,\nTSS_j = \\displaystyle \\sum_{i=1}^{n} (x_{ij} - \\bar{x_j})^{2} \\tag{44}\nThe optimal number of clusters is suggested by the maximum value of the index. Finally, the index runs in O(m(k^{2} + N)) time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#ray-turi-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#ray-turi-index",
    "title": "Cluster Validity Indices",
    "section": "4.20 Ray-Turi Index",
    "text": "4.20 Ray-Turi Index\nThis index, proposed by Ray and Turi in 1999, came off of the heels of image segmentation, which separates parts of images into differing components. The index is simply the ratio between the intra-cluster distance and the inter-cluster distance.\nMathematically, it can be written as:\nRT = \\frac {intra}{inter} \\tag{45}\nwhere:\nintra = \\frac {1}{N} \\displaystyle \\sum_{i=1}^{k} \\sum ||x - c_{k}||^{2} \\tag{46}\nand,\ninter = min(||c_{k} - c_{k+1}||^{2}) \\tag{47}\nand c_k is the centroid of cluster C_{k}\nSince the goal is to minimize the intra-cluster distance - the numerator - the optimal cluster partitioning corresponds to the minimum value of the Ray-Turi index."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#r-squared-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#r-squared-index",
    "title": "Cluster Validity Indices",
    "section": "4.21 R Squared Index",
    "text": "4.21 R Squared Index\nWhile not included as part of the NbClust standard library, the R^{2} index is inherently necessary to compute, since it is relied upon by the cubic clustering criterion, as mentioned above. The maximum difference between successive clustering levels is regarded as the best partition. The mathematical formulation is as follows:\nR^{2} = 1 - \\frac {p  + \\sum u^{2}_j}{\\sum u^{2}_j} \\tag{48}\nR^{2} = 1 - \\frac {tr(X^{T}X - \\bar{X}^{T}Z^{T}Z\\bar{X})}{tr(X^{T}X)} \\tag{49}\nwhere:\n\\bar{X} = (Z^{T}Z)^{-1}Z^{T}X \\tag{50}\nand,\nX^{T}X \\tag{51}\nis the total sum of squares and cross-products matrix of shape (p \\times p)."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#rubin-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#rubin-index",
    "title": "Cluster Validity Indices",
    "section": "4.22 Rubin Index",
    "text": "4.22 Rubin Index\nThe Rubin Index, proposed by Friedman and Rubin in 1967, is based on the ratio of the determinants of the data covariance matrix and within-group covariance matrix.\nMathematically, it can be formed like so:\nRubin = \\frac {det(T)}{det(W_k)} \\tag{52}\nThe optimal clustering solution is given by the minimum value of the second differences between clustering levels. In terms of computational complexity, the Rubin index runs in O(m^{2}N + m^{3}) time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#scott-symons-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#scott-symons-index",
    "title": "Cluster Validity Indices",
    "section": "4.23 Scott-Symons Index",
    "text": "4.23 Scott-Symons Index\nThe Scott-Symons index, Scott and Symons, 1971, uses information from the determinants of the data covariance matrix and the within-group covariance matrix, similar to the Rubin index.\nMathematically, it can be written as:\nn*log(\\frac {det(T)}{det(W_k)}) \\tag{53}\nwhere:\nT = \\displaystyle \\sum_{i=1}^{N} (x_i - \\bar{x})(x_i - \\bar{x})^{T}\nand,\nW_k = \\displaystyle \\sum (x_i - \\bar{x_m})(x_i - \\bar{x_m})^{T} \\tag{54}\nThe optimal suggested number of clusters is given by the maximum value of the index. Finally, the index runs in O(m^{2}N + m^{3}) time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#sd-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#sd-index",
    "title": "Cluster Validity Indices",
    "section": "4.24 SD Index",
    "text": "4.24 SD Index\nThe SD validity index is based on the concepts of average scattering for clusters and total separation between clusters.\nMathematically, it is computed as:\nSDindex(q) = \\alpha*Scat(q) + Dis(q) \\tag{55}\nThe scattering term, or the average compactness of clusters, is computed as:\nScat(q) = \\frac {\\frac {1}{q} \\sum ||\\sigma^{k}||}{||\\sigma||} \\tag{56}\nwhere:\nDis(q) = \\frac {D_{max}}{D_{min}} \\displaystyle \\sum_{k=1}^{q} (\\sum_{z=1}^{q}||c_k - c_z||)^{-1} \\tag{57}\nand D_{max} is the maximum distance between cluster centers and D_{min} is the minimum distance between cluster centers. In addition, \\sigma is the vector of variances for each variable in the data set and \\sigma^{k} is the variance vector for each cluster, C_k.\nIt should be noted that \\alpha is a weighting factor equal to Dis(q_{max}) where q_{max} is the maximum number of clusters. The optimal suggested number of clusters is indicated by the minimum value of the SD index."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#sdbw-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#sdbw-index",
    "title": "Cluster Validity Indices",
    "section": "4.25 SDbw Index",
    "text": "4.25 SDbw Index\nThe SDbw index is based on the compactness and separation between clusters.\nMathematically, it can be computed as:\nSDbw(q) = Scat(q) + Density(q) \\tag{58}\nwhere:\nDensity(q) = \\frac {1}{q(q-1)} \\displaystyle \\sum_{i=1}^{q} (\\sum_{j=1}^{q} \\frac {density(u_{ij})}{max(density(c_i), density(c_j)}) \\tag{59}\nand,\ndensity(u_{ij}) = \\displaystyle \\sum_{l=1}^{n_{ij}} f(x_l, u_{ij}) \\tag{60}\nThe Scat(q) term is calculated in the same way as above. The new term - Density(q) - is the inter-cluster density, which is the average density in the region among clusters in relation to the density of the clusters.\nThe optimal number of clusters is the partition that minimizes the SDbw index."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#silhouette-score",
    "href": "posts/clustering-indices-project/cluster-indices.html#silhouette-score",
    "title": "Cluster Validity Indices",
    "section": "4.26 Silhouette Score",
    "text": "4.26 Silhouette Score\nIntroduced by Rousseeuw in 1987, the Silhouette Coefficient is composed of two scores:\n\na: The mean distance between a sample and all other points in the same class\nb: The mean distance between a sample and all other points in the next nearest cluster\n\nMathematically, this can be written as:\ns = \\frac {b - a}{max(a, b)} \\tag{61}\nAdditionally, the Silhouette Score for a set of sample is just the mean Silhouette Coefficient for each sample, like so:\nSilhouette = \\frac {\\sum_{i=1}^{n} S(i)}{n} \\tag{62}\nwhere:\na = \\frac {\\sum d_{ij}}{n_r - 1} \\tag{63}\nand,\nb = min(d_{iCs}) \\tag{64}\nand,\nd_{iCs} = \\frac {\\sum d_{ij}}{n_s} \\tag{65}\nThe optimal number of clusters is indicated by the maximum value of the index. Scores around 0 indicate overlapping clusters. Finally, the index can be calculated in O(mN^{2}) time."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#tau-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#tau-index",
    "title": "Cluster Validity Indices",
    "section": "4.27 Tau Index",
    "text": "4.27 Tau Index\nThe Tau index is based on the \\tau correlation between the matrix that stores all of the distances between pairs of observations and a matrix that represents whether a given pair of observations belongs to the same cluster or not. It can be written in terms of concordant and discordant pairs of observations, just like the Gamma and G+ indices.\nMathematically, it can be written as:\n\\displaystyle \\frac {s(+) - s(-)}{\\sqrt{[(N_t (N_t - 1) / (2 - t)) * (N_t (N_t - 1) / 2)]}} \\tag{66}\nwhere N_t is the total number of distances and t is the number of comparisons between two pairs of observations.\nThe optimal suggested clustering solution is given by the maximum value of this index. In terms of computational complexity, the Tau index runs in the same time as the Gamma index; namely, O(mN^{2} + \\frac {N^{4}}{k}) making it suitable for small datasets."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#trace-cov-w-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#trace-cov-w-index",
    "title": "Cluster Validity Indices",
    "section": "4.28 Trace-Cov-W Index",
    "text": "4.28 Trace-Cov-W Index\nThe Trace(Cov(W)) index involves using the pooled covariance matrix instead of the within-group covariance matrix.\nMathematically, it can be written as:\nTraceCovW = trace(COV[W_k]) = trace(W_p) \\tag{67}\nwhere:\nW_k = \\displaystyle \\sum \\sum (x_i - c_k)(x_i - c_k)^{T} \\tag{68}\nand,\nW_p = \\frac {1}{N-k} * \\displaystyle \\sum_{i=1}^{k}W_i \\tag{69}\nand,\nW_i = \\displaystyle \\sum(x_i - \\bar{x_i})(x_i - \\bar{x_i})^{T} \\tag{70}\nThe optimal suggested number of clusters is indicated by the maximal difference in index scores. Finally, in terms of computational efficiency, the index can be calculated in O(mN) time, making it ideal for larger data sets."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#trace-w-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#trace-w-index",
    "title": "Cluster Validity Indices",
    "section": "4.29 Trace-W Index",
    "text": "4.29 Trace-W Index\nThe Trace(W) index is a simple, difference-like criterion and is one of the most popular choices when selecting the appropriate number of clusters.\nMathematically, it can be calculated as:\nTrace(W) = trace(W_k) \\tag{71}\nwhere W_k is the within-group covariance matrix.\nThe optimal suggested number of clusters corresponds to the maximum value of the second differences of the index. Finally, in terms of computational complexity, the index runs in O(mN) time, making it ideal for large data sets."
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#wemmert-gancarski-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#wemmert-gancarski-index",
    "title": "Cluster Validity Indices",
    "section": "4.30 Wemmert-Gancarski Index",
    "text": "4.30 Wemmert-Gancarski Index\nThe Wemmert-Gancarski index, is simply the weighted mean of the quotient of distances between a set of points and the barycenter of the cluster those points belong to.\nMathematically, it can be calculated as:\nWG = \\displaystyle \\frac {1}{N} \\sum_{k=1}^{K} \\max{(0, n_k - \\sum_{i \\in I_k} R(M_i))} \\tag{72}\nwhere:\nR(M_i) = \\displaystyle \\frac {||M_i - G^{k}||}{\\displaystyle \\min_{k \\ne k'} ||M_i - G^{k'}||} \\tag{73}\nand,\nn_k = |x_{k}| \\tag{74}\nis the cardinality / number of points in a particular cluster\nand:\n\\displaystyle \\sum_{k=1}^{K} n_k = N \\tag{75}"
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#xie-beni-index",
    "href": "posts/clustering-indices-project/cluster-indices.html#xie-beni-index",
    "title": "Cluster Validity Indices",
    "section": "4.31 Xie-Beni Index",
    "text": "4.31 Xie-Beni Index\nThe Xie-Beni index is an index primarily applied to fuzzy clustering solutions. It is defined as the quotient between the mean quadratic error and the minimum of the minimal squared distances between the points in the clusters.\nMathematically, the index can be calculated as:\nXB = \\frac {1}{N} * \\frac {WGSS}{min(D(C_k, C_{k'})^{2})} \\tag{76}\nwhere:\nD(C_k, C_{k'}) = min(d(M_i, M_j)) \\tag{77}\nand d(M_i, M_j) is the distance between the cluster centroids.\nThe optimal suggested number of clusters corresponds to the minimum value of the index.\n\n\nCode\nimport sklearn.cluster\nimport scipy.cluster\nimport sklearn.datasets\nimport numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom scipy.spatial.distance import pdist, cdist, euclidean\nfrom sklearn.datasets import make_blobs\nfrom timeit import default_timer as timer\npd.set_option('display.max_columns', 30)\n\nsns.set_context('poster')\nsns.set_palette('Paired', 10)\nsns.set_color_codes()\n\n\nCreating sample data-frames to cluster via NumPy:\n\n\nCode\ndataset_sizes = np.hstack([np.arange(1, 6) * 500, np.arange(3,7) * 1000, np.arange(4,17) * 2000])\n\n\nPlotting the results:"
  },
  {
    "objectID": "posts/clustering-indices-project/cluster-indices.html#indices-performance",
    "href": "posts/clustering-indices-project/cluster-indices.html#indices-performance",
    "title": "Cluster Validity Indices",
    "section": "5.2 Indices Performance",
    "text": "5.2 Indices Performance\nThis part will pertain to demonstrating the efficiency of the various clustering metrics offered by the proposed library. In addition, it will detail the various methods used to improve upon the efficiency of previous implementations, if they exist.\n\n\nCode\ndef benchmark_algorithm(dataset_sizes, indice_function, function_args, function_kwds,\n                        n_columns=10, n_clusters=3, max_time=100, repeats=3):\n    \n    # * Initialize the result with NaNs so that any unfilled entries \n    # * will be considered NULL when we convert to a pandas dataframe at the end\n    result = np.nan * np.ones((len(dataset_sizes), repeats))\n    for index, size in enumerate(dataset_sizes):\n        for s in range(repeats):\n            # * Use sklearns make_blobs to generate a random dataset with specified size\n            # * dimension and number of clusters\n            data, labels = make_blobs(n_samples=size, \n                                      n_features=n_columns, \n                                      centers=3)\n\n            # * Start the clustering with a timer\n            start_time = timer()\n            indice_function(data, labels, *function_args, **function_kwds)\n            time_taken = np.round(timer() - start_time, 4)\n            print(f\"Run complete for size: {size}; took {np.round(time_taken, 4)} seconds\")\n            # * If we are taking more than max_time then abort -- we don't\n            # * want to spend excessive time on slow algorithms\n            if time_taken > max_time:\n                result[index, s] = time_taken\n                return pd.DataFrame(np.vstack([dataset_sizes.repeat(repeats), \n                                               result.flatten()]).T, columns=['Size','Seconds'])\n            else:\n                result[index, s] = time_taken\n        \n    # * Return the result as a dataframe for easier handling with seaborn afterwards\n    return pd.DataFrame(np.vstack([dataset_sizes.repeat(repeats), \n                                   result.flatten()]).T, columns=['Size','Seconds'])\n\n\n\n5.2.1 Ball-Hall Index:\nThe Ball-Hall Index is a relatively simple index that is essentially the mean dispersion of all points, relative to the cluster they belong to. As such, the index can be computed in O(nm) time, making it ideal for larger datasets since it only requires computing the centroids of each cluster and the distances of each point within each cluster.\nThe author’s first attempt at implementing the index was unsuccessful, mostly due to a lack of understanding of the index. As a result, the initial run will not be used as a comparison and instead, only the NbClust version and the author’s new version will be compared.\nStarting with the NbClust algorithm first, for a dataset with shape (10000, 5) and K=3, the mean time to compute the index was 0.01027 seconds for 100 trial runs. In addition, the mean amount of resources used to compute the index was around 14.2 MiB. In contrast, the new, working implementation included in the proposed library had a mean run time of 0.00257 seconds, using the same circumstances as the first experiment. In addition, the mean amount of memory used was around 0.059 MiB, used to store the K new, split dataframes from the for-loop.\nAll together, the new, proposed implementation offers about a 9.5 \\times speedup and uses around 0.41 \\% of the memory. Reasons for these improvements come mostly from NbClust’s use of the sweep function to compute the mean differences, as well as the multiple uses of the split function, which splits a dataframe into chunks.\n\n\n5.2.2 C Index:\nThe author does not believe the NbClust library calculates the C index correctly. For instance, while the source code for the C index correctly calculates the number of within-cluster distances, N_W, it does not correctly compute the sum of the N_W largest/smallest pairwise distances, S_{min} and S_{max} respectively. In addition, it does not utilize a sorting/partitioning method of any kind, which is necessary for finding the largest/smallest pairwise distances.\nTo alleviate this problem, I rewrote the index to account for all parts of the original equation, from Hubert and Levin, 1976.\nIn terms of performance, the NbClust implementation will not be considered, for reasons listed above. It should be noted that a previous implementation of the C index was made available in the Python language by John Vorsten in 2020, around the same time that I was working on this very project.\nAs stated previously, the C index can be computed in O(m^{2}(n + log_2m)) time, which can be prohibitive if the number of variables used in the clustering is large. For reference, when N = 10,000, the number of pairwise distances is 49,995,000; for N observations, the number of pairwise distances is N(N-1)/2. Using the above example, the mean time to compute the index via John’s implementation was 27.5 seconds, compared to the proposed library’s mean run-time of 8.31 seconds.\nThe largest bottleneck in computing this index is calculating S_{min} and S_{max}, because it requires - at minimum - O(n+ n\\log N_{W}) time to locate the N_{W} smallest/largest pairwise distances. The main difference between the proposed library’s implementation and John’s is the calculation of S_{min} and S_{max}. In particular, John’s approach uses Numpy’s sort function, which by itself run’s in O(n^{2}) time, because the underlying algorithm used is a quicksort. However, the new library utilizes Numpy’s partition function, which runs in O(n) time, via the introselect algorithm, and does not require sorting the N_{W} largest/smallest arrays.\n\n\n5.2.3 Dunn Index:\nThe Dunn index requires the computation of the entire pairwise distance matrix, just like the C index above, and has a time complexity of O(N^{2}m). This becomes a problem as N grows large, but can be dealt with by precomputing the distance matrix and storing it in memory to query when needed. This reduces the time complexity of the algorithm to just O(K^{2}), where K is the number of clusters under consideration.\nIn a more practical setting, for a dataset with dimensions (10000, 5) and K = 3, the mean time to compute the Dunn index, without precomputing the pairwise distances, is 4.72 seconds via the proposed Python library. In contrast, the mean time to compute the same index with the same controls in place is 10.84 seconds via the NbClust library. However, when utilizing a pre-computed distance matrix, the times fall to 4.03 seconds and 9.42 seconds for the proposed library and NbClust library, respectively.\nThe reasoning behind the O(K^{2}) solution lies behind the fact that in order find the maximum diameter / minimum intercluster distance, it requires a nested for-loop, which is used to go through each of the K labels. Indexing an array takes O(1) time, so that lookup is dominated by the outer loops in the expression.\n\n\n5.2.4 Hartigan Index:\nHartigan is credited with two internal clustering indices, the Hartigan Index and the Log Between/Within SS Index. This section will focus on the Hartigan index mentioned in the NbClust library. It was previously mentioned that the index runs in O(n(k^{2} + m)) time, which can be quite fast when the number of requested clusters is small and the number of observations is also relatively small.\nIn the original implementation from the NbClust library, using the same setup as above, the index had a mean run-time of 0.0286 seconds and used about 0.92 MiB of memory to compute. In contrast, the proposed library had a mean run-time of 0.0046 seconds and used about 0.004 MiB of memory. For easier reference, the proposed library offers around a 6 \\times performance gain while utilizing about 230 \\times less memory.\nIt should be noted that both implementations perform fairly well on most datasets, so long as the dimensions of the dataset are of medium-size or lower.\n\n\n5.2.5 Point-biserial Index:\nAs mentioned earlier, the point-biserial index, referenced as the PB index from here on, runs in O(nm^{2}) time, where m is the number of variables being clustered. However, an original implementation of the algorithm to compute the PB index runs in O(n^{2}m) time, on average. For smaller inputs, this difference in run-time may be negligible; however, for larger datasets, the speed-up is quite dramatic. Using posterior analysis, the proposed library offers a speed-up in run-time up to 2600% (n=10,000).\nThe main bottlenecks to the original implementation were, in decreasing order of execution time: the double for-loop to compute the distances and the design arrays (\\mu = 112.5 seconds), converting the design array to a factor (\\mu = 84 seconds), and garbage collection (\\mu = 43.5 seconds).\nA few methods were employed to improve upon the efficiency of the R implementation. Firstly, the design vector was discarded in favor of computing the number of points within/between the given clusters (N_W, N_B), which both run in O(k) time instead of O(n^{2}m) time, since the most tedious computation is counting the number of labels in each cluster. In addition, the biserial function used in NbClust’s version uses a separate code block to compute the biserial correlation between the design matrix and the distances, even though both libraries employ the following equality:\nPB = s_n * r_{pb}(A, B) = [(S_W / N_W - S_B / N_B) * \\displaystyle \\frac {\\sqrt{N_W * N_B}}{N_T}] / s_d \\tag{78}\n\n\n5.2.6 R^{2} Index:\nThe R^{2} Index has the usual interpretation of the amount of variation explained by the clustering solution. It should also be noted that the R^{2} index is not the best method of criterion if your clustering solution is irregularly shaped or highly elongated.\nRegarding the time complexity, the original implementation of the algorithm ran in O(k*n^{2}*m^{2}) time. This is because the algorithm has to, for each cluster k and for each variable m: compute the euclidean distance between the cluster center and the observation n, write the product to an array, and then sum the result. The reasoning for the O(n^{2}) in the computation is because the euclidean distance requires O(nm) time to be computed. For datasets with lots of observations and many variables to cluster, this time can be unfeasible to work with.\nThe new algorithm offers a 812x speedup in compute time, since it does not rely on a double for-loop or the computation of euclidean distances. Rather, it takes the raw data matrix and labels and computes a series of matrix multiplications.\n\n\n5.2.7 Wemmert-Gancarski Index:\nThe Wemmert-Gancarski Index was an interesting metric since there was not a lot of literature regarding this index. For reference, the main source for the mathematical formulation of this index was the clusterCrit library vignette.\nInitially, the index seemed a bit daunting since a plethora of methods were tried but none of them worked out. Instead, it required a pivot in thinking about how to structure and implement; rather than thinking about building the index in terms of what the output of each step should be, a shape-first approach was taken. More specifically, there were 5 steps to successfully implement the index using the approach of output shapes rather than just the output itself. A diagram of the author’s approach is shown below for easier reference:\n\n\nWG Index:\n\nRegarding the computational cost of computing the WG index, the modified OpenEnsembles version of the WG index had a mean run time of 2.74 seconds; this is in stark contrast to the author’s proposed implementation that runs in 0.118 seconds. The proposal is not only more than 23 times faster than the previous implementation, but it also uses less memory as well (not a significant difference). Using the same setup as the other indices previously described, the mean amount of memory used per run for the new algorithm was around 0.578 MiB, compared to the original implementation’s 0.622 MiB memory usage; or 8\\% less memory.\nIt should be noted that there were two resource-demanding tasks in the proposed implementation: namely, finding a quotient and finding the distances from each point to its centroid. Finding the quotient between the distance from a point to its barycenter and the minimum distance from a point and all of the other barycenters took, on average, only 0.0013 seconds. Finding the distances from all of the points to their centroids resulted in a mean run time of about 0.003 seconds, using the same setup as above. Finally, the same tasks that were bottlenecks for the new version were also present in the old implementation, but the key difference lies in how those obstacles were handled.\n\n\n5.2.8 Xie-Beni Index:\nThe Xie-Beni Index defines the inter-cluster separation as the minimum square distance between cluster centers, and the intra-cluster compactness as the mean square distance between each data object and its cluster center.\nIn terms of computational complexity, the original implementation of the Xie-Beni index ran in O(k*n^{2}*m^{2}) time. With similar reasoning as the R^{2} index, this can be costly to run for larger datasets, especially if the true clustering solution is not fuzzy, which this index was specifically created for. The updated implementation of the index runs in O(n^{2}) time, necessary for the computation of the distance matrix. In addition, the new algorithm offers up to a 291x speedup compared to the author’s previous application.\n\n\nSpeed-testing the Xie-Beni Index:\n\n\nCode\n# Old version:\ndef oldXB(X, labels):\n    X = pd.DataFrame(X)\n    numCluster = int(max(labels) + 1)\n    numObject = len(labels)\n    sumNorm = 0\n    list_centers = []\n\n    for i in range(numCluster):\n        # get all members from cluster i\n        indices = [t for t, x in enumerate(labels) if x == i]\n        clusterMember = X.iloc[indices, :]\n        # compute the cluster center\n        clusterCenter = np.mean(clusterMember, 0)\n        list_centers.append(clusterCenter)\n        # iterate through each member of the cluster\n        for member in clusterMember.iterrows():\n            sumNorm = sumNorm + np.power(euclidean(member[1], clusterCenter), 2)\n\n    minDis = min(pdist(list_centers))\n\n    # compute the fitness\n    score = sumNorm / (numObject * pow(minDis, 2))\n    return score\n\ndef centers2(X, labels):\n    x = pd.DataFrame(X)\n    labels = np.array(labels)\n    k = int(np.max(labels) + 1)\n    n_cols = x.shape[1]\n    centers = np.array(np.zeros(shape=(k, n_cols)))\n\n    # Getting the centroids:\n    for i in range(k):\n        centers[i, :] = np.mean(x.iloc[labels == i], axis=0)\n\n    return centers\n\n# New version:\ndef myXB(X, labels):\n    \"My version of computing the Xie-Beni index.\"\n    \n    nrows = X.shape[0]\n\n    # Get the centroids:\n    centroids = centers2(X, labels)\n\n    # Computing the WGSS:\n    def getMinDist(obs, code_book):\n            dist = cdist(obs, code_book)\n            code = dist.argmin(axis=1)\n            min_dist = dist[np.arange(len(code)), code]\n            return min_dist\n                \n    euc_distance_to_centroids = getMinDist(X, centroids)\n\n    WGSS = np.sum(euc_distance_to_centroids**2)\n\n    # Computing the minimum squared distance to the centroids:\n    MinSqCentroidDist = np.min(pdist(centroids, metric='sqeuclidean'))\n\n    # COmputing the XB index:\n    xb = (1 / nrows) * (WGSS / MinSqCentroidDist)\n\n    return xb\n\n\n\n\nCode\noldxb = benchmark_algorithm(dataset_sizes, oldXB, (), {})\n\n\nRun complete for size: 500; took 0.0439 seconds\nRun complete for size: 500; took 0.0422 seconds\nRun complete for size: 500; took 0.0408 seconds\nRun complete for size: 1000; took 0.0748 seconds\n\n\nRun complete for size: 1000; took 0.0761 seconds\nRun complete for size: 1000; took 0.0814 seconds\nRun complete for size: 1500; took 0.1104 seconds\n\n\nRun complete for size: 1500; took 0.1092 seconds\nRun complete for size: 1500; took 0.1027 seconds\n\n\nRun complete for size: 2000; took 0.1386 seconds\nRun complete for size: 2000; took 0.1345 seconds\n\n\nRun complete for size: 2000; took 0.1965 seconds\n\n\nRun complete for size: 2500; took 0.2259 seconds\nRun complete for size: 2500; took 0.1997 seconds\n\n\nRun complete for size: 2500; took 0.1871 seconds\n\n\nRun complete for size: 3000; took 0.2377 seconds\n\n\nRun complete for size: 3000; took 0.2225 seconds\n\n\nRun complete for size: 3000; took 0.3046 seconds\n\n\nRun complete for size: 4000; took 0.2689 seconds\n\n\nRun complete for size: 4000; took 0.2988 seconds\n\n\nRun complete for size: 4000; took 0.3213 seconds\n\n\nRun complete for size: 5000; took 0.3179 seconds\n\n\nRun complete for size: 5000; took 0.3531 seconds\n\n\nRun complete for size: 5000; took 0.3521 seconds\n\n\nRun complete for size: 6000; took 0.3976 seconds\n\n\nRun complete for size: 6000; took 0.4837 seconds\n\n\nRun complete for size: 6000; took 0.4561 seconds\n\n\nRun complete for size: 8000; took 0.5604 seconds\n\n\nRun complete for size: 8000; took 0.5961 seconds\n\n\nRun complete for size: 8000; took 0.568 seconds\n\n\nRun complete for size: 10000; took 0.7559 seconds\n\n\nRun complete for size: 10000; took 0.6787 seconds\n\n\nRun complete for size: 10000; took 0.6996 seconds\n\n\nRun complete for size: 12000; took 0.9949 seconds\n\n\nRun complete for size: 12000; took 0.9771 seconds\n\n\nRun complete for size: 12000; took 1.0966 seconds\n\n\nRun complete for size: 14000; took 1.2651 seconds\n\n\nRun complete for size: 14000; took 1.2118 seconds\n\n\nRun complete for size: 14000; took 1.3354 seconds\n\n\nRun complete for size: 16000; took 1.3454 seconds\n\n\nRun complete for size: 16000; took 1.2079 seconds\n\n\nRun complete for size: 16000; took 1.0968 seconds\n\n\nRun complete for size: 18000; took 1.2166 seconds\n\n\nRun complete for size: 18000; took 1.215 seconds\n\n\nRun complete for size: 18000; took 1.2833 seconds\n\n\nRun complete for size: 20000; took 1.4117 seconds\n\n\nRun complete for size: 20000; took 1.3423 seconds\n\n\nRun complete for size: 20000; took 1.3897 seconds\n\n\nRun complete for size: 22000; took 1.4809 seconds\n\n\nRun complete for size: 22000; took 1.5475 seconds\n\n\nRun complete for size: 22000; took 1.492 seconds\n\n\nRun complete for size: 24000; took 1.674 seconds\n\n\nRun complete for size: 24000; took 1.633 seconds\n\n\nRun complete for size: 24000; took 1.6321 seconds\n\n\nRun complete for size: 26000; took 1.7576 seconds\n\n\nRun complete for size: 26000; took 1.7878 seconds\n\n\nRun complete for size: 26000; took 1.7258 seconds\n\n\nRun complete for size: 28000; took 1.8395 seconds\n\n\nRun complete for size: 28000; took 1.9374 seconds\n\n\nRun complete for size: 28000; took 1.8941 seconds\n\n\nRun complete for size: 30000; took 2.0428 seconds\n\n\nRun complete for size: 30000; took 2.041 seconds\n\n\nRun complete for size: 30000; took 2.0524 seconds\n\n\nRun complete for size: 32000; took 2.1921 seconds\n\n\nRun complete for size: 32000; took 2.2152 seconds\n\n\nRun complete for size: 32000; took 2.154 seconds\n\n\n\n\nCode\nnewXB = benchmark_algorithm(dataset_sizes, myXB, (), {})\n\n\nRun complete for size: 500; took 0.0035 seconds\nRun complete for size: 500; took 0.003 seconds\nRun complete for size: 500; took 0.0029 seconds\nRun complete for size: 1000; took 0.0031 seconds\nRun complete for size: 1000; took 0.0032 seconds\nRun complete for size: 1000; took 0.0021 seconds\nRun complete for size: 1500; took 0.0026 seconds\nRun complete for size: 1500; took 0.0028 seconds\nRun complete for size: 1500; took 0.0022 seconds\nRun complete for size: 2000; took 0.0023 seconds\nRun complete for size: 2000; took 0.0021 seconds\nRun complete for size: 2000; took 0.0027 seconds\nRun complete for size: 2500; took 0.0028 seconds\nRun complete for size: 2500; took 0.0025 seconds\nRun complete for size: 2500; took 0.0023 seconds\nRun complete for size: 3000; took 0.0022 seconds\nRun complete for size: 3000; took 0.0028 seconds\nRun complete for size: 3000; took 0.0022 seconds\nRun complete for size: 4000; took 0.0025 seconds\nRun complete for size: 4000; took 0.0025 seconds\nRun complete for size: 4000; took 0.0028 seconds\nRun complete for size: 5000; took 0.0027 seconds\nRun complete for size: 5000; took 0.0027 seconds\nRun complete for size: 5000; took 0.003 seconds\nRun complete for size: 6000; took 0.0029 seconds\nRun complete for size: 6000; took 0.0027 seconds\nRun complete for size: 6000; took 0.003 seconds\nRun complete for size: 8000; took 0.0031 seconds\nRun complete for size: 8000; took 0.0032 seconds\nRun complete for size: 8000; took 0.0031 seconds\nRun complete for size: 10000; took 0.0034 seconds\nRun complete for size: 10000; took 0.004 seconds\nRun complete for size: 10000; took 0.0036 seconds\nRun complete for size: 12000; took 0.0037 seconds\nRun complete for size: 12000; took 0.004 seconds\nRun complete for size: 12000; took 0.0038 seconds\n\n\nRun complete for size: 14000; took 0.0049 seconds\nRun complete for size: 14000; took 0.0046 seconds\nRun complete for size: 14000; took 0.0047 seconds\nRun complete for size: 16000; took 0.0045 seconds\nRun complete for size: 16000; took 0.0045 seconds\nRun complete for size: 16000; took 0.0041 seconds\nRun complete for size: 18000; took 0.0052 seconds\nRun complete for size: 18000; took 0.0049 seconds\nRun complete for size: 18000; took 0.0058 seconds\nRun complete for size: 20000; took 0.0052 seconds\nRun complete for size: 20000; took 0.0054 seconds\nRun complete for size: 20000; took 0.0056 seconds\nRun complete for size: 22000; took 0.0063 seconds\nRun complete for size: 22000; took 0.0065 seconds\nRun complete for size: 22000; took 0.0067 seconds\n\n\nRun complete for size: 24000; took 0.0066 seconds\nRun complete for size: 24000; took 0.0068 seconds\nRun complete for size: 24000; took 0.0065 seconds\nRun complete for size: 26000; took 0.0069 seconds\nRun complete for size: 26000; took 0.0068 seconds\nRun complete for size: 26000; took 0.007 seconds\nRun complete for size: 28000; took 0.0072 seconds\nRun complete for size: 28000; took 0.0071 seconds\nRun complete for size: 28000; took 0.0078 seconds\nRun complete for size: 30000; took 0.0074 seconds\n\n\nRun complete for size: 30000; took 0.009 seconds\nRun complete for size: 30000; took 0.0079 seconds\nRun complete for size: 32000; took 0.0081 seconds\nRun complete for size: 32000; took 0.0081 seconds\nRun complete for size: 32000; took 0.0073 seconds\n\n\n\n\nCode\nmeans = oldxb.groupby(\"Size\")[\"Seconds\"].mean()\nstds = oldxb.groupby(\"Size\")[\"Seconds\"].std()\ninter = pd.DataFrame(pd.concat([means, stds], keys=[\"Mean\", \"Std. Dev.\"], axis=1))\nolderXB = inter.reset_index()\n\nmeans2 =newXB.groupby(\"Size\")[\"Seconds\"].mean()\nstds2 = newXB.groupby(\"Size\")[\"Seconds\"].std()\ninter2 = pd.DataFrame(pd.concat([means2, stds2], keys=[\"Mean\", \"Std. Dev.\"], axis=1))\nnewerXB = inter2.reset_index()\n\n\n\n\nCode\nresultsXB = pd.concat([olderXB, newerXB],\n                    axis=0, keys=[\"Old XB\", \"My XB\"])\nresultsXB = resultsXB.reset_index()\nresultsXB.columns = [\"Indice\", \"Run\", \"NObs.\", \"Mean Time\", \"SD Time\"]\nresultsXB.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Indice\n      Run\n      NObs.\n      Mean Time\n      SD Time\n    \n  \n  \n    \n      0\n      Old XB\n      0\n      500.0\n      0.042300\n      0.001552\n    \n    \n      1\n      Old XB\n      1\n      1000.0\n      0.077433\n      0.003496\n    \n    \n      2\n      Old XB\n      2\n      1500.0\n      0.107433\n      0.004143\n    \n    \n      3\n      Old XB\n      3\n      2000.0\n      0.156533\n      0.034673\n    \n    \n      4\n      Old XB\n      4\n      2500.0\n      0.204233\n      0.019793\n    \n  \n\n\n\n\n\n\nCode\nfig = px.scatter(resultsXB, x=\"NObs.\", y=\"Mean Time\", color=\"Indice\",\n                 title=\"Performance Comparison of Various Indice Implementations\").update_traces(mode='lines+markers')\nfig.update_layout(xaxis_title=\"Number of Observations\",\n                 yaxis_title=\"Time taken to complete (s)\")\nfig.show()"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html",
    "href": "posts/multi-factor-model/multi-factor-model.html",
    "title": "Multi-factor Model",
    "section": "",
    "text": "Utilize what we learned in our advanced investments class to predict and make decisions on whether a variety of funds were providing its investors excess returns (or \\alpha).\n\n\n\n\n\n\nWe made the recommendation to buy AAPL stock because its cost of equity was low compared to what our results had placed it on."
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#theoretical-multifactor-model",
    "href": "posts/multi-factor-model/multi-factor-model.html#theoretical-multifactor-model",
    "title": "Multi-factor Model",
    "section": "Theoretical Multifactor Model:",
    "text": "Theoretical Multifactor Model:\n\nR - R_f = \\alpha + \\beta_{1}(K_M - R_f) + \\beta_{2}(SMB) + \\beta_{3}(HML) + \\beta_{4}(MOM)\n\n\nVariables:\n\nMKTRF: R_M - R_f, is the excess return on the market\n\n\n\nSMB: the average return on small portfolios minus the average return on big portfolios\nHML: the average return on high book-to-market equity firms minus the average return on low book-to-market firms\nUMD: tendency to invest in recent winners over recent losers"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#fund-truthfulness",
    "href": "posts/multi-factor-model/multi-factor-model.html#fund-truthfulness",
    "title": "Multi-factor Model",
    "section": "Fund Truthfulness:",
    "text": "Fund Truthfulness:\n\nDTMVX (8300):\nThe DTMVX mutual fund invests in mid- and small-cap U.S. stocks, providing cheaper alternatives to competitors, and low value stocks. A linear regression on the excess returns between the period of January 2000 to December 2004 was run to determine if the fund was able to generate significant alpha and to analyze the fund’s holdings relative to the fund’s mandate.\n\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund was able to generate statistically significant alpha during the holding period. (p=0.8935)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in small cap firms than large cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in high value firms than low value firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, suggesting the fund managers have not kept the fund’s mandate. (p < 0.001)\n\n\n\nDVPEX (9183):\nAccording to the prospectus, the DVPEX mutual fund aims to provide “a high level of capital appreciation through investment in a diversified portfolio of common stocks of small to medium-sized companies.”\n\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund was not able to generate statistically significant alpha during the holding period. (p=0.0169)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in small cap firms than large cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in high value firms than low value firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, so it is uncertain whether the fund managers have a preference in losers or winners. It should be noted that failing to reject the null hypothesis does not prove that true value of this beta is equal to zero (although such a value is still a possibility), so further testing should be conducted on the fund’s preferences. (p = 0.0633)\n\n\n\nGLCGX (13813):\nThe GLCGX mutual fund from Goldman Sach’s invest in large cap, high growth, low value stocks with higher risk than just a normal index fund like SPY. Top fund holdings include Apple, Amazon, and Microsoft and top sectors include technology and healthcare. A linear regression on the excess returns between the period of January 2000 to December 2004 was run to determine if the fund was able to generate significant alpha and to analyze the fund’s holdings relative to the fund’s mandate.\n\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund was able to generate statistically significant alpha during the holding period. (p=0.4205)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests in higher risks stocks than the broad market, suggesting the fund managers have kept the fund’s mandate for high growth. (p < 0.001). However, it should be noted that the 95% confidence interval of β_1 is between 0.96048 and 1.12554, indicating that the fund may be more neutral than aggressive in its investment positioning.\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in large cap firms than small cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.0002)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in low value firms than high value firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, suggesting the fund managers have not kept the fund’s mandate. (p < 0.001) It should be noted that the null hypothesis for all of the factors’ betas is equal to 0, so this indicates that there is not enough proof that the fund has a particular preference in losers over winners. This is in line with the fund’s mandate as it does closely mimic the overall market.\n\n\n\nTRBCX (26985):\nThe TRBCX mutual fund from T. Rowe Price invests in very high growth, very large stocks, while staying away from high value firms. According to Morningstar, 53.48% of the fund is in giant market cap firms and 43.85% is in large cap firms. For sector weights, 22.96% is in consumer cyclical, 29.02% is in technology, and 20.5% is in healthcare.\n\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund was able to generate statistically significant alpha during the holding period. (p=0.1417)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in large cap firms than small cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in low value firms than high value firms, suggesting the fund managers have kept the fund’s mandate. (p = 0.0216) It is recommended that further testing should be done on the effect size of this factor, considering the p-value would not be significant at the more rigorous 1% level of significance.\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, so it is uncertain whether the fund managers have a preference in losers or winners. It should be noted that failing to reject the null hypothesis does not prove that true value of this beta is equal to zero (although such a value is still a possibility), so further testing should be conducted on the fund’s preferences. (p = 0.5335)"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#cost-of-equity-for-apple-inc",
    "href": "posts/multi-factor-model/multi-factor-model.html#cost-of-equity-for-apple-inc",
    "title": "Multi-factor Model",
    "section": "Cost of Equity for Apple Inc:",
    "text": "Cost of Equity for Apple Inc:\n\n\\hat{Cost of Equity} = R_f + \\beta_{1}(\\hat{MKTRF}) + \\beta_{2}(\\hat{SMB}) + \\beta_{3}(\\hat{HML}) + \\beta_{4}(\\hat{MOM})\n\nWe chose the stock of Apple, using data from 1999 - 2004, and estimated a cost of equity of 1.53%. In January of 1999, Apple stock was priced at $1.46, remaining consistent until the end of 1999. During March of 2000, the stock reached its highest price of $4.82, dropping drastically to $1.43 in October of 2000, remaining at similar prices until 2004. We felt that the cost of equity was very low because of the drastic price drop and the relatively constant price from then onward to 2004.\n\n\n\nApple Stock Price Over Time"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#sources",
    "href": "posts/multi-factor-model/multi-factor-model.html#sources",
    "title": "Multi-factor Model",
    "section": "Sources",
    "text": "Sources\n\nSO - Two Column Layout in Quarto"
  },
  {
    "objectID": "posts/xgboost/xgboost.html",
    "href": "posts/xgboost/xgboost.html",
    "title": "XGBoost",
    "section": "",
    "text": "Matthias Quinn, Matthew Brigham\nFall 2021"
  },
  {
    "objectID": "posts/xgboost/xgboost.html#decision-trees",
    "href": "posts/xgboost/xgboost.html#decision-trees",
    "title": "XGBoost",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision Trees are simple clustering algorithms that split the predictor space into distinct, non-overlapping regions.Trees are easy to visualize, are flexible since they make no assumption about the functional form, and can model highly non-linear relationships.They can also be used in the context of regression and classification.\nGenerally speaking, the algorithm works by searching through the available predictors and selecting the one that splits the leads to the greatest reduction in residual sums of squares (RSS), called recursive binary splitting.However, the model does not look beyond a single split to see if other predictors may lead to a greater overall reduction of RSS over the course of many splits.This is called a greedy algorithm. Considering the image below, we see that decision trees are a top-down approach, meaning that they start with the full predictor space then, moving down, create rules that split the space into distinct regions. The end result is a set divided predictor space of non-overlapping regions, as seen on the left. The splitting process continues until a criterion is met, whether it be a defined maximal depth, number of observations per leaf, or number of observations per split.\n\n\n\nBishop 2006 from 2 Charles U ML reference\n\n\nTree pruning is a method that can improve the predictive accuracy of decision trees. Pruning refers to the reduction in depth of the tree. One concern about decision trees is over-fitting by modeling noise or being too complex. Utilizing a process known as cost complexity pruning, the optimal tree depth may be determined.This process considers the prediction error and the number of terminal nodes, as illustrated in the equation below.Cost complexity pruning aims to minimize this equation. The tuning parameter, \\alpha, controls the complexity, or depth, of the tree. A large \\alpha will result in a deep, complex tree (less pruning) and a small will result in a shallow tree (more pruning). In general, shallow trees cannot model all of the patterns in the data and will have higher prediction error at the cost of low complexity.Deep trees can model the patterns in the data at the cost of high complexity and potential over-fitting.There exists a happy medium between the two.\nClassification requires small modifications to the fitting process compared to regression. Notably, instead of considering RSS, classification trees may consider either classification error, entropy, or the Gini Index.When the goal of modelling is prediction accuracy, it is generally advisable to minimize classification error for the splits. Classification error looks at the proportion of observations in each region belonging to a certain class and compares this to the most common classification of observations in that region. The equation below is used to calculate the classification error given region m and class k. \nCE = 1 - max(\\hat{p}_{mk})\n\nEntropy and the Gini Index are most commonly used for classification and are very similar in that they consider node purity. Node purity refers to the assortment of observations in a node. If all of the observations in a node have the same class label, then it is said to have high purity. If there are many different observation classes in a node, then it is not pure.High node purity is generally preferred. The equations for entropy and Gini Index are below.Just as regression trees aim to minimize RSS, classification trees aim to minimize entropy and Gini.\n\nG = \\Large \\sum_{k=1}^{K} \\hat{p}_{mk} (1 - \\hat{p}_{mk})\n\n\nE = \\Large - \\sum_{k=1}^{K} \\hat{p}_{mk} log(\\hat{p}_{mk})\n\nDecision trees can be advantageous since they are easy to interpret and can model a variety of data types. However, they have some weaknesses. A primary weakness is that they can have a high degree of variance.Another weakness is that they are not robust in the sense that the addition/removal of a single observation may result in a very different tree shape.There are several ways to handle these concerns, commonly bagging, random forests, and boosting. We will discuss boosting, in particular."
  },
  {
    "objectID": "posts/xgboost/xgboost.html#boosting",
    "href": "posts/xgboost/xgboost.html#boosting",
    "title": "XGBoost",
    "section": "Boosting",
    "text": "Boosting\nBoosting is a general ensemble algorithm that can be used to improve the predictive accuracy of decision trees. Boosting assumes a generic function then fits a shallow tree of the residuals. After incorporating the new tree to get an updated function, another shallow tree is fit to the updated function’s residuals.This process repeats until a criterion is met. Just like decision trees, the final resulting model is a single tree.\nThe boosting algorithm has three main steps:\n\nDefine an initial model to be fit to the data.\nIteratively fit B-many shallow trees to the residuals of the previous model and add them to the previous model\n\nFit a tree with d-many splits to the prior model’s residuals\nUpdate the previous model by adding a shrunken version of the shallow tree.\nUpdate the residuals by subtracting the shrunken version of the shallow tree\n\nAfter B-many shallow trees have been fit to the residuals, the result is a single boosted model.\n\nFrom the algorithm, we can see that the boosted model is a single tree that consists of a collection of weak learners (shallow trees). It is powerful because it can control for both bias and variance. Each addition of a weak learner aims to reduce the bias of the model. The collection of weak learners has the effect of reducing the variance of the model. Even though it controls for both, most of the focus is on bias reduction.If the initial function in the first step of the algorithm is \\hat{f}_{x} , then the initial residuals are the y-values.When this is the case, the final boosted model will be the following equation, where \\hat{f_{b}}(x) is the weak-learning decision tree from each iteration.\n\n\\Large \\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f_{b}}(x)\n\nThere are three tuning parameters for boosting: B, , and d. The parameter B represents the number of shallow trees that will be fit to the residuals. This value should be determined using cross-validation since a large value may result in over-fitting. The parameter B is closely tied to the shrinkage parameter . The parameter is called the shrinkage parameter and this controls the rate at which the model learns. A small lambda value confers a small adjustment to each sequential model. If the value of is extremely small, the model will need a large value for B in order for the data to be fit accurately. The parameter d defines the number of splits in each of the shallow trees. When we are referring to these sequentially added shallow trees, we mean that the value of d is small.\nThere are several algorithms for implementing boosting. Some example algorithms are AdaBoost, Gradient Boosting, and XGBoost (there are others). AdaBoost is commonly recognized as the first algorithm that successfully implemented the boosting algorithm. Gradient Boosting is a generalization of AdaBoost, and XGBoost is an optimized gradient boosting algorithm. One problem with boosting is that it can be very slow, especially for large datasets. This is part of the reason why XGBoost has become so popular.\n\nXGBoost\nThere are many algorithms to implement boosting. The first boosting algorithm is recognized as AdaBoost and was mostly used for binary classification. Gradient Boosting is a generalization of AdaBoost and allows for the use of different loss functions. This improvement meant that it could be applied to regression and multi-class classification problems.One issue with Gradient Boosting, which is common among boosting in general, is that the learning process can be very slow.XGBoost is an improved implementation of the Gradient Boosting algorithm optimized for speed and accuracy.It is optimized for speed and has been observed to be more accurate in many cases, which is how it got its name: eXtreme Gradient Boosting. The success of XGBoost is attributed to several key algorithmic improvements:\n\n\nRegularized learning objective\nNovel tree-based algorithm for handling sparse data\nWeighted quantile sketch for learning\nOptimized computer hardware utilization\n\nTo implement a boosting algorithm, there is a chosen loss function (usually based on prediction error) that gets minimized. Gradient Boosting uses gradients to accomplish this. XGBoost improves upon this process by\n\nXGBoost uses regularization of the loss function. Briefly, boosting works by iteratively adding weak learner shallow trees. This improvement helps to prevent over fitting by smoothing the solutions of terminal nodes in the shallow trees that are used for boosting.Regularization works by adding a penalty to a traditional loss function, usually based on errors or residuals. Common types of regularization are the l1 and l2 regularization.The loss function used by XGBoost is given below.The first term is a loss function that measures the difference between the predicted and actual values of y.The second term is the regularization term, or penalty, and uses both l1 and l2 regularization.The penalty works by reducing the complexity of the model.This rationale is analogous to cost complexity pruning in decision trees.There are alternative methods for regularization, but this method has been shown to be easier to implement for parallelization.Note that when the regularization term is set to zero, the loss function is identical to traditional gradient boosting.\n\n\n\\Large L = \\sum_{i}l(y_{i'}, \\hat{y_{i}}) + \\sum_{k}(\\gamma T + \\frac {1}{2} \\lambda ||w||^{2})\n\nThe second major improvement on boosting is the use of second-order derivatives in the optimization of the loss function.The use of second-derivatives yields more information about how to minimize the loss function faster.Usually for large datasets, it is not possible to explore all possible tree structures. Therefore, the use of the second derivative helps to pick which splits are optimal.The loss function cannot be optimized using traditional methods.The designers of XGBoost proposed an additive training strategy that necessitates the use of the second derivative for large datasets.\nA third major feature is shrinkage and feature sub-sampling to prevent over-fitting.Shrinkage refers to the parameter used in the boosting algorithm described in the previous section. Feature sub-sampling is a technique that refers to the construction of the shallow trees during boosting.When these trees are constructed, at each split, only a fraction of the total features are considered.Random forests are known for employing this methodology as it decorrelates successive trees and allows for different patterns to be captured.\nThere are several splitting algorithms incorporated into the function that can be defined by the user.These splitting algorithms are listed below.\n\nExact Greedy Algorithm: This algorithm considers all of the possible splits for the features and picks a split based on the greatest reduction in the loss function.For continuous variables, the observations are sorted in ascending order then calculates split statistics (such as Sum of Squared Errors)between each observation.For n-many observations and m-many features, there are (n-1) possible splits per feature and m(n-1) possible splits. For large datasets, this is infeasible and requires the use of the approximate algorithm.\nApproximate Algorithm: This algorithm improves upon the exact greedy algorithm and can work with large datasets.Instead of exploring all possible splits, the algorithm proposes split points based on quantiles and maps continuous features to these “bins”.Split statistics are calculated for the bins as a whole and then the split point is chosen based on which was best. The quantile strategy is distributable and recomputable, meaning it is faster.The designers also showed that this approximate quantile strategy can yield similar predictive performance as models that use the exact strategy. This quantile strategy is called weighted quantile sketch, and can be used for weighted data.\nSparsity-Aware Split Finding: Many datasets contain sparse data (such as missing data and zero-entries from one-hot encoding or other values). By incorporating a pattern recognition algorithm for sparse data, the model can run much faster for datasets with lots of sparse data.XGBoost assigns a default direction to each node when it is sparse.This “unified” approach allows for faster computation times.\n\n\nThe final improvement for XGBoost is the system optimization.Tree construction and sorting data (during the splitting algorithm) are the most computationally intensive and time consuming. Large datasets may not always be able to be modeled on a device. XGBoost was designed to be able to work efficiently on any device and in parallel with other devices in a distributed manner.To take the most advantage of a computer system, large datasets need to first be divided into blocks of data. XGBoost utilizes the following computational system improvements:\n\nColumn Block for Parallel Learning - To reduce the time of sorting, data is stored in blocks and only sorted once. The data is stored in a compressed column format.\nParallelization - The data blocks are distributed among the CPU cores. Therefore, collecting statistics from columns can be done using a parallel algorithm for finding splits.\nDistributed Computing - Blocking the data allows for it to be distributed among different machines or disks. This allows the model to work on large datasets.\nCache Awareness - Storing gradient statistics in the cache of a CPU is computationally more efficient.\nBlocks for Out-of-Core Computation - Dividing the data into blocks and using computer science techniques known as block compression and sharding allows for the model to work with very large datasets.\n\nIn summary, XGBoost is a gradient boosting algorithm that is designed for speed and scalability.It incorporates many different algorithms that the user can choose to use depending on their data structure.XGBoost is commonly used to take advantage of these design features and has been shown to perform just as well, oftentimes better, than other gradient boosting algorithms"
  },
  {
    "objectID": "posts/xgboost/xgboost.html#application-forest-cover-type",
    "href": "posts/xgboost/xgboost.html#application-forest-cover-type",
    "title": "XGBoost",
    "section": "Application: Forest Cover Type",
    "text": "Application: Forest Cover Type\nGiven forestry data from four wilderness areas in Roosevelt National Forest, classify the patches into one of 7 cover types, listed below:\n\n1. Spruce/fir\n\nLodgepole Pine\nPonderosa Pine\nCottonwood/Willow\nAspen\nDouglas/fir\nKrummholz\n\n\nData Description\nThe forest cover type problem requires a prediction of the type of trees that are growing on a plot of land from a variety of descriptive features that affect which species are able to grow in those conditions.An effective model would be able to accurately predict the cover type, allowing researchers to make these predictions without using remotely sensed data.There are 12 predictor variables that can be used to predict the cover type.Table 1 below summarizes each of these predictors. Each observation in the dataset consists of measurements from these variables from a 30 meter by 30 meter plot of land in northern Colorado.There are 581,012 observations in this dataset.The dataset was released in 1998.\n\nTable 1: Above is a summary of all of the features in the dataset. The dataset contains a total of 54 features because some of the variables are one-hot encoded, however, there are only 13 truly unique features.\n\n\n\n\n\n\n\n\nVariable Name\nType\nMeasurement Unit\nDescription\n\n\n\n\nElevation\nQuantitative\nMeters\nElevation in Meters\n\n\nAspect\nQuantitative\nAzimuthal Angle\nAspect in Degrees Azimuth\n\n\nSlope\nQuantitative\nDegrees\nSlope in Degrees\n\n\nHorizontal Distance to Hydrology\nQuantitative\nMeters\nHorizontal distance to nearest surface water features\n\n\nVertical Distance to Hydrology\nQuantitative\nMeters\nVertical distance to nearest surface water features\n\n\nHorizontal Distance to Roadways\nQuantitative\nMeters\nHorizontal distance to nearest roadway\n\n\nHillshade 9 AM\nQuantitative\n0-255 Index\nHillshade index at 9 AM, during summer solstice\n\n\nHillshade Noon\nQuantitative\n0-255 Index\nHillshade index at noon, during summer solstice\n\n\nHillshade 3 PM\nQuantitative\n0-255 Index\nHillshade index at 3 PM, during summer solstice\n\n\nHorizontal Distance to Fire Points\nQuantitative\nMeters\nHorizontal distance to nearest wildfire ignition points\n\n\nWilderness Area\nQualitative\n0 (absent), 1 (present)\nWilderness area designation\n\n\nSoil Type\nQualitative\n0 (absent), 1 (present)\nSoil type designation\n\n\nCover Type\nInteger\n1 to 7\nForest cover type designation\n\n\n\nThe data was collected from four different wilderness areas (out of a total 6 wilderness areas) within the Roosevelt National Forest in Northern Colorado.The cover type was determined by the US Forest Service Region 2 Resource Information System data. A wilderness area is an area that is relatively untouched by humans; this means that any ecological processes are the result of nature rather than forest management services. The four wilderness areas studied in Roosevelt National Forest were Neota, Rawah, Comanche, and Cache la Poudre. These four areas differ in elevation and geography, even though they are nearby, resulting in different species of trees covering the land.Neota consists mostly of spruce/fir.Rawah and Comanche are mostly lodgepole pine, with a smattering of spruce/fir and aspen. Cache la Poudre consists mostly of Ponderosa Pine, Douglas/Fir, and Cottonwood/Willow. It should be noted that approximately 85% of the observations consist of two types of classes.There are no missing values.\nTables 1-2 and Figure 1 show some frequency distributions of the cover types and wilderness areas. From these, it can be seen that the dataset is dominated by the two cover types and two wilderness areas. This is attributable to the relative sizes of the wilderness areas. It is important to note the imbalance of observations as this may impact the bias of the model. The Rawah wilderness area is approximately 119.4 sq. miles; the Neota wilderness area is approximately 15.5 sq. miles; the Comanche wilderness area is approximately 104.4 sq. miles; the Cache la Poudre wilderness area is approximately 14.5 sq. miles. From Figure 1, there is a significant proportion of the total observations (approximately 20%) that are in the same soil type and cover type. Figure 2 displays a matrix of correlation values between all of the numeric variables. No significant conclusions regarding the relationship between variables can be concluded. Figure 3 shows that elevation does a great job distinguishing between the cover types. Other variables were explored, but there were no significant patterns identified.\n\nTable 2: A table of observation frequencies shows that the observations are dominated by Spruce/Fir and Lodgepole Pine.\n\n\nClass\nCover Type\nFrequency\nRelative Frequency\n\n\n\n\n1\nSpruce/fir\n211840\n0.365\n\n\n2\nLodgepole Pine\n283301\n0.488\n\n\n3\nPonderosa Pine\n35754\n0.062\n\n\n4\nCottonwood/Willow\n2747\n0.005\n\n\n5\nAspen\n9493\n0.016\n\n\n6\nDouglas/fir\n17367\n0.03\n\n\n7\nKrummholz\n20510\n0.035\n\n\n\n\nTable 3: A table of wilderness area frequencies shows that approximately 88% of the observations came from two of the four areas.\n\n\nWilderness Area\nFrequency\n\n\n\n\nRawah\n260796\n\n\nNeota\n29884\n\n\nComanche Peak\n253364\n\n\nCache la Poudre\n36968"
  },
  {
    "objectID": "pytorch.html",
    "href": "pytorch.html",
    "title": "PyTorch",
    "section": "",
    "text": "Tensors\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasets & DataLoaders\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPyTorch Datasets\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\n\n\nThis project serves as an introduction to PyTorch and working with datasets.\n\n\n\n\n\n\nFeb 14, 2023\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch Tensors\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\n\n\nThis project serves as an introduction to PyTorch and tensors.\n\n\n\n\n\n\nNov 20, 2022\n\n\nMatthias Quinn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "PyTorch/tensors/tensors.html",
    "href": "PyTorch/tensors/tensors.html",
    "title": "PyTorch Tensors",
    "section": "",
    "text": "Learn more about the Machine Learning framework known as PyTorch.\n\n\n\n\n\n\nA better understanding of PyTorch and what tensors are.\nThis walkthrough is my interpretation of the official PyTorch tutorial on tensors."
  },
  {
    "objectID": "PyTorch/tensors/tensors.html#tensors",
    "href": "PyTorch/tensors/tensors.html#tensors",
    "title": "PyTorch Tensors",
    "section": "Tensors",
    "text": "Tensors\nTensors are a data structure similar to arrays and matrices. PyTorch uses tensors to encode both the inputs and outputs of a model as well as model parameters.\nTensors are like NumPy arrays, but tensors can be run on GPUs and are also optimized for automatic differentiation.\n\nLoad in the libraries\n\n\nCode\nimport numpy as np\nimport torch\n\n\n\n\nInitializing a Tensor\nTensors can be initialized in various ways, like so:\nDirectly from data:\n\n\nCode\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\n\nFrom a NumPy array:\n\n\nCode\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\n\n\nFrom another tensor:\n\n\nCode\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\n\nOnes Tensor: \n tensor([[1, 1],\n        [1, 1]]) \n\nRandom Tensor: \n tensor([[0.4359, 0.1517],\n        [0.9181, 0.1814]])"
  },
  {
    "objectID": "PyTorch/tensors/tensor_tutorial.html",
    "href": "PyTorch/tensors/tensor_tutorial.html",
    "title": "N3uralN3twork's Website",
    "section": "",
    "text": "Learn the Basics <intro.html>_ || Quickstart <quickstart_tutorial.html>_ || Tensors || Datasets & DataLoaders <data_tutorial.html>_ || Transforms <transforms_tutorial.html>_ || Build Model <buildmodel_tutorial.html>_ || Autograd <autograd_tutorial.html>_ || Optimization <optimization_tutorial.html>_ || Save & Load Model <saveloadrun_tutorial.html>_\n\nTensors\nTensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\nTensors are similar to NumPy’s <https://numpy.org/>_ ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see bridge-to-np-label). Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd <autograd_tutorial.html>__ section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!\n\n\nCode\nimport torch\nimport numpy as np\n\n\nInitializing a Tensor ~~~~~~~~~~~~~~~~~~~~~\nTensors can be initialized in various ways. Take a look at the following examples:\nDirectly from data\nTensors can be created directly from data. The data type is automatically inferred.\n\n\nCode\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\n\nFrom a NumPy array\nTensors can be created from NumPy arrays (and vice versa - see bridge-to-np-label).\n\n\nCode\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\n\n\nFrom another tensor:\nThe new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n\n\nCode\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\n\nWith random or constant values:\nshape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n\n\nCode\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\n\n\nAttributes of a Tensor ~~~~~~~~~~~~~~~~~\nTensor attributes describe their shape, datatype, and the device on which they are stored.\n\n\nCode\ntensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")\n\n\n\nOperations on Tensors ~~~~~~~~~~~~~~~~~\nOver 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described here <https://pytorch.org/docs/stable/torch.html>__.\nEach of these operations can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\nBy default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!\n\n\nCode\n# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')\n\n\nTry out some of the operations from the list. If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use.\nStandard numpy-like indexing and slicing:\n\n\nCode\ntensor = torch.ones(4, 4)\nprint('First row: ',tensor[0])\nprint('First column: ', tensor[:, 0])\nprint('Last column:', tensor[..., -1])\ntensor[:,1] = 0\nprint(tensor)\n\n\nJoining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack <https://pytorch.org/docs/stable/generated/torch.stack.html>__, another tensor joining op that is subtly different from torch.cat.\n\n\nCode\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\n\nArithmetic operations\n\n\nCode\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(tensor)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\n\nSingle-element tensors If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():\n\n\nCode\nagg = tensor.sum()\nagg_item = agg.item()  \nprint(agg_item, type(agg_item))\n\n\nIn-place operations Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x.\n\n\nCode\nprint(tensor, \"\\n\")\ntensor.add_(5)\nprint(tensor)\n\n\n\n\nNote\n\n\nIn-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.\n\n\n\nBridge with NumPy ~~~~~~~~~~~~~~~~~ Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\nTensor to NumPy array ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\nCode\nt = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")\n\n\nA change in the tensor reflects in the NumPy array.\n\n\nCode\nt.add_(1)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\n\nNumPy array to Tensor ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\nCode\nn = np.ones(5)\nt = torch.from_numpy(n)\n\n\nChanges in the NumPy array reflects in the tensor.\n\n\nCode\nnp.add(n, 1, out=n)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")"
  },
  {
    "objectID": "PyTorch/datasets/datasets.html",
    "href": "PyTorch/datasets/datasets.html",
    "title": "PyTorch Datasets",
    "section": "",
    "text": "Learn more about the Machine Learning framework known as PyTorch.\n\n\n\n\n\n\nA better understanding of PyTorch and working with dataset functionality."
  },
  {
    "objectID": "PyTorch/datasets/datasets.html#datasets-dataloaders",
    "href": "PyTorch/datasets/datasets.html#datasets-dataloaders",
    "title": "PyTorch Datasets",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders\nPyTorch provides two data primitives:\n\ntorch.utils.data.DataLoader\ntorch.utils.data.Dataset\n\nThey both allow you to use pre-loaded datasets as well as your own. Dataset allows you to store the samples and the labels while DataLoader wraps an iterable around the Dataset to enable access to the samples.\nThere are a number of prepackaged datasets that come with PyTorch, like FashionMNIST, and more can be found here:\n\nImage Datasets\nText Datasets\nAudio Datasets"
  },
  {
    "objectID": "PyTorch/datasets/datasets.html#how-to-load-a-dataset",
    "href": "PyTorch/datasets/datasets.html#how-to-load-a-dataset",
    "title": "PyTorch Datasets",
    "section": "How to Load a Dataset",
    "text": "How to Load a Dataset\nHere’s how to load the Fashion-MNIST dataset from TorchVision. This dataset is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises of a 28 \\times 28 greyscale image and an associated label from one of 10 classes.\nWe’ll load the FashionMNIST dataset with the following parameters:\n\nroot = the path where the train/test datasets are stored\ntrain = specifies the training or test dataset\ndownload = downloads the dataset from the internet if it’s not available on your computer\ntransform = specifies the feature transformations to use, if requested\n\n\n\nCode\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)"
  },
  {
    "objectID": "PyTorch/datasets/data_tutorial.html",
    "href": "PyTorch/datasets/data_tutorial.html",
    "title": "N3uralN3twork's Website",
    "section": "",
    "text": "Code\n%matplotlib inline\nLearn the Basics || Quickstart || Tensors || Datasets & DataLoaders || Transforms || Build Model || Autograd || Optimization || Save & Load Model"
  },
  {
    "objectID": "PyTorch/datasets/data_tutorial.html#loading-a-dataset",
    "href": "PyTorch/datasets/data_tutorial.html#loading-a-dataset",
    "title": "N3uralN3twork's Website",
    "section": "Loading a Dataset",
    "text": "Loading a Dataset\nHere is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.\nWe load the FashionMNIST Dataset with the following parameters: - root is the path where the train/test data is stored, - train specifies training or test dataset, - download=True downloads the data from the internet if it’s not available at root. - transform and target_transform specify the feature and label transformations\n\n\nCode\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)"
  },
  {
    "objectID": "PyTorch/datasets/data_tutorial.html#iterating-and-visualizing-the-dataset",
    "href": "PyTorch/datasets/data_tutorial.html#iterating-and-visualizing-the-dataset",
    "title": "N3uralN3twork's Website",
    "section": "Iterating and Visualizing the Dataset",
    "text": "Iterating and Visualizing the Dataset\nWe can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data.\n\n\nCode\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n.. .. figure:: /_static/img/basics/fashion_mnist.png :alt: fashion_mnist"
  },
  {
    "objectID": "PyTorch/datasets/data_tutorial.html#creating-a-custom-dataset-for-your-files",
    "href": "PyTorch/datasets/data_tutorial.html#creating-a-custom-dataset-for-your-files",
    "title": "N3uralN3twork's Website",
    "section": "Creating a Custom Dataset for your files",
    "text": "Creating a Custom Dataset for your files\nA custom Dataset class must implement three functions: __init__, __len__, and __getitem__. Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file.\nIn the next sections, we’ll break down what’s happening in each of these functions.\n\n\nCode\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label\n\n\n\ninit\nThe init function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\nThe labels.csv file looks like: ::\ntshirt1.jpg, 0\ntshirt2.jpg, 0\n......\nankleboot999.jpg, 9\n\n\nCode\ndef __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform\n\n\n\n\nlen\nThe len function returns the number of samples in our dataset.\nExample:\n\n\nCode\ndef __len__(self):\n    return len(self.img_labels)\n\n\n\n\ngetitem\nThe getitem function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple.\n\n\nCode\ndef __getitem__(self, idx):\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = read_image(img_path)\n    label = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n        label = self.target_transform(label)\n    return image, label"
  },
  {
    "objectID": "PyTorch/datasets/data_tutorial.html#preparing-your-data-for-training-with-dataloaders",
    "href": "PyTorch/datasets/data_tutorial.html#preparing-your-data-for-training-with-dataloaders",
    "title": "N3uralN3twork's Website",
    "section": "Preparing your data for training with DataLoaders",
    "text": "Preparing your data for training with DataLoaders\nThe Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\nDataLoader is an iterable that abstracts this complexity for us in an easy API.\n\n\nCode\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
  },
  {
    "objectID": "PyTorch/datasets/data_tutorial.html#iterate-through-the-dataloader",
    "href": "PyTorch/datasets/data_tutorial.html#iterate-through-the-dataloader",
    "title": "N3uralN3twork's Website",
    "section": "Iterate through the DataLoader",
    "text": "Iterate through the DataLoader\nWe have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).\n\n\nCode\n# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {label}\")"
  },
  {
    "objectID": "PyTorch/datasets/data_tutorial.html#further-reading",
    "href": "PyTorch/datasets/data_tutorial.html#further-reading",
    "title": "N3uralN3twork's Website",
    "section": "Further Reading",
    "text": "Further Reading\n\ntorch.utils.data API"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Hello there.\nMy name is Matthias and I am a data scientist currently working in the insurance industry.\nI am an analytically minded developer with 5 years of experience in data analysis and 2 years of experience in model building. I hold a Master of Science in mathematics with a specialization in statistics, where I created software in Python for analyzing the validity of clustering models."
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Homepage",
    "section": "Goals",
    "text": "Goals\nMy goals for this website are to learn more about frontend development as well as new technologies, like Quarto. I also wanted to have a more interactive method of discovering and keep track of my projects; opposed to the current method of static Word documents and mono repositories."
  },
  {
    "objectID": "index.html#what-is-quarto",
    "href": "index.html#what-is-quarto",
    "title": "Homepage",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto helps you have your ideas and your code in one place, and present it in a beautiful way.\nQuarto unifies and extends the RMarkdown ecosystem - it unifies by combining the functionality of R Markdown, bookdown, distill, xaringian, etc into a single consistent system. And it extends in several ways: all features are possible beyond R too, including Python and Javascript. It also has more “guardrails”: accessibility and inclusion are centered in the design. Quarto is for people who love RMarkdown, and it’s for people who have never used RMarkdown."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Explaining the school-to-prison pipeline via race and gender: The impact of trauma, school suspension, school dropout, and delinquency\n\n\n\n\n\nAuthors: Tedor, Mallett, Quinn, Quinn\n\n\n\n\n\n\n\n\n\nExamining Trauma and Crime by Gender and Sexual Orientation among Youth: Findings from the Add Health National Longitudinal Study\n\n\n\n\n\nAuthors: Yun, Tedor, Mallett, Quinn, Quinn\nPaper: Link to Paper - SAGE Journal\n\n\n\n🚧 This page is under construction. 🚧"
  }
]