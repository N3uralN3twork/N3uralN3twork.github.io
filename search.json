[
  {
    "objectID": "posts/naive-bayes/NaiveBayes.html",
    "href": "posts/naive-bayes/NaiveBayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Source\nNaive Bayes is a supervised machine learning algorithm base on Bayes Theorem.\nNaive because it assumes all of the predictor variables to be completely independent of each other.\nBayes Rule:\nP(A|B) = \\frac{P(B|A)P(A)}{P(B}\nIn Naive Bayes, there are multiple predictor variables and more than 1 output class.\nThe objective of a Naive Bayes algorithm is to measure the conditional probability of an event with a feature vector\nProblem Statement:\nPredict employee attrition\nCreate training (70%) and test (30%) sets and utilize reproducibility.\nDistribution of Attrition rates across train and test sets.\nNotice that they are very similar."
  },
  {
    "objectID": "posts/naive-bayes/NaiveBayes.html#advantages-and-shortcoming",
    "href": "posts/naive-bayes/NaiveBayes.html#advantages-and-shortcoming",
    "title": "Naive Bayes",
    "section": "Advantages and Shortcoming",
    "text": "Advantages and Shortcoming\nThe Naive Bayes classifier is simple, fast, and scales well to large n.\nA major disadvantage is that it relies on the often-wrong assumption of equally important and independent features."
  },
  {
    "objectID": "posts/naive-bayes/NaiveBayes.html#implementation",
    "href": "posts/naive-bayes/NaiveBayes.html#implementation",
    "title": "Naive Bayes",
    "section": "Implementation",
    "text": "Implementation\nWe will utilize the caret package in R.\n\nCreate response and feature data\n\n\n\nCode\nfeatures <- setdiff(names(train), \"Attrition\")\nx <- train[, features]\ny <- train$Attrition\n\n\nInitialize 10-fold cross validation using caret’s trainControl function.\n\n\nCode\ntrControl <- trainControl(method = \"cv\", number = 10)\n\n\nTraining the Naive Bayes model:\n\n\nCode\nnb.fit <- train(\n  x = x,\n  y = y,\n  method = \"nb\",\n  trControl = trControl\n)\n\n\n#Confusion Matrix to analyze our results:\n\n\nCode\nconfusionMatrix(nb.fit)\n\n\nCross-Validated (10 fold) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction   No  Yes\n       No  76.8  8.2\n       Yes  7.1  7.9\n                            \n Accuracy (average) : 0.8473\n\n\n\n\nCode\nplot(nb.fit)\n\n\n\n\n\nTo test the accuracy on our test data set:\n\n\nCode\npreds <- predict(nb.fit, newdata = test)\n\n\n\n\nCode\nconfusionMatrix(preds, reference = test$Attrition)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  329  38\n       Yes  41  34\n                                          \n               Accuracy : 0.8213          \n                 95% CI : (0.7823, 0.8559)\n    No Information Rate : 0.8371          \n    P-Value [Acc > NIR] : 0.8333          \n                                          \n                  Kappa : 0.3554          \n                                          \n Mcnemar's Test P-Value : 0.8220          \n                                          \n            Sensitivity : 0.8892          \n            Specificity : 0.4722          \n         Pos Pred Value : 0.8965          \n         Neg Pred Value : 0.4533          \n             Prevalence : 0.8371          \n         Detection Rate : 0.7443          \n   Detection Prevalence : 0.8303          \n      Balanced Accuracy : 0.6807          \n                                          \n       'Positive' Class : No"
  },
  {
    "objectID": "posts/xgboost/xgboost.html",
    "href": "posts/xgboost/xgboost.html",
    "title": "XGBoost",
    "section": "",
    "text": "Matthias Quinn, Matthew Brigham\nFall 2021"
  },
  {
    "objectID": "posts/xgboost/xgboost.html#decision-trees",
    "href": "posts/xgboost/xgboost.html#decision-trees",
    "title": "XGBoost",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision Trees are simple clustering algorithms that split the predictor space into distinct, non-overlapping regions.Trees are easy to visualize, are flexible since they make no assumption about the functional form, and can model highly non-linear relationships.They can also be used in the context of regression and classification.\nGenerally speaking, the algorithm works by searching through the available predictors and selecting the one that splits the leads to the greatest reduction in residual sums of squares (RSS), called recursive binary splitting.However, the model does not look beyond a single split to see if other predictors may lead to a greater overall reduction of RSS over the course of many splits.This is called a greedy algorithm. Considering the image below, we see that decision trees are a top-down approach, meaning that they start with the full predictor space then, moving down, create rules that split the space into distinct regions. The end result is a set divided predictor space of non-overlapping regions, as seen on the left. The splitting process continues until a criterion is met, whether it be a defined maximal depth, number of observations per leaf, or number of observations per split.\n\n\n\nBishop 2006 from 2 Charles U ML reference\n\n\nTree pruning is a method that can improve the predictive accuracy of decision trees. Pruning refers to the reduction in depth of the tree. One concern about decision trees is over-fitting by modeling noise or being too complex. Utilizing a process known as cost complexity pruning, the optimal tree depth may be determined.This process considers the prediction error and the number of terminal nodes, as illustrated in the equation below.Cost complexity pruning aims to minimize this equation. The tuning parameter, \\alpha, controls the complexity, or depth, of the tree. A large \\alpha will result in a deep, complex tree (less pruning) and a small will result in a shallow tree (more pruning). In general, shallow trees cannot model all of the patterns in the data and will have higher prediction error at the cost of low complexity.Deep trees can model the patterns in the data at the cost of high complexity and potential over-fitting.There exists a happy medium between the two.\nClassification requires small modifications to the fitting process compared to regression. Notably, instead of considering RSS, classification trees may consider either classification error, entropy, or the Gini Index.When the goal of modelling is prediction accuracy, it is generally advisable to minimize classification error for the splits. Classification error looks at the proportion of observations in each region belonging to a certain class and compares this to the most common classification of observations in that region. The equation below is used to calculate the classification error given region m and class k. \nCE = 1 - max(\\hat{p}_{mk})\n\nEntropy and the Gini Index are most commonly used for classification and are very similar in that they consider node purity. Node purity refers to the assortment of observations in a node. If all of the observations in a node have the same class label, then it is said to have high purity. If there are many different observation classes in a node, then it is not pure.High node purity is generally preferred. The equations for entropy and Gini Index are below.Just as regression trees aim to minimize RSS, classification trees aim to minimize entropy and Gini.\n\nG = \\Large \\sum_{k=1}^{K} \\hat{p}_{mk} (1 - \\hat{p}_{mk})\n\n\nE = \\Large - \\sum_{k=1}^{K} \\hat{p}_{mk} log(\\hat{p}_{mk})\n\nDecision trees can be advantageous since they are easy to interpret and can model a variety of data types. However, they have some weaknesses. A primary weakness is that they can have a high degree of variance.Another weakness is that they are not robust in the sense that the addition/removal of a single observation may result in a very different tree shape.There are several ways to handle these concerns, commonly bagging, random forests, and boosting. We will discuss boosting, in particular."
  },
  {
    "objectID": "posts/xgboost/xgboost.html#boosting",
    "href": "posts/xgboost/xgboost.html#boosting",
    "title": "XGBoost",
    "section": "Boosting",
    "text": "Boosting\nBoosting is a general ensemble algorithm that can be used to improve the predictive accuracy of decision trees. Boosting assumes a generic function then fits a shallow tree of the residuals. After incorporating the new tree to get an updated function, another shallow tree is fit to the updated function’s residuals.This process repeats until a criterion is met. Just like decision trees, the final resulting model is a single tree.\nThe boosting algorithm has three main steps:\n\nDefine an initial model to be fit to the data.\nIteratively fit B-many shallow trees to the residuals of the previous model and add them to the previous model\n\nFit a tree with d-many splits to the prior model’s residuals\nUpdate the previous model by adding a shrunken version of the shallow tree.\nUpdate the residuals by subtracting the shrunken version of the shallow tree\n\nAfter B-many shallow trees have been fit to the residuals, the result is a single boosted model.\n\nFrom the algorithm, we can see that the boosted model is a single tree that consists of a collection of weak learners (shallow trees). It is powerful because it can control for both bias and variance. Each addition of a weak learner aims to reduce the bias of the model. The collection of weak learners has the effect of reducing the variance of the model. Even though it controls for both, most of the focus is on bias reduction.If the initial function in the first step of the algorithm is \\hat{f}_{x} , then the initial residuals are the y-values.When this is the case, the final boosted model will be the following equation, where \\hat{f_{b}}(x) is the weak-learning decision tree from each iteration.\n\n\\Large \\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f_{b}}(x)\n\nThere are three tuning parameters for boosting: B, , and d. The parameter B represents the number of shallow trees that will be fit to the residuals. This value should be determined using cross-validation since a large value may result in over-fitting. The parameter B is closely tied to the shrinkage parameter . The parameter is called the shrinkage parameter and this controls the rate at which the model learns. A small lambda value confers a small adjustment to each sequential model. If the value of is extremely small, the model will need a large value for B in order for the data to be fit accurately. The parameter d defines the number of splits in each of the shallow trees. When we are referring to these sequentially added shallow trees, we mean that the value of d is small.\nThere are several algorithms for implementing boosting. Some example algorithms are AdaBoost, Gradient Boosting, and XGBoost (there are others). AdaBoost is commonly recognized as the first algorithm that successfully implemented the boosting algorithm. Gradient Boosting is a generalization of AdaBoost, and XGBoost is an optimized gradient boosting algorithm. One problem with boosting is that it can be very slow, especially for large datasets. This is part of the reason why XGBoost has become so popular.\n\nXGBoost\nThere are many algorithms to implement boosting. The first boosting algorithm is recognized as AdaBoost and was mostly used for binary classification. Gradient Boosting is a generalization of AdaBoost and allows for the use of different loss functions. This improvement meant that it could be applied to regression and multi-class classification problems.One issue with Gradient Boosting, which is common among boosting in general, is that the learning process can be very slow.XGBoost is an improved implementation of the Gradient Boosting algorithm optimized for speed and accuracy.It is optimized for speed and has been observed to be more accurate in many cases, which is how it got its name: eXtreme Gradient Boosting. The success of XGBoost is attributed to several key algorithmic improvements:\n\n\nRegularized learning objective\nNovel tree-based algorithm for handling sparse data\nWeighted quantile sketch for learning\nOptimized computer hardware utilization\n\nTo implement a boosting algorithm, there is a chosen loss function (usually based on prediction error) that gets minimized. Gradient Boosting uses gradients to accomplish this. XGBoost improves upon this process by\n\nXGBoost uses regularization of the loss function. Briefly, boosting works by iteratively adding weak learner shallow trees. This improvement helps to prevent over fitting by smoothing the solutions of terminal nodes in the shallow trees that are used for boosting.Regularization works by adding a penalty to a traditional loss function, usually based on errors or residuals. Common types of regularization are the l1 and l2 regularization.The loss function used by XGBoost is given below.The first term is a loss function that measures the difference between the predicted and actual values of y.The second term is the regularization term, or penalty, and uses both l1 and l2 regularization.The penalty works by reducing the complexity of the model.This rationale is analogous to cost complexity pruning in decision trees.There are alternative methods for regularization, but this method has been shown to be easier to implement for parallelization.Note that when the regularization term is set to zero, the loss function is identical to traditional gradient boosting.\n\n\n\\Large L = \\sum_{i}l(y_{i'}, \\hat{y_{i}}) + \\sum_{k}(\\gamma T + \\frac {1}{2} \\lambda ||w||^{2})\n\nThe second major improvement on boosting is the use of second-order derivatives in the optimization of the loss function.The use of second-derivatives yields more information about how to minimize the loss function faster.Usually for large datasets, it is not possible to explore all possible tree structures. Therefore, the use of the second derivative helps to pick which splits are optimal.The loss function cannot be optimized using traditional methods.The designers of XGBoost proposed an additive training strategy that necessitates the use of the second derivative for large datasets.\nA third major feature is shrinkage and feature sub-sampling to prevent over-fitting.Shrinkage refers to the parameter used in the boosting algorithm described in the previous section. Feature sub-sampling is a technique that refers to the construction of the shallow trees during boosting.When these trees are constructed, at each split, only a fraction of the total features are considered.Random forests are known for employing this methodology as it decorrelates successive trees and allows for different patterns to be captured.\nThere are several splitting algorithms incorporated into the function that can be defined by the user.These splitting algorithms are listed below.\n\nExact Greedy Algorithm: This algorithm considers all of the possible splits for the features and picks a split based on the greatest reduction in the loss function.For continuous variables, the observations are sorted in ascending order then calculates split statistics (such as Sum of Squared Errors)between each observation.For n-many observations and m-many features, there are (n-1) possible splits per feature and m(n-1) possible splits. For large datasets, this is infeasible and requires the use of the approximate algorithm.\nApproximate Algorithm: This algorithm improves upon the exact greedy algorithm and can work with large datasets.Instead of exploring all possible splits, the algorithm proposes split points based on quantiles and maps continuous features to these “bins”.Split statistics are calculated for the bins as a whole and then the split point is chosen based on which was best. The quantile strategy is distributable and recomputable, meaning it is faster.The designers also showed that this approximate quantile strategy can yield similar predictive performance as models that use the exact strategy. This quantile strategy is called weighted quantile sketch, and can be used for weighted data.\nSparsity-Aware Split Finding: Many datasets contain sparse data (such as missing data and zero-entries from one-hot encoding or other values). By incorporating a pattern recognition algorithm for sparse data, the model can run much faster for datasets with lots of sparse data.XGBoost assigns a default direction to each node when it is sparse.This “unified” approach allows for faster computation times.\n\n\nThe final improvement for XGBoost is the system optimization.Tree construction and sorting data (during the splitting algorithm) are the most computationally intensive and time consuming. Large datasets may not always be able to be modeled on a device. XGBoost was designed to be able to work efficiently on any device and in parallel with other devices in a distributed manner.To take the most advantage of a computer system, large datasets need to first be divided into blocks of data. XGBoost utilizes the following computational system improvements:\n\nColumn Block for Parallel Learning - To reduce the time of sorting, data is stored in blocks and only sorted once. The data is stored in a compressed column format.\nParallelization - The data blocks are distributed among the CPU cores. Therefore, collecting statistics from columns can be done using a parallel algorithm for finding splits.\nDistributed Computing - Blocking the data allows for it to be distributed among different machines or disks. This allows the model to work on large datasets.\nCache Awareness - Storing gradient statistics in the cache of a CPU is computationally more efficient.\nBlocks for Out-of-Core Computation - Dividing the data into blocks and using computer science techniques known as block compression and sharding allows for the model to work with very large datasets.\n\nIn summary, XGBoost is a gradient boosting algorithm that is designed for speed and scalability.It incorporates many different algorithms that the user can choose to use depending on their data structure.XGBoost is commonly used to take advantage of these design features and has been shown to perform just as well, oftentimes better, than other gradient boosting algorithms"
  },
  {
    "objectID": "posts/xgboost/xgboost.html#application-forest-cover-type",
    "href": "posts/xgboost/xgboost.html#application-forest-cover-type",
    "title": "XGBoost",
    "section": "Application: Forest Cover Type",
    "text": "Application: Forest Cover Type\nGiven forestry data from four wilderness areas in Roosevelt National Forest, classify the patches into one of 7 cover types, listed below:\n\n1. Spruce/fir\n\nLodgepole Pine\nPonderosa Pine\nCottonwood/Willow\nAspen\nDouglas/fir\nKrummholz\n\n\nData Description\nThe forest cover type problem requires a prediction of the type of trees that are growing on a plot of land from a variety of descriptive features that affect which species are able to grow in those conditions.An effective model would be able to accurately predict the cover type, allowing researchers to make these predictions without using remotely sensed data.There are 12 predictor variables that can be used to predict the cover type.Table 1 below summarizes each of these predictors. Each observation in the dataset consists of measurements from these variables from a 30 meter by 30 meter plot of land in northern Colorado.There are 581,012 observations in this dataset.The dataset was released in 1998.\n\nTable 1: Above is a summary of all of the features in the dataset. The dataset contains a total of 54 features because some of the variables are one-hot encoded, however, there are only 13 truly unique features.\n\n\n\n\n\n\n\n\nVariable Name\nType\nMeasurement Unit\nDescription\n\n\n\n\nElevation\nQuantitative\nMeters\nElevation in Meters\n\n\nAspect\nQuantitative\nAzimuthal Angle\nAspect in Degrees Azimuth\n\n\nSlope\nQuantitative\nDegrees\nSlope in Degrees\n\n\nHorizontal Distance to Hydrology\nQuantitative\nMeters\nHorizontal distance to nearest surface water features\n\n\nVertical Distance to Hydrology\nQuantitative\nMeters\nVertical distance to nearest surface water features\n\n\nHorizontal Distance to Roadways\nQuantitative\nMeters\nHorizontal distance to nearest roadway\n\n\nHillshade 9 AM\nQuantitative\n0-255 Index\nHillshade index at 9 AM, during summer solstice\n\n\nHillshade Noon\nQuantitative\n0-255 Index\nHillshade index at noon, during summer solstice\n\n\nHillshade 3 PM\nQuantitative\n0-255 Index\nHillshade index at 3 PM, during summer solstice\n\n\nHorizontal Distance to Fire Points\nQuantitative\nMeters\nHorizontal distance to nearest wildfire ignition points\n\n\nWilderness Area\nQualitative\n0 (absent), 1 (present)\nWilderness area designation\n\n\nSoil Type\nQualitative\n0 (absent), 1 (present)\nSoil type designation\n\n\nCover Type\nInteger\n1 to 7\nForest cover type designation\n\n\n\nThe data was collected from four different wilderness areas (out of a total 6 wilderness areas) within the Roosevelt National Forest in Northern Colorado.The cover type was determined by the US Forest Service Region 2 Resource Information System data. A wilderness area is an area that is relatively untouched by humans; this means that any ecological processes are the result of nature rather than forest management services. The four wilderness areas studied in Roosevelt National Forest were Neota, Rawah, Comanche, and Cache la Poudre. These four areas differ in elevation and geography, even though they are nearby, resulting in different species of trees covering the land.Neota consists mostly of spruce/fir.Rawah and Comanche are mostly lodgepole pine, with a smattering of spruce/fir and aspen. Cache la Poudre consists mostly of Ponderosa Pine, Douglas/Fir, and Cottonwood/Willow. It should be noted that approximately 85% of the observations consist of two types of classes.There are no missing values.\nTables 1-2 and Figure 1 show some frequency distributions of the cover types and wilderness areas. From these, it can be seen that the dataset is dominated by the two cover types and two wilderness areas. This is attributable to the relative sizes of the wilderness areas. It is important to note the imbalance of observations as this may impact the bias of the model. The Rawah wilderness area is approximately 119.4 sq. miles; the Neota wilderness area is approximately 15.5 sq. miles; the Comanche wilderness area is approximately 104.4 sq. miles; the Cache la Poudre wilderness area is approximately 14.5 sq. miles. From Figure 1, there is a significant proportion of the total observations (approximately 20%) that are in the same soil type and cover type. Figure 2 displays a matrix of correlation values between all of the numeric variables. No significant conclusions regarding the relationship between variables can be concluded. Figure 3 shows that elevation does a great job distinguishing between the cover types. Other variables were explored, but there were no significant patterns identified.\n\nTable 2: A table of observation frequencies shows that the observations are dominated by Spruce/Fir and Lodgepole Pine.\n\n\nClass\nCover Type\nFrequency\nRelative Frequency\n\n\n\n\n1\nSpruce/fir\n211840\n0.365\n\n\n2\nLodgepole Pine\n283301\n0.488\n\n\n3\nPonderosa Pine\n35754\n0.062\n\n\n4\nCottonwood/Willow\n2747\n0.005\n\n\n5\nAspen\n9493\n0.016\n\n\n6\nDouglas/fir\n17367\n0.03\n\n\n7\nKrummholz\n20510\n0.035\n\n\n\n\nTable 3: A table of wilderness area frequencies shows that approximately 88% of the observations came from two of the four areas.\n\n\nWilderness Area\nFrequency\n\n\n\n\nRawah\n260796\n\n\nNeota\n29884\n\n\nComanche Peak\n253364\n\n\nCache la Poudre\n36968"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html",
    "href": "posts/multi-factor-model/multi-factor-model.html",
    "title": "Multi-factor Model",
    "section": "",
    "text": "Utilize what we learned in our advanced investments class to predict and make decisions on whether a variety of funds were providing its investors excess returns (or \\alpha).\n\n\n\n\n\n\nWe made the recommendation to buy AAPL stock because its cost of equity was low compared to what our results had placed it on."
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#theoretical-multifactor-model",
    "href": "posts/multi-factor-model/multi-factor-model.html#theoretical-multifactor-model",
    "title": "Multi-factor Model",
    "section": "Theoretical Multifactor Model:",
    "text": "Theoretical Multifactor Model:\n\nR - R_f = \\alpha + \\beta_{1}(K_M - R_f) + \\beta_{2}(SMB) + \\beta_{3}(HML) + \\beta_{4}(MOM)\n\n\nVariables:\n\nMKTRF: R_M - R_f, is the excess return on the market\n\n\n\nSMB: the average return on small portfolios minus the average return on big portfolios\nHML: the average return on high book-to-market equity firms minus the average return on low book-to-market firms\nUMD: tendency to invest in recent winners over recent losers"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#fund-truthfulness",
    "href": "posts/multi-factor-model/multi-factor-model.html#fund-truthfulness",
    "title": "Multi-factor Model",
    "section": "Fund Truthfulness:",
    "text": "Fund Truthfulness:\n\nDTMVX (8300):\nThe DTMVX mutual fund invests in mid- and small-cap U.S. stocks, providing cheaper alternatives to competitors, and low value stocks. A linear regression on the excess returns between the period of January 2000 to December 2004 was run to determine if the fund was able to generate significant alpha and to analyze the fund’s holdings relative to the fund’s mandate.\n\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund was able to generate statistically significant alpha during the holding period. (p=0.8935)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in small cap firms than large cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in high value firms than low value firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, suggesting the fund managers have not kept the fund’s mandate. (p < 0.001)\n\n\n\nDVPEX (9183):\nAccording to the prospectus, the DVPEX mutual fund aims to provide “a high level of capital appreciation through investment in a diversified portfolio of common stocks of small to medium-sized companies.”\n\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund was not able to generate statistically significant alpha during the holding period. (p=0.0169)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in small cap firms than large cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in high value firms than low value firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, so it is uncertain whether the fund managers have a preference in losers or winners. It should be noted that failing to reject the null hypothesis does not prove that true value of this beta is equal to zero (although such a value is still a possibility), so further testing should be conducted on the fund’s preferences. (p = 0.0633)\n\n\n\nGLCGX (13813):\nThe GLCGX mutual fund from Goldman Sach’s invest in large cap, high growth, low value stocks with higher risk than just a normal index fund like SPY. Top fund holdings include Apple, Amazon, and Microsoft and top sectors include technology and healthcare. A linear regression on the excess returns between the period of January 2000 to December 2004 was run to determine if the fund was able to generate significant alpha and to analyze the fund’s holdings relative to the fund’s mandate.\n\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund was able to generate statistically significant alpha during the holding period. (p=0.4205)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests in higher risks stocks than the broad market, suggesting the fund managers have kept the fund’s mandate for high growth. (p < 0.001). However, it should be noted that the 95% confidence interval of β_1 is between 0.96048 and 1.12554, indicating that the fund may be more neutral than aggressive in its investment positioning.\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in large cap firms than small cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.0002)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in low value firms than high value firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, suggesting the fund managers have not kept the fund’s mandate. (p < 0.001) It should be noted that the null hypothesis for all of the factors’ betas is equal to 0, so this indicates that there is not enough proof that the fund has a particular preference in losers over winners. This is in line with the fund’s mandate as it does closely mimic the overall market.\n\n\n\nTRBCX (26985):\nThe TRBCX mutual fund from T. Rowe Price invests in very high growth, very large stocks, while staying away from high value firms. According to Morningstar, 53.48% of the fund is in giant market cap firms and 43.85% is in large cap firms. For sector weights, 22.96% is in consumer cyclical, 29.02% is in technology, and 20.5% is in healthcare.\n\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund was able to generate statistically significant alpha during the holding period. (p=0.1417)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in large cap firms than small cap firms, suggesting the fund managers have kept the fund’s mandate. (p < 0.001)\nThere is sufficient evidence to propose, at the 5% level of significance, that the mutual fund invests more aggressively in low value firms than high value firms, suggesting the fund managers have kept the fund’s mandate. (p = 0.0216) It is recommended that further testing should be done on the effect size of this factor, considering the p-value would not be significant at the more rigorous 1% level of significance.\nThere is insufficient evidence to propose, at the 5% level of significance, that the mutual fund invest more in past losers over past winners, so it is uncertain whether the fund managers have a preference in losers or winners. It should be noted that failing to reject the null hypothesis does not prove that true value of this beta is equal to zero (although such a value is still a possibility), so further testing should be conducted on the fund’s preferences. (p = 0.5335)"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#cost-of-equity-for-apple-inc",
    "href": "posts/multi-factor-model/multi-factor-model.html#cost-of-equity-for-apple-inc",
    "title": "Multi-factor Model",
    "section": "Cost of Equity for Apple Inc:",
    "text": "Cost of Equity for Apple Inc:\n\n\\hat{Cost of Equity} = R_f + \\beta_{1}(\\hat{MKTRF}) + \\beta_{2}(\\hat{SMB}) + \\beta_{3}(\\hat{HML}) + \\beta_{4}(\\hat{MOM})\n\nWe chose the stock of Apple, using data from 1999 - 2004, and estimated a cost of equity of 1.53%. In January of 1999, Apple stock was priced at $1.46, remaining consistent until the end of 1999. During March of 2000, the stock reached its highest price of $4.82, dropping drastically to $1.43 in October of 2000, remaining at similar prices until 2004. We felt that the cost of equity was very low because of the drastic price drop and the relatively constant price from then onward to 2004.\n\n\n\nApple Stock Price Over Time"
  },
  {
    "objectID": "posts/multi-factor-model/multi-factor-model.html#sources",
    "href": "posts/multi-factor-model/multi-factor-model.html#sources",
    "title": "Multi-factor Model",
    "section": "Sources",
    "text": "Sources\n\nSO - Two Column Layout in Quarto"
  },
  {
    "objectID": "posts/logistic-regress-caret/logregcaret.html",
    "href": "posts/logistic-regress-caret/logregcaret.html",
    "title": "Logistic Regression with caret",
    "section": "",
    "text": "Logistic Regression"
  },
  {
    "objectID": "posts/logistic-regress-caret/logregcaret.html#step-1.-split-the-data-into-training-and-test-sets",
    "href": "posts/logistic-regress-caret/logregcaret.html#step-1.-split-the-data-into-training-and-test-sets",
    "title": "Logistic Regression with caret",
    "section": "Step 1. Split the data into training and test sets",
    "text": "Step 1. Split the data into training and test sets\n\n\nCode\nset.seed(12345789)\ninTrain <- createDataPartition(y = GermanCredit$Class, p = 0.6, list = FALSE, )\ntraining <- GermanCredit[inTrain, ]\ntesting  <- GermanCredit[-inTrain, ]\n\n\nTo create the logistic model, we’ll use the train function from caret.\n\n\nCode\nlmodel <- train(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + CreditHistory.Critical,\n                data = training,\n                method = \"glm\",\n                family = \"binomial\")\n\n\nTo obtain the odds for the coefficients and remove the log:\n\n\nCode\nexp(coef(lmodel$finalModel))\n\n\n           (Intercept)                    Age          ForeignWorker \n             3.0516947              1.0095028              0.2494196 \n   Property.RealEstate            Housing.Own CreditHistory.Critical \n             1.7752920              1.7580389              2.2363912 \n\n\nKeep in mind that you can’t just read the coefficients like you would in simple linear regression.\nFor example, our model is suggesting that for every one unit increase in Age, the (odds) of the consumer having good credit increases by a factor of 1.02\n95% Confidence Interval for terms:\n\n\nCode\nlogistic = glm(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + CreditHistory.Critical,\n                data = training,\n                family = \"binomial\")\n\ncbind(OR = exp(coef(logistic)), exp(confint(logistic)), pValue = (summary(logistic)$coefficients[, 4]))\n\n\nWaiting for profiling to be done...\n\n\n                              OR      2.5 %     97.5 %       pValue\n(Intercept)            3.0516947 0.74027688 20.9221296 0.1700607984\nAge                    1.0095028 0.99357880  1.0262071 0.2501534852\nForeignWorker          0.2494196 0.03925941  0.8803206 0.0651729645\nProperty.RealEstate    1.7752920 1.15611482  2.7758529 0.0100252083\nHousing.Own            1.7580389 1.18604549  2.6003291 0.0047917432\nCreditHistory.Critical 2.2363912 1.45002355  3.5270854 0.0003723358\n\n\nEssentially, if the confidence interval contains 1, it is not very helpful\n##Making Predictions: To make predictions, simple use the predict function from base R.\n\n\nCode\npredictions = predict(lmodel, newdata = testing, type = \"prob\")\nhead(predictions)\n\n\n         Bad      Good\n2  0.2547727 0.7452273\n6  0.4854772 0.5145228\n10 0.2040837 0.7959163\n15 0.5020246 0.4979754\n17 0.1683434 0.8316566\n18 0.3710473 0.6289527"
  },
  {
    "objectID": "posts/logistic-regress-caret/logregcaret.html#model-evaluation-and-diagnostics",
    "href": "posts/logistic-regress-caret/logregcaret.html#model-evaluation-and-diagnostics",
    "title": "Logistic Regression with caret",
    "section": "Model Evaluation and Diagnostics",
    "text": "Model Evaluation and Diagnostics\nIs this model any good? How well does it fit our data? Which predictors are the most important?\n\n1. Likelihood Ratio Test:\nTests whether the full model or a lesser model is the better fit for the data. A smaller p-value provides sufficient evidence that the full model is better than a reduced model.\n\n\nCode\nmod_fit_one <- glm(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + \n                     CreditHistory.Critical,\n                   data=training,\n                   family=\"binomial\")\nmod_fit_two <- glm(Class ~ Age + ForeignWorker,\n                   data=training,\n                   family = binomial(link = \"logit\"))\n\nanova(mod_fit_one, mod_fit_two, test =\"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + \n    CreditHistory.Critical\nModel 2: Class ~ Age + ForeignWorker\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       594     691.14                          \n2       597     723.12 -3  -31.976 5.294e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConclusion The full model is a better fit than a reduced model.\n\n\n2. Pseudo R^2\nLinear regression has the well-known R^{2}. However, logistic regression does not, so we’ll use something called McFadden’s R^{2}, which is given by: 1-\\frac{ln(LM)}{ln(L0)} where ln(LM) is the log likelihood value for the fitted model and ln(L0) is the log likelihood for the null model with just the intercept. Values closer to 0 indicate less predictive power and values closer to 1 indicate more predictive power.\n\n\nCode\nlibrary(pscl)\n\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\n\nCode\npR2(mod_fit_one)\n\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-345.57110076 -366.51858123   41.89496095    0.05715257    0.06744294 \n         r2CU \n   0.09562580 \n\n\nThe output above indicates that are model has low predictive power\n\n\n3. Hosmer-Lemeshow Test\nThis test examines whether the proportion of events are similar to the predicted probabilities of occurrence in subgroups of the data using a chi-square test. Interpretation: Small values with large p-values indicate a good fit to the data while large values with p-values below 0.05 indicate a poor fit. Null hypothesis: the model fits the data\n\n\n4. Variable Importance\nMeasures the absolute value of the t-statistic for each model parameter\n\n\nCode\nvarImp(lmodel)\n\n\nglm variable importance\n\n                       Overall\nCreditHistory.Critical  100.00\nHousing.Own              69.35\nProperty.RealEstate      59.15\nForeignWorker            28.81\nAge                       0.00\n\n\n\n\n5. Confusion Matrix\n\n\nCode\npreds = predict(lmodel, newdata = testing)\nconfusionMatrix(data = preds, reference = testing$Class)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Bad Good\n      Bad   12   15\n      Good 108  265\n                                          \n               Accuracy : 0.6925          \n                 95% CI : (0.6447, 0.7374)\n    No Information Rate : 0.7             \n    P-Value [Acc > NIR] : 0.651           \n                                          \n                  Kappa : 0.0596          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.1000          \n            Specificity : 0.9464          \n         Pos Pred Value : 0.4444          \n         Neg Pred Value : 0.7105          \n             Prevalence : 0.3000          \n         Detection Rate : 0.0300          \n   Detection Prevalence : 0.0675          \n      Balanced Accuracy : 0.5232          \n                                          \n       'Positive' Class : Bad"
  },
  {
    "objectID": "posts/misc/test.html",
    "href": "posts/misc/test.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "This project will focus on the famous XGBoost system and its application on a moderately large dataset. Starting with a history of the system, then exploring the algorithm itself, and finally ending with an application to forest cover types, this project will hopefully provide a framework on which to base further research and applications.\n\n\n\n\n\n\nHello there my friend!"
  },
  {
    "objectID": "posts/misc/test.html#sources",
    "href": "posts/misc/test.html#sources",
    "title": "Miscellaneous",
    "section": "Sources",
    "text": "Sources\n\nSO - Two Column Layout in Quarto"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Hello there.\nMy name is Matthias and I am a data analyst currently working in the insurance industry.\nI am an analytically minded developer with 5 years of experience in data analysis and 2 years of experience in model building. I hold a Master of Science in mathematics with a specialization in statistics, where I created software in Python for analyzing the validity of clustering models."
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Homepage",
    "section": "Goals",
    "text": "Goals\nMy goals for this website are to learn more about frontend development as well as new technologies, like Quarto. I also wanted to have a more interactive method of discovering and keep track of my projects; opposed to the current method of static Word documents and mono repositories."
  },
  {
    "objectID": "index.html#what-is-quarto",
    "href": "index.html#what-is-quarto",
    "title": "Homepage",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto helps you have your ideas and your code in one place, and present it in a beautiful way.\nQuarto unifies and extends the RMarkdown ecosystem - it unifies by combining the functionality of R Markdown, bookdown, distill, xaringian, etc into a single consistent system. And it extends in several ways: all features are possible beyond R too, including Python and Javascript. It also has more “guardrails”: accessibility and inclusion are centered in the design. Quarto is for people who love RMarkdown, and it’s for people who have never used RMarkdown."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "code\n\n\n\n\nA test notebook for multi-column outputs\n\n\n\n\n\n\nNov 4, 2022\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nThis project was the seminal work of my master’s degree in statistics.\n\n\n\n\n\n\nNov 1, 2022\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nA notebook on what XGBoost is and how it works, including a sample application to forest cover types.\n\n\n\n\n\n\nDec 8, 2021\n\n\nMatthias Quinn, Matthew Brigham\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nA quick notebook on how to create and interpret naive bayes models.\n\n\n\n\n\n\nJan 29, 2020\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncode\n\n\nML\n\n\nalgorithms\n\n\ncollege\n\n\n\n\nA quick notebook on how to create and interpret LR models.\n\n\n\n\n\n\nOct 26, 2019\n\n\nMatthias Quinn\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncode\n\n\nfinance\n\n\ncollege\n\n\n\n\nThis project is a financial case study on mutual funds and using MFM to predict risk-adjusted scores of execess returns.\n\n\n\n\n\n\nApr 4, 2019\n\n\nMatthias Quinn, Touw, Lux\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Tech Stack\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\nSQL\n\n\n\n\nFigure 1: Languages\n\n\n\n\nRemarks\nI also used to be a researcher that worked on developing a better understanding of a social phenomenon called the school-to-prison pipeline. It sounds depressing, but that’s because it really is.\nIn my free time, I work on projects like this website and - as of Oct. 3rd, 2022 - a project for performance metrics with researchers from Vietnam. I also enjoy learning about my field and keeping up with current advances in ML and DL. I don’t (usually) fully understand what most of the papers are doing, but I make attempts to research topics of interest.\n\n\nTimeline\n\"\n\n\nPerception of Skills\n\n\n\nAs of November 2022, Made with ggplot2"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Examining Trauma and Crime by Gender and Sexual Orientation among Youth: Findings from the Add Health National Longitudinal Study\n\n\n\n\n\nAuthors: Yun, Tedor, Mallett, Quinn, Quinn\nPaper: Link to Paper - SAGE Journal\n\n\n\n🚧 This page is under construction. 🚧"
  },
  {
    "objectID": "resumes.html",
    "href": "resumes.html",
    "title": "Resumes",
    "section": "",
    "text": "Notice\n\n\n\nAdditional information coming soon!"
  }
]